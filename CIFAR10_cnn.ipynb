{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"CIFAR10_cnn.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"TFGGSTpW_RbD"},"source":["# CIFAR 10"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ayq3B9TI_Kdm","executionInfo":{"status":"ok","timestamp":1637020827699,"user_tz":300,"elapsed":18706,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}},"outputId":"c54a289b-1333-47bb-954c-a30794a76d2c"},"source":["#! git clone https://github.com/cesar-claros/synergistic\n","#% cd synergistic/\n","from google.colab import drive\n","drive.mount('/content/drive')\n","% cd drive/MyDrive/synergistic"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/MyDrive/synergistic\n"]}]},{"cell_type":"markdown","metadata":{"id":"f-lBcXow_Zz4"},"source":["## Dependencies"]},{"cell_type":"code","metadata":{"id":"4NehHQNB_bjw"},"source":["#%%\n","# Command line instalation\n","# ---------------------------\n","!pip install torch\n","!pip install gpytorch\n","!pip install tensorflow-determinism\n","\n","# Imports\n","# ---------------------------\n","import io #Used as buffer\n","import sys\n","import matplotlib\n","import tensorflow as tf # Keras model for MNIST \n","# matplotlib.use('qt5Agg')\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import math\n","import auxfunc.funcs as sgn\n","import seaborn as sns\n","import torch\n","from scipy.stats import entropy, spearmanr\n","from sklearn import model_selection, svm, ensemble, linear_model, pipeline, metrics,\\\n","      tree, neighbors, discriminant_analysis, gaussian_process, preprocessing, impute, decomposition\n","from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n","plt.style.use(['ggplot','style/style.mplstyle'])\n","import os\n","import random"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JAZez0Yd_sar"},"source":["## Auxiliar Functions"]},{"cell_type":"code","metadata":{"id":"oPr-qGxurI6H","executionInfo":{"status":"ok","timestamp":1637020989884,"user_tz":300,"elapsed":157,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Define classic MLP architecture\n","def CNN_model(input_dim):\n","    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n","    model = tf.keras.Sequential()\n","    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(32,32,3))) \n","    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n","    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n","    model.add(tf.keras.layers.Dropout(0.5))\n","    model.add(tf.keras.layers.Flatten())\n","    model.add(tf.keras.layers.Dense(128, activation='relu'))\n","    model.add(tf.keras.layers.Dropout(0.5))\n","    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n","    return model"],"execution_count":5,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3WjH5CGADeK","executionInfo":{"status":"ok","timestamp":1637020991734,"user_tz":300,"elapsed":4,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Signaling function fitting and evaluation\n","def signalingFunction(X_train, y_train, y_train_pred_th, X_val, y_val, y_val_pred_th, X_test, y_test, y_test_pred_th, kernel='exponential', norm='l01', ex_dim=1):\n","    # X_train, X_val should be scaled\n","    # Fit signaling function \n","    exp = sgn.signaling(norm=norm) # idx = [train,test,val]\n","    exp.fit(X_train, y_train, y_train_pred_th, kernel=kernel, n_iter=500, lr=0.01, ex_dim=ex_dim)\n","    table_val = exp.evaluate(X_val, y_val, y_val_pred_th, rule_grid=np.linspace(0,3,30, endpoint=False), rho_grid=[0.1, 0.15])\n","    table_test = exp.test(X_test, y_test, y_test_pred_th, table_val['rule'].to_numpy(), table_val['eta'].to_numpy())\n","    table = pd.concat([table_val,table_test],axis=1)\n","    return table, exp"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"nRVYBv5lxPJk","executionInfo":{"status":"ok","timestamp":1637020993669,"user_tz":300,"elapsed":3,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Initialize model\n","def init_model(input_dim):\n","    # svm = False\n","    # model = MLP_model(input_dim=Data_X.shape[1], svm_obj=svm)\n","    model = CNN_model(input_dim=input_dim)\n","    loss = tf.keras.losses.categorical_crossentropy\n","    metric = ['accuracy']\n","    model.compile(loss=loss,\n","                optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, \n","                                        beta_2=0.999, epsilon=1e-07, amsgrad=False,\n","                                        name='Adam'),\n","                metrics=metric)\n","    # model.summary()\n","    return model"],"execution_count":7,"outputs":[]},{"cell_type":"code","metadata":{"id":"pMK50NRixYI9","executionInfo":{"status":"ok","timestamp":1637020996456,"user_tz":300,"elapsed":167,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Soft and thresholded output predictions\n","def pred_output(model, X):\n","    y_pred_soft = model.predict(X)\n","    y_pred_th = np.argmax(y_pred_soft, axis=1)\n","    return y_pred_soft, y_pred_th"],"execution_count":8,"outputs":[]},{"cell_type":"code","metadata":{"id":"t174yG8_f1_2","executionInfo":{"status":"ok","timestamp":1637020998549,"user_tz":300,"elapsed":241,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Jaccard similarity index\n","def jaccard_similarity(list1, list2):\n","    s1 = set(list1)\n","    s2 = set(list2)\n","    return len(s1.intersection(s2)) / len(s1.union(s2))"],"execution_count":9,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6VM8w4vf4lN","executionInfo":{"status":"ok","timestamp":1637021000712,"user_tz":300,"elapsed":3,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["#%%\n","# Baseline comparison\n","def baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp):\n","      direction = 'further'\n","      # p_val = np.concatenate(y_val_pred_soft,axis=1)\n","      crit_val = entropy(y_val_pred_soft, axis=1, base=10)\n","      # p_test = np.concatenate(y_test_pred_soft,axis=1)\n","      crit_test = entropy(y_test_pred_soft, axis=1, base=10)\n","      \n","      critFunc = sgn.critEvaluation(norm='l01',direction=direction)\n","      d_val = critFunc.evaluate(y_val, y_val_pred_th, crit_val, rho_grid=[0.1, 0.15])\n","      d_test = critFunc.test(y_test, y_test_pred_th, crit_test, d_val['thresh'].to_numpy())\n","      crit_table = pd.concat([d_val,d_test],axis=1)\n","\n","      gamma = table['rule'].to_numpy().reshape(-1,1)\n","      f_test = exp.gpr_mean_test + gamma*np.sqrt(exp.gpr_var_test)\n","      eta = table['eta'].to_numpy().reshape(-1,1)\n","      theta = crit_table['thresh'].to_numpy().reshape(-1,1)\n","      if direction == 'closer':\n","        f_mask, f_idx = np.nonzero(f_test>eta)\n","      else:\n","        f_mask, f_idx = np.nonzero(f_test<eta)\n","      crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)<theta)\n","      print(list(np.unique(f_mask)))\n","      print(list(np.unique(crit_mask)))\n","      print(f_test.shape[0])\n","      shared = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask))))\n","      J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan for i in range(f_test.shape[0])]\n","      # if (list(np.unique(f_mask))==list(np.unique(crit_mask))):\n","      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) for i in np.unique(f_mask)]\n","      # else:\n","      #   shared = set(a).intersection(set(b))\n","      #   union = set(a).union(set(b))\n","      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan  for i in union]\n","      crit_table['jaccard']=J\n","      Sp = [spearmanr(f_test[i,:],crit_test)[0] for i in range(f_test.shape[0])]\n","      crit_table['spearman'] = Sp\n","      crit_table['gamma'] = gamma\n","      return crit_table"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Wbo4e_6RA8_b"},"source":["## Signailing function and baselines"]},{"cell_type":"code","metadata":{"id":"ICCVCQKDU00u","executionInfo":{"status":"ok","timestamp":1637021005761,"user_tz":300,"elapsed":162,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["# For reproducibility\n","os.environ['TF_DETERMINISTIC_OPS'] = '1'\n","SEED = 12345\n","os.environ['PYTHONHASHSEED']=str(SEED)\n","random.seed(SEED)\n","np.random.seed(SEED)\n","tf.random.set_seed(SEED)\n","torch.manual_seed(SEED)\n","torch.cuda.manual_seed_all(SEED)"],"execution_count":11,"outputs":[]},{"cell_type":"code","metadata":{"id":"R3VCXn3_rTYj","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637021016531,"user_tz":300,"elapsed":8202,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}},"outputId":"e36b6aa3-0736-4d48-82b7-ea332bfc7430"},"source":["# %%\n","# INITIALIZATION\n","# ==============\n","# EXPERIMENT SETUP\n","# ================\n","# Load data set\n","(Data_X, Data_y), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n","# y, y_test = y.astype('int8'), y_test.astype('int8')\n","# Rescale the images from [0,255] to the [0.0,1.0] range.\n","Data_X, X_test = Data_X[...]/255.0, X_test[...]/255.0\n","print(\"Number of original training examples:\", len(Data_X))\n","# reshape the data\n","# Data_X = Data_X.reshape(60000, 28*28).astype('float32')\n","# X_test = X_test.reshape(10000, 28*28).astype('float32')\n","# for keras classification, we need to use `to_categorical` to transform the label to appropriate format\n","# Data_y = tf.keras.utils.to_categorical(Data_y)\n","# y_test = tf.keras.utils.to_categorical(y_test)\n","Data_X, Data_X_sep, Data_y, Data_y_sep = model_selection.train_test_split(Data_X, Data_y, stratify=Data_y, test_size=0.75, random_state=SEED)\n","X_test, X_test_sep, y_test, y_test_sep = model_selection.train_test_split(X_test, y_test, stratify=y_test, test_size=0.75, random_state=SEED)\n","print(Data_X.shape)\n","print(X_test.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n","170500096/170498071 [==============================] - 4s 0us/step\n","170508288/170498071 [==============================] - 4s 0us/step\n","Number of original training examples: 50000\n","(12500, 32, 32, 3)\n","(2500, 32, 32, 3)\n"]}]},{"cell_type":"code","metadata":{"id":"0jsIZNLPri8D","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637022592084,"user_tz":300,"elapsed":1524817,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}},"outputId":"cafa82cb-f6c4-4f1d-dfd3-1116e2bf4ec2"},"source":["#%%\n","# Assign labels\n","report_table = []\n","report_criteria = []\n","report_plot = []\n","kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n","clf = 'softmax_act'\n","addPredictions = True\n","accuracy = 0\n","es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', verbose=1)\n","y_test = tf.keras.utils.to_categorical(y_test)\n","for train, val in kf.split(Data_X, Data_y):\n","    # print(train[:10])\n","    # print(val[:10])\n","\n","    X_train = Data_X[train]\n","    y_train = Data_y[train]\n","    # print(y_train[:10])\n","    y_train = tf.keras.utils.to_categorical(y_train)\n","    X_val = Data_X[val]\n","    y_val = Data_y[val]\n","    # print(y_val[:10])\n","    y_val = tf.keras.utils.to_categorical(y_val)\n","\n","    # sample = sample[:12500]\n","    # test = test[:2000]\n","    # X = Data_X[sample]\n","    # y = Data_y[sample]\n","    # X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.20, random_state=123)\n","    # X_test = Data_X[test]\n","    # y_test = Data_y[test]\n","\n","    # TRAINING MODEL\n","    model = init_model(input_dim=Data_X.shape[1:])\n","    # model.fit(X_train, y_train, batch_size=128, epochs=30, verbose=0, validation_data=(X_val, y_val), callbacks=[es])\n","    model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=0, validation_data=(X_val, y_val))\n","    y_train_pred_soft, y_train_pred_th = pred_output(model, X_train)\n","    print('accuracy(Train)={}'.format(np.sum(y_train_pred_th==np.argmax(y_train,axis=1))/y_train_pred_th.size))\n","    y_val_pred_soft, y_val_pred_th = pred_output(model, X_val)\n","    y_test_pred_soft, y_test_pred_th = pred_output(model, X_test)\n","\n","    # layer_outputs = [layer.output for layer in model.layers[4]]\n","    if clf == 'softmax_act':\n","            activation_model = tf.keras.models.Model(inputs=model.input, outputs=model.layers[5].output)\n","            X_train_GP = activation_model.predict(X_train)\n","            X_val_GP = activation_model.predict(X_val)\n","            X_test_GP = activation_model.predict(X_test)\n","    elif clf == 'softmax':\n","            X_train_GP = X_train.reshape(-1,np.prod(X_train.shape[1:]))\n","            X_val_GP = X_val.reshape(-1,np.prod(X_val.shape[1:]))\n","            X_test_GP = X_test.reshape(-1,np.prod(X_test.shape[1:]))\n","\n","    if addPredictions:\n","            # Add predictions\n","            X_train_GP = np.concatenate((X_train_GP, y_train_pred_soft), axis=1)\n","            X_val_GP = np.concatenate((X_val_GP, y_val_pred_soft), axis=1)\n","            X_test_GP = np.concatenate((X_test_GP, y_test_pred_soft), axis=1)\n","    scaleX_GP = preprocessing.StandardScaler().fit(np.concatenate((X_train_GP, X_val_GP), axis=0))\n","    X_train_GP = scaleX_GP.transform(X_train_GP)\n","    X_val_GP = scaleX_GP.transform(X_val_GP)\n","    X_test_GP = scaleX_GP.transform(X_test_GP)\n","    \n","    table, exp = signalingFunction(X_train_GP, np.argmax(y_train, axis=1), y_train_pred_th, \\\n","                                   X_val_GP, np.argmax(y_val, axis=1), y_val_pred_th,\\\n","                                   X_test_GP, np.argmax(y_test, axis=1), y_test_pred_th,\\\n","                                   kernel='e*e', ex_dim=y_train_pred_soft.shape[1])\n","    print('gpr_mean=',exp.gpr_mean_test[:10])\n","    print('gpr_mean=',exp.gpr_mean_val[:10])\n","\n","    report_table.append(table)\n","    # Baseline for comparison\n","    crit_table = baselineCriteria(np.argmax(y_val, axis=1), y_val_pred_soft, y_val_pred_th,\\\n","                                  np.argmax(y_test, axis=1), y_test_pred_soft, y_test_pred_th,\\\n","                                  table, exp)\n","    report_criteria.append(crit_table)\n","\n","    score = np.sum(np.argmax(y_val, axis=1)==y_val_pred_th)/np.size(np.argmax(y_val, axis=1))\n","    if accuracy < score:\n","      accuracy = score\n","      table_best = table\n","      crit_table_best = crit_table\n","      exp_best = exp\n","      y_test_best = y_test\n","      y_test_pred_soft_best = y_test_pred_soft\n","      y_test_pred_th_best = y_test_pred_th\n","      X_test_best = X_test\n","    del(model)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["accuracy(Train)=0.7637\n","initializing cuda...\n","lr=0.01, n_iterations=500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"]},{"output_type":"stream","name":"stdout","text":["Iter 491/500 - Loss: 0.397  noise: 0.073\n","Iter 492/500 - Loss: 0.397  noise: 0.073\n","Iter 493/500 - Loss: 0.398  noise: 0.073\n","Iter 494/500 - Loss: 0.399  noise: 0.073\n","Iter 495/500 - Loss: 0.397  noise: 0.073\n","Iter 496/500 - Loss: 0.398  noise: 0.072\n","Iter 497/500 - Loss: 0.397  noise: 0.072\n","Iter 498/500 - Loss: 0.397  noise: 0.072\n","Iter 499/500 - Loss: 0.397  noise: 0.072\n","Iter 500/500 - Loss: 0.397  noise: 0.072\n","evaluating with cuda...\n","evaluating with cuda...\n","gpr_mean= [0.58200586 0.687774   0.10061729 0.03780665 0.28242505 0.5804637\n"," 0.53093624 0.5839175  0.27867338 0.4405837 ]\n","gpr_mean= [0.6496083  0.02120446 0.07774353 0.665051   0.14885487 0.3067841\n"," 0.27879626 0.11616373 0.13538209 0.3921111 ]\n","[0, 1]\n","[0, 1]\n","2\n","accuracy(Train)=0.7351\n","initializing cuda...\n","lr=0.01, n_iterations=500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"]},{"output_type":"stream","name":"stdout","text":["Iter 491/500 - Loss: 0.455  noise: 0.087\n","Iter 492/500 - Loss: 0.454  noise: 0.087\n","Iter 493/500 - Loss: 0.455  noise: 0.087\n","Iter 494/500 - Loss: 0.455  noise: 0.087\n","Iter 495/500 - Loss: 0.455  noise: 0.087\n","Iter 496/500 - Loss: 0.456  noise: 0.087\n","Iter 497/500 - Loss: 0.456  noise: 0.086\n","Iter 498/500 - Loss: 0.455  noise: 0.086\n","Iter 499/500 - Loss: 0.454  noise: 0.086\n","Iter 500/500 - Loss: 0.456  noise: 0.086\n","evaluating with cuda...\n","evaluating with cuda...\n","gpr_mean= [0.32224905 0.6059368  0.189987   0.12346278 0.6795134  0.393834\n"," 0.6850247  0.6312304  0.45675743 0.36212194]\n","gpr_mean= [ 0.6557771   0.13108897  0.28739834  0.18052056  0.02941845 -0.02822219\n","  0.05373994  0.05520813  0.33857048  0.4182504 ]\n","[0, 1]\n","[0, 1]\n","2\n","accuracy(Train)=0.7676\n","initializing cuda...\n","lr=0.01, n_iterations=500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"]},{"output_type":"stream","name":"stdout","text":["Iter 491/500 - Loss: 0.401  noise: 0.076\n","Iter 492/500 - Loss: 0.402  noise: 0.076\n","Iter 493/500 - Loss: 0.403  noise: 0.076\n","Iter 494/500 - Loss: 0.402  noise: 0.076\n","Iter 495/500 - Loss: 0.403  noise: 0.076\n","Iter 496/500 - Loss: 0.402  noise: 0.076\n","Iter 497/500 - Loss: 0.403  noise: 0.076\n","Iter 498/500 - Loss: 0.402  noise: 0.076\n","Iter 499/500 - Loss: 0.403  noise: 0.076\n","Iter 500/500 - Loss: 0.402  noise: 0.075\n","evaluating with cuda...\n","evaluating with cuda...\n","gpr_mean= [ 0.15681095  0.45321873  0.2051257  -0.00738452  0.02040692  0.39629465\n","  0.7240012   0.7442411   0.45499825  0.25973102]\n","gpr_mean= [0.41550398 0.02068242 0.00049561 0.13012555 0.18489906 0.23362249\n"," 0.399621   0.40808502 0.03812411 0.19217832]\n","[0, 1]\n","[0, 1]\n","2\n","accuracy(Train)=0.7652\n","initializing cuda...\n","lr=0.01, n_iterations=500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"]},{"output_type":"stream","name":"stdout","text":["Iter 491/500 - Loss: 0.406  noise: 0.078\n","Iter 492/500 - Loss: 0.405  noise: 0.077\n","Iter 493/500 - Loss: 0.406  noise: 0.077\n","Iter 494/500 - Loss: 0.405  noise: 0.077\n","Iter 495/500 - Loss: 0.405  noise: 0.077\n","Iter 496/500 - Loss: 0.404  noise: 0.077\n","Iter 497/500 - Loss: 0.405  noise: 0.077\n","Iter 498/500 - Loss: 0.405  noise: 0.077\n","Iter 499/500 - Loss: 0.405  noise: 0.077\n","Iter 500/500 - Loss: 0.405  noise: 0.077\n","evaluating with cuda...\n","evaluating with cuda...\n","gpr_mean= [ 0.58118176  0.53355366  0.18673049 -0.01973408  0.4038078   0.4190059\n","  0.5605661   0.42274097  0.44251627  0.3912616 ]\n","gpr_mean= [ 0.73932505  0.5539346   0.00444715  0.00792214  0.37075084 -0.02252111\n","  0.06795825  0.23518023  0.39218006  0.09107177]\n","[0, 1]\n","[0, 1]\n","2\n","accuracy(Train)=0.772\n","initializing cuda...\n","lr=0.01, n_iterations=500\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n","  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"]},{"output_type":"stream","name":"stdout","text":["Iter 491/500 - Loss: 0.408  noise: 0.077\n","Iter 492/500 - Loss: 0.409  noise: 0.077\n","Iter 493/500 - Loss: 0.407  noise: 0.077\n","Iter 494/500 - Loss: 0.407  noise: 0.077\n","Iter 495/500 - Loss: 0.407  noise: 0.076\n","Iter 496/500 - Loss: 0.407  noise: 0.076\n","Iter 497/500 - Loss: 0.408  noise: 0.076\n","Iter 498/500 - Loss: 0.406  noise: 0.076\n","Iter 499/500 - Loss: 0.408  noise: 0.076\n","Iter 500/500 - Loss: 0.407  noise: 0.076\n","evaluating with cuda...\n","evaluating with cuda...\n","gpr_mean= [0.57749856 0.38556975 0.0354155  0.01567383 0.27746496 0.20872684\n"," 0.54679394 0.57790965 0.4021756  0.33554047]\n","gpr_mean= [ 0.40375686  0.21846443 -0.01204202  0.03499709  0.06870238  0.14974013\n","  0.05177575  0.04053752  0.07587171  0.4277088 ]\n","[0, 1]\n","[0, 1]\n","2\n"]}]},{"cell_type":"code","metadata":{"id":"NnV3S5ntjoRB","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637023870011,"user_tz":300,"elapsed":172,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}},"outputId":"20cb08c2-d640-4e5e-b0ba-bdaee43474e1"},"source":["report_table_concat = pd.concat(report_table)\n","# report_table_concat\n","report_criteria_concat = pd.concat(report_criteria)\n","# report_criteria_concat\n","# p-value column\n","p_value_col = report_table_concat['p_value'] #Add\n","cols_CQT = ['rho_user','corrected_test','queries_test','total_wrong_test','loss_query_test']\n","cols_rholoss = ['rho_user','rho_hat_test','%loss_red_test']\n","\n","# Dataframes for f(x)\n","df_fx_CQT = pd.DataFrame(report_table_concat[cols_CQT])\n","df_fx_rholoss = pd.DataFrame(report_table_concat[cols_rholoss])\n","# results_fx_CQT = df_fx_CQT.loc[p_value_col <= 0.05].copy()\n","results_fx_rholoss = df_fx_rholoss.loc[p_value_col <= 0.05].copy()\n","results_fx_CQT = df_fx_CQT.copy()\n","# results_fx_rholoss = df_fx_rholoss.copy()\n","\n","results_fx_rholoss_bri = results_fx_rholoss.groupby(results_fx_rholoss.index)\n","fx_rholoss_median = results_fx_rholoss_bri.median()\n","fx_rholoss_q1 = results_fx_rholoss_bri.quantile(q=0)\n","fx_rholoss_q3 = results_fx_rholoss_bri.quantile(q=1)\n","results_fx_CQT_bri = results_fx_CQT.groupby(results_fx_CQT.index)\n","fx_CQT_median = results_fx_CQT_bri.median()\n","fx_CQT_q1 = results_fx_CQT_bri.quantile(q=0)\n","fx_CQT_q3 = results_fx_CQT_bri.quantile(q=1)\n","\n","# Dataframes for g(x)\n","df_gx_CQT = pd.DataFrame(report_criteria_concat[cols_CQT])\n","df_gx_rholoss = pd.DataFrame(report_criteria_concat[cols_rholoss])\n","# results_gx_CQT = df_gx_CQT.loc[p_value_col <= 0.05].copy()\n","results_gx_rholoss = df_gx_rholoss.loc[p_value_col <= 0.05].copy()\n","results_gx_CQT = df_gx_CQT.copy()\n","# results_gx_rholoss = df_gx_rholoss.copy()\n","\n","results_gx_rholoss_bri = results_gx_rholoss.groupby(results_gx_rholoss.index)\n","gx_rholoss_median = results_gx_rholoss_bri.median()\n","gx_rholoss_q1 = results_gx_rholoss_bri.quantile(q=0)\n","gx_rholoss_q3 = results_gx_rholoss_bri.quantile(q=1)\n","results_gx_CQT_bri = results_gx_CQT.groupby(results_gx_CQT.index)\n","gx_CQT_median = results_gx_CQT_bri.median()\n","gx_CQT_q1 = results_gx_CQT_bri.quantile(q=0)\n","gx_CQT_q3 = results_gx_CQT_bri.quantile(q=1)\n","\n","# Signaling function statistics (median(q1-q3)) LaTex\n","output_test = io.StringIO()\n","# numRows = fx_median.shape[0]\n","# numCols = fx_median.shape[1]\n","output_test.write(\"results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\\n\")\n","output_test.write(\"----------\\n\")\n","\n","for rho in [0.10,0.15]:\n","  # output_test.write(\"rho={:.2f}\\\\\\\\\\n\".format(rho))\n","\n","  fx_CQT_filtered = results_fx_CQT.loc[results_fx_CQT['rho_user']==rho]\n","  gx_CQT_filtered = results_gx_CQT.loc[results_gx_CQT['rho_user']==rho]\n","  p_value_filtered = p_value_col[results_fx_CQT['rho_user']==rho]\n","  # print(p_value_filtered)\n","  n_folds = fx_CQT_filtered.shape[0]\n","  row_fx = [' ', ' ',' ',r'{:.2f}'.format(rho),r'$f(x)$']\n","  row_gx = [' ',' ',' ',' ',r'$g(x)$']\n","\n","  row_fx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'queries_test'],\\\n","                                                      fx_CQT_median.loc[fx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n","                                                      fx_CQT_median.loc[fx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n","  row_fx_rho = [r'{:.2f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n","  row_fx_loss = [r'{:.1f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n","  row_fx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n","                                                      fx_CQT_q3.loc[fx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n","                                                      fx_CQT_q1.loc[fx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n","  row_fx = row_fx + row_fx_CQT + row_fx_rho + row_fx_loss + row_fx_lq\n","  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_fx)))\n","\n","  row_gx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'queries_test'],\\\n","                                                      gx_CQT_median.loc[gx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n","                                                      gx_CQT_median.loc[gx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n","  row_gx_rho = [r'{:.2f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n","  row_gx_loss = [r'{:.1f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n","  row_gx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n","                                                      gx_CQT_q3.loc[gx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n","                                                      gx_CQT_q1.loc[gx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n","  row_gx = row_gx + row_gx_CQT + row_gx_rho + row_gx_loss + row_gx_lq\n","  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_gx)))\n","  output_test.write(\"\\\\cline{4-9}\\n\")\n","\n","\n","print(output_test.getvalue())"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\n","----------\n","  &   &   & 0.10 & $f(x)$ & (242,186,1021) & 0.10 & 17.6 & 0.74(0.77-0.72)\\\\\n","  &   &   &   & $g(x)$ & (264,190,1021) & 0.11 & 18.6 & 0.71(0.72-0.65)\\\\\n","\\cline{4-9}\n","  &   &   & 0.15 & $f(x)$ & (386,281,1021) & 0.15 & 27.5 & 0.71(0.75-0.69)\\\\\n","  &   &   &   & $g(x)$ & (383,263,1021) & 0.15 & 25.2 & 0.69(0.69-0.63)\\\\\n","\\cline{4-9}\n","\n"]}]},{"cell_type":"code","metadata":{"id":"zQeWpMFeoR5I"},"source":["##%\n","# Boxplot (loss reduction in test set)\n","report_table_concat = pd.concat(report_table)\n","cols_table = ['p_value','rho_user','%reduction_test']\n","df_boxplot_table = pd.DataFrame(report_table_concat[cols_table])\n","df_boxplot_table['label'] = df_boxplot_table.shape[0]*['$f(x)$']\n","report_criteria_concat = pd.concat(report_criteria)\n","columns_crit = ['rho_user','%reduction_test']\n","df_boxplot_crit = pd.DataFrame(report_criteria_concat[columns_crit])\n","df_boxplot_crit['label'] = df_boxplot_crit.shape[0]*['$g(x)$']\n","# p-value median\n","p_value_col = df_boxplot_table['p_value'] #Add\n","p_value_by_row_index = df_boxplot_table['p_value'].groupby(df_boxplot_table.index)\n","p_value_median = p_value_by_row_index.median()\n","# Boxplot (jaccard index in test set)\n","columns_jac = ['rho_user','jaccard']\n","df_jaccard = pd.DataFrame(report_criteria_concat[columns_jac])\n","# Unfiltered Result dataframes\n","cols_fx = ['rho_user','%reduction_val','budget','%reduction_test']\n","results_fx = pd.DataFrame(report_table_concat[cols_fx])\n","cols_fxgx = ['rho_user','%reduction_test', 'jaccard']\n","results_fxgx = pd.concat([df_boxplot_table[cols_fxgx[:2]], df_boxplot_crit[cols_fxgx[1]], df_jaccard[cols_fxgx[2]]], axis=1)\n","# Filter experiments with p_value > 0.05\n","df_boxplot_crit = df_boxplot_crit.loc[df_boxplot_table['p_value'] <= 0.05]\n","df_jaccard = df_jaccard.loc[df_boxplot_table['p_value'] <= 0.05]\n","df_boxplot_table = df_boxplot_table.loc[df_boxplot_table['p_value'] <= 0.05]\n","# Boxplot with filtered values only\n","frames = [df_boxplot_table, df_boxplot_crit]\n","df = pd.concat(frames)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U9UhCBCvoXT0"},"source":["# Avoid plotting when median(p_value)>0.5\n","for i in range(p_value_median.shape[0]):\n","    if p_value_median.iloc[i]>0.05:\n","      df.loc[df.index==i,'%reduction_test'] = np.nan\n","      df_jaccard.loc[df_jaccard.index==i, 'jaccard'] = np.nan"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Q-zU6_XqoZ9N"},"source":["# Dataframe for results f(x)\n","results_fx = results_fx.loc[p_value_col <= 0.05].copy()\n","results_fx_by_row_index = results_fx.groupby(results_fx.index)\n","fx_median = results_fx_by_row_index.median()\n","fx_q1 = results_fx_by_row_index.quantile(q=0.25)\n","fx_q3 = results_fx_by_row_index.quantile(q=0.75)\n","# Signaling function statistics (median(q1-q3)) LaTex\n","output_fx = io.StringIO()\n","numRows = fx_median.shape[0]\n","numCols = fx_median.shape[1]\n","output_fx.write(\"results_fx (\\\\rho|%reduction_val|sig_rate|%reduction_test|H0)\\n\")\n","output_fx.write(\"----------\\n\")\n","for i in range(numRows):\n","  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n","         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==2)\\\n","         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fx_median.iloc[i],fx_q1.iloc[i],fx_q3.iloc[i],range(numCols))]\n","  output_fx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n","print(output_fx.getvalue())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4obWyq2UodSY"},"source":["# Dataframe for comparison f(x)-g(x)\n","results_fxgx = results_fxgx.loc[p_value_col <= 0.05].copy()\n","results_fxgx_by_row_index = results_fxgx.groupby(results_fxgx.index)\n","fxgx_median = results_fxgx_by_row_index.median()\n","fxgx_q1 = results_fxgx_by_row_index.quantile(q=0.25)\n","fxgx_q3 = results_fxgx_by_row_index.quantile(q=0.75)\n","# Baseline comparison statistics (median(q1-q3)) LaTex\n","output_fxgx = io.StringIO()\n","numRows = fxgx_median.shape[0]\n","numCols = fxgx_median.shape[1]\n","output_fxgx.write(\"results_fxgx (\\\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\\n\")\n","output_fxgx.write(\"------------\\n\")\n","for i in range(numRows):\n","  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n","         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==3)\\\n","         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fxgx_median.iloc[i],fxgx_q1.iloc[i],fxgx_q3.iloc[i],range(numCols))]\n","  output_fxgx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n","print(output_fxgx.getvalue())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"worAyJtSopME"},"source":["#%%\n","# Save results in csv fomat\n","path_csv = \"drive/My Drive/NIPS2020/results/cifar10/results_{clf}_yhat{yhat}_pca{pca}.csv\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n","results = pd.concat([results_fx, results_fxgx, p_value_col.loc[p_value_col <= 0.05]], keys=['fx', 'fxgx', ''], axis=1).to_csv(path_csv, index=True, header=True)\n","# modified output\n","# Save results in tex fomat\n","L = [output_fx.getvalue(),output_fxgx.getvalue()]\n","path_txt = \"drive/My Drive/NIPS2020/results/cifar10/results_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n","txt = open(path_txt, \"w\") \n","txt.writelines(L) \n","txt.close() #to change file access modes"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dt4yHhb1o6Ck"},"source":["fig, ax = plt.subplots(1,2,figsize=(15, 5.1), constrained_layout=False, dpi=90)\n","pal = sns.color_palette('Paired')\n","sns.boxplot(x=df['rho_user'], y=df['%reduction_test'], hue='label', data=df, ax=ax[0], palette=pal)\n","ax[0].set_xlabel(r'budget $\\rho$')\n","ax[0].set_ylabel(r'Loss reduction $r_{test}(\\%)$')\n","ax[0].legend(loc='upper left')\n","pal = sns.color_palette('BuGn_r')\n","sns.boxplot(x=df_jaccard['rho_user'], y=df_jaccard['jaccard'], data=df_jaccard, ax=ax[1], palette=pal)\n","ax[1].set_xlabel(r'budget $\\rho$')\n","ax[1].set_ylabel(r'Jaccard index $J$')\n","plt.tight_layout()\n","path_fig_fxgx = \"drive/My Drive/NIPS2020/results/cifar10/fig_fxgx_{clf}_yhat{yhat}_pca{pca}.pdf\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n","plt.savefig(path_fig_fxgx, bbox_inches='tight', facecolor='w')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mj6x1QVqcRgO","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1637024028001,"user_tz":300,"elapsed":156,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}},"outputId":"0d807049-4657-49c4-f7eb-24e551a39d27"},"source":["rho = 0.15\n","X_test_best = X_test_best.squeeze()\n","rule = table_best.loc[table_best.rho_user == rho]['rule'].to_numpy()\n","eta = table_best.loc[table_best.rho_user == rho]['eta'].to_numpy()[0]\n","theta = crit_table_best.loc[crit_table_best.rho_user == rho]['thresh'].to_numpy()[0]\n","f_test = exp_best.gpr_mean_test+rule*np.sqrt(exp_best.gpr_var_test)\n","top_n = 5 # Top n selected instances in test set\n","top_f_idx = np.argpartition(f_test, -top_n)[-top_n:]\n","top_f_idx = top_f_idx[np.argsort(f_test[top_f_idx])[::-1]]# Added\n","# p_test = np.concatenate((y_test_pred_soft_best,1-y_test_pred_soft_best),axis=1)\n","crit_test = entropy(y_test_pred_soft_best, axis=1, base=10)\n","top_crit_idx = np.argpartition(crit_test, -top_n)[-top_n:]\n","top_crit_idx = top_crit_idx[np.argsort(crit_test[top_crit_idx])[::-1]]# Added\n","\n","output_text = io.StringIO()\n","print('eta={:.3f},theta={:.3f}'.format(eta,theta))"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["eta=0.538,theta=0.722\n"]}]},{"cell_type":"code","metadata":{"id":"BAt0-8IaWG2c","executionInfo":{"status":"ok","timestamp":1637020526708,"user_tz":300,"elapsed":164,"user":{"displayName":"Nicolás López","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gj5A5lrxkaC-D0vX_KynIoFk5O4b9T0lh-buwn3Cg=s64","userId":"01323730031600165904"}}},"source":["roc_f = metrics.roc_auc_score(exp_best.L_test, f_test)\n","roc_crit = metrics.roc_auc_score(exp_best.L_test, crit_test)"],"execution_count":23,"outputs":[]},{"cell_type":"code","metadata":{"id":"PJlMUtf_wTWa"},"source":["# Plot selected instances\n","# initialize the label \n","labelNames = ['airplane', 'auto', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n","# X_test_best = X_test.reshape(-1,28,28)\n","# y_hat_best = y_test_pred_th[top_f_idx]\n","# y_th_best = np.argmax(y_test[top_f_idx], axis=1)\n","# Plot instances\n","row,col = 2,top_n\n","fig1, ax = plt.subplots(row, col, figsize=(6.0, 3.5), constrained_layout=True, dpi=120)\n","# fig1.subplots_adjust(wspace=0.1, hspace=0.35, top=0.5)\n","# for i in top_f_idx:\n","j = 0\n","for i,k in zip(top_f_idx, top_crit_idx):\n","    if np.argmax(y_test_best, axis=1)[i] != y_test_pred_th_best[i]:\n","        color = 'red'\n","    else:\n","        color = 'green'\n","    # ax = fig1.add_subplot(3, 4, j+1)\n","    ax[0][j].imshow(X_test_best[i, :, :, :])\n","    ax[0][j].set_title(r'$y=${s1}'.format(s1=labelNames[np.argmax(y_test_best, axis=1)[i]])+',\\n'+\\\n","                    r'$\\hat{{y}}=${s2}'.format(s2=labelNames[y_test_pred_th_best[i]]), color=color, fontsize=13)\n","    ax[0][j].set_xlabel(r\"$g(x)=${:.2f}\".format(crit_test[i])+'\\n$f(x)=${:.2f}'.format(f_test[i]), fontsize=13)\n","    ax[0][j].set_xticks([])\n","    ax[0][j].set_yticks([])\n","\n","    if np.argmax(y_test_best, axis=1)[k] != y_test_pred_th_best[k]:\n","        color = 'red'\n","    else:\n","        color = 'green'\n","    # ax = fig1.add_subplot(3, 4, j+1)\n","    ax[1][j].imshow(X_test_best[k, :, :, :])\n","    ax[1][j].set_title(r'$y=${s1}'.format(s1=labelNames[np.argmax(y_test_best, axis=1)[k]])+',\\n'+\\\n","                    r'$\\hat{{y}}=${s2}'.format(s2=labelNames[y_test_pred_th_best[k]]), color=color, fontsize=13)\n","    ax[1][j].set_xlabel(r\"$g(x)=${:.2f}\".format(crit_test[k])+'\\n$f(x)=${:.2f}'.format(f_test[k]), fontsize=13)\n","    ax[1][j].set_xticks([])\n","    ax[1][j].set_yticks([])\n","    j = j + 1\n","fig1.set_constrained_layout_pads(w_pad=0, h_pad=0, hspace=0.01, wspace=-.5)\n","ax[0][0].set_ylabel(r\"$f(x)>\\eta$\"+\"\\n\"+r\"$(AUC={:.2f})$\".format(roc_f), fontsize=13)\n","# ax[0][0].set_ylabel(r\"$f(x)>\\eta$\", fontsize=14)\n","ax[1][0].set_ylabel(r\"$g(x)>\\theta$\"+\"\\n\"+r\"$(AUC={:.2f})$\".format(roc_crit), fontsize=13)\n","# ax[1][0].set_ylabel(r\"$g(x)>\\theta$\", fontsize=14)\n","\n","# fig1.text(0.5, 0.01, r'$\\rho={},~|f(x)>\\eta|={},~|g(x)>\\theta|={}$'.format(rho,np.sum(f_test>eta),np.sum(crit_test>theta)), ha='center', fontsize = 12)\n","# plt.suptitle(r'Top {} selected instances'.format(top_n), fontsize=15)\n","# plt.tight_layout()\n","path_fig_fxgx_test = \"drive/My Drive/NIPS2020/results/cifar10/fig_fxgx_test_{clf}_yhat{yhat}_pca{pca}.svg\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n","plt.savefig(path_fig_fxgx_test, bbox_inches='tight', facecolor='w')"],"execution_count":null,"outputs":[]}]}