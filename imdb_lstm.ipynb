{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_lstm",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesar-claros/synergistic/blob/master/imdb_lstm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4sBUso-gM_4",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 256
        },
        "outputId": "64a8848e-be11-4c47-fb86-9026b441f3c6"
      },
      "source": [
        "!pip install torch\n",
        "!pip install gpytorch"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (1.5.1+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torch) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch) (0.16.0)\n",
            "Collecting gpytorch\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5d/c7/0c31802b84fc55aa069943c844eaccb0e420e91d7f4ed07cc5e1d127c458/gpytorch-1.1.1.tar.gz (240kB)\n",
            "\u001b[K     |████████████████████████████████| 245kB 7.1MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: gpytorch\n",
            "  Building wheel for gpytorch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gpytorch: filename=gpytorch-1.1.1-py2.py3-none-any.whl size=400467 sha256=e87898f391e84f8661b53c3b85bd8090b8e4d062025785def161865b2d514ef5\n",
            "  Stored in directory: /root/.cache/pip/wheels/e5/a5/29/4dafc0624adf678108e0067836556f0c72588e85d851d78ae0\n",
            "Successfully built gpytorch\n",
            "Installing collected packages: gpytorch\n",
            "Successfully installed gpytorch-1.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FymsszQy2n68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! sudo apt-get install texlive-latex-recommended #1\n",
        "! sudo apt-get install dvipng texlive-fonts-recommended #2\n",
        "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip #3\n",
        "! unzip type1cm.zip -d /tmp/type1cm #4\n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins  #5\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm #6\n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm #7\n",
        "! sudo texhash #8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0gzdUCWgQOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 129
        },
        "outputId": "19943ee8-b1cf-4ead-97d6-994c04e82a8a"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pODgafakgcZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/NIPS2020/auxfunc/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/datasets/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/style/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/runs/\" ."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrowTWaDjHhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKBZk4trqzQV",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "outputId": "93484439-9694-448c-c8aa-1a3f71ac76be"
      },
      "source": [
        "# Imports\n",
        "import io #Used as buffer\n",
        "import sys\n",
        "import matplotlib\n",
        "import tensorflow as tf # Keras model for MNIST \n",
        "# matplotlib.use('qt5Agg')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import auxfunc.sigfunc as sgn\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from scipy.stats import entropy, spearmanr\n",
        "from sklearn import model_selection, svm, ensemble, linear_model, pipeline, \\\n",
        "      tree, neighbors, discriminant_analysis, gaussian_process, preprocessing, impute, decomposition\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
        "plt.style.use(['ggplot','style/style.mplstyle'])"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/statsmodels/tools/_testing.py:19: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
            "  import pandas.util.testing as tm\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPr-qGxurI6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Define LSTM architecture\n",
        "def LSTM_model(top_words, emb_vector_length, max_review_length, objective, reg=0.01):\n",
        "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(top_words,emb_vector_length,input_length=max_review_length))\n",
        "    model.add(tf.keras.layers.LSTM(100))\n",
        "    if objective == 'svm':\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units=1, kernel_initializer='uniform', activation='linear', kernel_regularizer=tf.keras.regularizers.l2(reg)\n",
        "        ))\n",
        "    elif objective == 'softmax':\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units=1, kernel_initializer='uniform', activation='sigmoid'\n",
        "        ))\n",
        "    return model"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYYvRG4AmtEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Signaling function fitting and evaluation\n",
        "def signalingFunction(X_train, y_train, y_train_pred_th, X_val, y_val, y_val_pred_th, X_test, y_test, y_test_pred_th, kernel='exponential', norm='l01'):\n",
        "    # X_train, X_val should be scaled\n",
        "    # Fit signaling function \n",
        "    exp = sgn.signaling(norm=norm) # idx = [train,test,val]\n",
        "    exp.fit(X_train, y_train, y_train_pred_th, kernel=kernel, n_iter=500, lr=0.01)\n",
        "    table_val = exp.evaluate(X_val, y_val, y_val_pred_th, rule_grid=np.linspace(0,3,30, endpoint=False))\n",
        "    table_test = exp.test(X_test, y_test, y_test_pred_th, table_val['rule'].to_numpy(), table_val['eta'].to_numpy())\n",
        "    table = pd.concat([table_val,table_test],axis=1)\n",
        "    return table, exp"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRVYBv5lxPJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Initialize model\n",
        "def init_model(input_dim, objective):\n",
        "    model = LSTM_model(top_words, emb_vector_length, max_review_length, objective)\n",
        "    if objective=='svm':\n",
        "        loss = tf.keras.losses.hinge\n",
        "        metric = ['hinge']\n",
        "\n",
        "    elif objective=='softmax':\n",
        "        loss = tf.keras.losses.binary_crossentropy\n",
        "        metric = ['accuracy']\n",
        "\n",
        "    model.compile(loss=loss,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=metric)\n",
        "    # model.summary()\n",
        "    print('loss={}'.format(loss.__name__))\n",
        "    return model"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMK50NRixYI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Soft and thresholded output predictions\n",
        "def pred_output(model, X, objective):\n",
        "    if objective=='svm':\n",
        "        y_pred_soft = model.predict(X)\n",
        "        y_pred_th = np.array([1 if i[0] >= 0 else 0 for i in y_pred_soft])\n",
        "    elif objective=='softmax':\n",
        "        y_pred_soft = model.predict(X)\n",
        "        y_pred_th = np.array([1 if i[0] > 0.5 else 0 for i in y_pred_soft])\n",
        "    return y_pred_soft, y_pred_th"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nf3lDBxjZcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Jaccard similarity index\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyWbCZTIjbiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Baseline comparison\n",
        "def baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf):\n",
        "      if clf=='svm':\n",
        "          direction = 'closer'\n",
        "          crit_val = np.abs(y_val_pred_soft.ravel())\n",
        "          crit_test = np.abs(y_test_pred_soft.ravel())\n",
        "      else:\n",
        "          direction = 'further'\n",
        "          p_val = np.concatenate((y_val_pred_soft,1-y_val_pred_soft),axis=1)\n",
        "          crit_val = entropy(p_val, axis=1, base=2)\n",
        "          p_test = np.concatenate((y_test_pred_soft,1-y_test_pred_soft),axis=1)\n",
        "          crit_test = entropy(p_test, axis=1, base=2)\n",
        "      \n",
        "      critFunc = sgn.critEvaluation(norm='l01',direction=direction)\n",
        "      d_val = critFunc.evaluate(y_val, y_val_pred_th, crit_val)\n",
        "      d_test = critFunc.test(y_test, y_test_pred_th, crit_test, d_val['thresh'].to_numpy())\n",
        "      crit_table = pd.concat([d_val,d_test],axis=1)\n",
        "\n",
        "      gamma = table['rule'].to_numpy().reshape(-1,1)\n",
        "      f_test = exp.gpr_mean_test + gamma*np.sqrt(exp.gpr_var_test)\n",
        "      eta = table['eta'].to_numpy().reshape(-1,1)\n",
        "      theta = crit_table['thresh'].to_numpy().reshape(-1,1)\n",
        "      if direction == 'closer':\n",
        "        f_mask, f_idx = np.nonzero(f_test>eta)\n",
        "      else:\n",
        "        f_mask, f_idx = np.nonzero(f_test<eta)\n",
        "      crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)<theta)\n",
        "      print(list(np.unique(f_mask)))\n",
        "      print(list(np.unique(crit_mask)))\n",
        "      print(f_test.shape[0])\n",
        "      shared = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask))))\n",
        "      J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan for i in range(f_test.shape[0])]\n",
        "      # if (list(np.unique(f_mask))==list(np.unique(crit_mask))):\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) for i in np.unique(f_mask)]\n",
        "      # else:\n",
        "      #   shared = set(a).intersection(set(b))\n",
        "      #   union = set(a).union(set(b))\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan  for i in union]\n",
        "      crit_table['jaccard']=J\n",
        "      Sp = [spearmanr(f_test[i,:],crit_test)[0] for i in range(f_test.shape[0])]\n",
        "      crit_table['spearman'] = Sp\n",
        "      crit_table['gamma'] = gamma\n",
        "      return crit_table"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3VCXn3_rTYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "3f2114e3-5673-4d86-c233-e7aee4e6aad8"
      },
      "source": [
        "# %%\n",
        "# INITIALIZATION\n",
        "# ==============\n",
        "# EXPERIMENT SETUP\n",
        "# ================\n",
        "top_words = 5000\n",
        "max_review_length = 500\n",
        "emb_vector_length = 32\n",
        "# Load data set\n",
        "(Data_X, Data_y), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=top_words)\n",
        "Data_X = tf.keras.preprocessing.sequence.pad_sequences(Data_X, maxlen=max_review_length)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print(\"Number of original training examples:\", len(Data_X))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17465344/17464789 [==============================] - 1s 0us/step\n",
            "Number of original training examples: 25000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jsIZNLPri8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "5dcf6f30-0969-45d1-9c8e-0c6b8009f83d"
      },
      "source": [
        "# For reproducibility\n",
        "tf.random.set_seed(54321)\n",
        "np.random.seed(12345)\n",
        "torch.manual_seed(0)\n",
        "#%%\n",
        "# Assign labels\n",
        "report_table = []\n",
        "report_criteria = []\n",
        "report_plot = []\n",
        "kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "clf = 'softmax'\n",
        "addPredictions = True\n",
        "applyPCA = True\n",
        "accuracy = 0\n",
        "for sample, test in kf.split(Data_X):\n",
        "    sample = sample[:12500]\n",
        "    test = test[:2000]\n",
        "    X = Data_X[sample]\n",
        "    y = Data_y[sample]\n",
        "    X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.20, random_state=123)\n",
        "    X_test = Data_X[test]\n",
        "    y_test = Data_y[test]\n",
        "\n",
        "    # TRAINING MODEL\n",
        "    model = init_model(input_dim=X.shape[1], objective=clf)\n",
        "    model.fit(X_train, y_train, batch_size=64, epochs=5, verbose=0, validation_data=(X_val, y_val))\n",
        "    # X_test = scaleX.transform(imputeX.transform(X_test))\n",
        "\n",
        "    y_train_pred_soft, y_train_pred_th = pred_output(model, X_train, clf)\n",
        "    print('accuracy(Train)={}'.format(np.sum(y_train==y_train_pred_th)/np.size(y_train)))\n",
        "    y_val_pred_soft, y_val_pred_th = pred_output(model, X_val, clf)\n",
        "    # print('accuracy(Val)={}'.format(np.sum(y_val==y_val_pred_th)/np.size(y_val)))\n",
        "    y_test_pred_soft, y_test_pred_th = pred_output(model, X_test, clf)\n",
        "\n",
        "    layer_outputs = [layer.output for layer in model.layers] \n",
        "    activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "    X_train_GP = activation_model.predict(X_train)[1]\n",
        "    X_val_GP = activation_model.predict(X_val)[1]\n",
        "    X_test_GP = activation_model.predict(X_test)[1]\n",
        "\n",
        "    if addPredictions:\n",
        "            # Add predictions\n",
        "            X_train_GP = np.concatenate((X_train_GP, y_train_pred_soft), axis=1)\n",
        "            X_val_GP = np.concatenate((X_val_GP, y_val_pred_soft), axis=1)\n",
        "            X_test_GP = np.concatenate((X_test_GP, y_test_pred_soft), axis=1)\n",
        "    scaleX_GP = preprocessing.StandardScaler().fit(np.concatenate((X_train_GP, X_val_GP), axis=0))\n",
        "    X_train_GP = scaleX_GP.transform(X_train_GP)\n",
        "    X_val_GP = scaleX_GP.transform(X_val_GP)\n",
        "    X_test_GP = scaleX_GP.transform(X_test_GP)\n",
        "    if applyPCA:\n",
        "            pca_GP = decomposition.PCA(.99).fit(np.concatenate((X_train_GP, X_val_GP), axis=0)) # set percentage of energy preserved by PCA\n",
        "            # Apply PCA transform to all sets\n",
        "            X_train_GP = pca_GP.transform(X_train_GP)\n",
        "            X_val_GP = pca_GP.transform(X_val_GP)\n",
        "            X_test_GP = pca_GP.transform(X_test_GP)\n",
        "\n",
        "    table, exp = signalingFunction(X_train_GP, y_train, y_train_pred_th, X_val_GP, y_val, y_val_pred_th, X_test_GP, y_test, y_test_pred_th)\n",
        "    report_table.append(table)\n",
        "    # Baseline for comparison\n",
        "    crit_table = baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf)\n",
        "    report_criteria.append(crit_table)\n",
        "\n",
        "    score = np.sum(y_val==y_val_pred_th)/np.size(y_val)\n",
        "    if accuracy < score:\n",
        "      accuracy = score\n",
        "      table_best = table\n",
        "      crit_table_best = crit_table\n",
        "      exp_best = exp\n",
        "      y_test_best = y_test\n",
        "      y_test_pred_soft_best = y_test_pred_soft\n",
        "      y_test_pred_th_best = y_test_pred_th\n",
        "      X_test_best = X_test\n",
        "    del(model)"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9495\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.239  noise: 0.026\n",
            "Iter 492/500 - Loss: -0.238  noise: 0.026\n",
            "Iter 493/500 - Loss: -0.237  noise: 0.026\n",
            "Iter 494/500 - Loss: -0.238  noise: 0.026\n",
            "Iter 495/500 - Loss: -0.239  noise: 0.026\n",
            "Iter 496/500 - Loss: -0.237  noise: 0.026\n",
            "Iter 497/500 - Loss: -0.237  noise: 0.026\n",
            "Iter 498/500 - Loss: -0.238  noise: 0.026\n",
            "Iter 499/500 - Loss: -0.236  noise: 0.026\n",
            "Iter 500/500 - Loss: -0.238  noise: 0.026\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "5\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9257\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.084  noise: 0.035\n",
            "Iter 492/500 - Loss: -0.081  noise: 0.035\n",
            "Iter 493/500 - Loss: -0.083  noise: 0.035\n",
            "Iter 494/500 - Loss: -0.084  noise: 0.035\n",
            "Iter 495/500 - Loss: -0.085  noise: 0.035\n",
            "Iter 496/500 - Loss: -0.081  noise: 0.035\n",
            "Iter 497/500 - Loss: -0.084  noise: 0.035\n",
            "Iter 498/500 - Loss: -0.082  noise: 0.035\n",
            "Iter 499/500 - Loss: -0.084  noise: 0.035\n",
            "Iter 500/500 - Loss: -0.084  noise: 0.035\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "5\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9488\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.175  noise: 0.030\n",
            "Iter 492/500 - Loss: -0.176  noise: 0.030\n",
            "Iter 493/500 - Loss: -0.173  noise: 0.030\n",
            "Iter 494/500 - Loss: -0.174  noise: 0.030\n",
            "Iter 495/500 - Loss: -0.176  noise: 0.030\n",
            "Iter 496/500 - Loss: -0.174  noise: 0.030\n",
            "Iter 497/500 - Loss: -0.177  noise: 0.030\n",
            "Iter 498/500 - Loss: -0.174  noise: 0.030\n",
            "Iter 499/500 - Loss: -0.175  noise: 0.030\n",
            "Iter 500/500 - Loss: -0.175  noise: 0.030\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "5\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9602\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.284  noise: 0.024\n",
            "Iter 492/500 - Loss: -0.286  noise: 0.024\n",
            "Iter 493/500 - Loss: -0.287  noise: 0.024\n",
            "Iter 494/500 - Loss: -0.286  noise: 0.024\n",
            "Iter 495/500 - Loss: -0.287  noise: 0.024\n",
            "Iter 496/500 - Loss: -0.286  noise: 0.024\n",
            "Iter 497/500 - Loss: -0.288  noise: 0.024\n",
            "Iter 498/500 - Loss: -0.286  noise: 0.024\n",
            "Iter 499/500 - Loss: -0.286  noise: 0.024\n",
            "Iter 500/500 - Loss: -0.286  noise: 0.024\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "5\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9364\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.103  noise: 0.034\n",
            "Iter 492/500 - Loss: -0.101  noise: 0.034\n",
            "Iter 493/500 - Loss: -0.101  noise: 0.034\n",
            "Iter 494/500 - Loss: -0.104  noise: 0.034\n",
            "Iter 495/500 - Loss: -0.102  noise: 0.034\n",
            "Iter 496/500 - Loss: -0.102  noise: 0.034\n",
            "Iter 497/500 - Loss: -0.100  noise: 0.034\n",
            "Iter 498/500 - Loss: -0.100  noise: 0.034\n",
            "Iter 499/500 - Loss: -0.102  noise: 0.033\n",
            "Iter 500/500 - Loss: -0.101  noise: 0.033\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1, 2, 3, 4]\n",
            "[0, 1, 2, 3, 4]\n",
            "5\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWPUAct1q2Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_base_dir = \"runs/\"\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lNsdkAGq5Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##%\n",
        "# Boxplot (loss reduction in test set)\n",
        "report_table_concat = pd.concat(report_table)\n",
        "cols_table = ['p_value','rho_user','%reduction_test']\n",
        "df_boxplot_table = pd.DataFrame(report_table_concat[cols_table])\n",
        "df_boxplot_table['label'] = df_boxplot_table.shape[0]*['$f(x)$']\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "columns_crit = ['rho_user','%reduction_test']\n",
        "df_boxplot_crit = pd.DataFrame(report_criteria_concat[columns_crit])\n",
        "df_boxplot_crit['label'] = df_boxplot_crit.shape[0]*['$g(x)$']\n",
        "# p-value median\n",
        "p_value_by_row_index = df_boxplot_table['p_value'].groupby(df_boxplot_table.index)\n",
        "p_value_median = p_value_by_row_index.median()\n",
        "# Boxplot (jaccard index in test set)\n",
        "columns_jac = ['rho_user','jaccard']\n",
        "df_jaccard = pd.DataFrame(report_criteria_concat[columns_jac])\n",
        "# Unfiltered Result dataframes\n",
        "cols_fx = ['rho_user','%reduction_val','budget','%reduction_test']\n",
        "results_fx = pd.DataFrame(report_table_concat[cols_fx])\n",
        "cols_fxgx = ['rho_user','%reduction_test', 'jaccard']\n",
        "results_fxgx = pd.concat([df_boxplot_table[cols_fxgx[:2]], df_boxplot_crit[cols_fxgx[1]], df_jaccard[cols_fxgx[2]]], axis=1)\n",
        "# Filter experiments with p_value > 0.05\n",
        "df_boxplot_crit = df_boxplot_crit.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_jaccard = df_jaccard.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_boxplot_table = df_boxplot_table.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "# Boxplot with filtered values only\n",
        "frames = [df_boxplot_table, df_boxplot_crit]\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OumIYSVNq7Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avoid plotting when median(p_value)>0.5\n",
        "for i in range(p_value_median.shape[0]):\n",
        "    if p_value_median.iloc[i]>0.05:\n",
        "      df.loc[df.index==i,'%reduction_test'] = np.nan\n",
        "      df_jaccard.loc[df_jaccard.index==i, 'jaccard'] = np.nan"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6d4xyQvq952",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "e756399a-e3cc-4eea-c128-b43c19ae9e6d"
      },
      "source": [
        "# Dataframe for results f(x)\n",
        "results_fx_by_row_index = results_fx.groupby(results_fx.index)\n",
        "fx_median = results_fx_by_row_index.median()\n",
        "fx_q1 = results_fx_by_row_index.quantile(q=0.25)\n",
        "fx_q3 = results_fx_by_row_index.quantile(q=0.75)\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_fx = io.StringIO()\n",
        "numRows = fx_median.shape[0]\n",
        "numCols = fx_median.shape[1]\n",
        "output_fx.write(\"results_fx (\\\\rho|%reduction_val|sig_rate|%reduction_test|H0)\\n\")\n",
        "output_fx.write(\"----------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==2)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fx_median.iloc[i],fx_q1.iloc[i],fx_q3.iloc[i],range(numCols))]\n",
        "  output_fx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fx.getvalue())"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results_fx (\\rho|%reduction_val|sig_rate|%reduction_test|H0)\n",
            "----------\n",
            "{} & {} & 0.01 & 3.1(2.6-3.5) & 0.01(0.01-0.01) & 3.1(2.5-3.1) & $\\surd$ \\\\\n",
            "{} & {} & 0.05 & 15.6(14.4-17.1) & 0.05(0.05-0.05) & 14.6(13.6-15.8) & $\\surd$ \\\\\n",
            "{} & {} & 0.10 & 29.4(26.1-30.5) & 0.10(0.10-0.11) & 28.8(28.1-30.7) & $\\surd$ \\\\\n",
            "{} & {} & 0.15 & 40.6(37.4-41.7) & 0.15(0.14-0.15) & 36.3(35.9-42.2) & $\\surd$ \\\\\n",
            "{} & {} & 0.20 & 48.8(46.7-50.2) & 0.20(0.20-0.21) & 48.3(45.1-49.8) & $\\surd$ \\\\\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjL-JtRhrC6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "1284aa37-d61e-4222-9e0d-45c3c5c3ed5c"
      },
      "source": [
        "# Dataframe for comparison f(x)-g(x)\n",
        "results_fxgx_by_row_index = results_fxgx.groupby(results_fxgx.index)\n",
        "fxgx_median = results_fxgx_by_row_index.median()\n",
        "fxgx_q1 = results_fxgx_by_row_index.quantile(q=0.25)\n",
        "fxgx_q3 = results_fxgx_by_row_index.quantile(q=0.75)\n",
        "# Baseline comparison statistics (median(q1-q3)) LaTex\n",
        "output_fxgx = io.StringIO()\n",
        "numRows = fxgx_median.shape[0]\n",
        "numCols = fxgx_median.shape[1]\n",
        "output_fxgx.write(\"results_fxgx (\\\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\\n\")\n",
        "output_fxgx.write(\"------------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==3)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fxgx_median.iloc[i],fxgx_q1.iloc[i],fxgx_q3.iloc[i],range(numCols))]\n",
        "  output_fxgx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fxgx.getvalue())"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results_fxgx (\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\n",
            "------------\n",
            "{} & {} & 0.01 & 3.1(2.5-3.1) & 3.4(3.4-3.6) & 0.98(0.98-0.98) & $\\surd$ \\\\\n",
            "{} & {} & 0.05 & 14.6(13.6-15.8) & 15.5(14.8-15.9) & 0.96(0.95-0.96) & $\\surd$ \\\\\n",
            "{} & {} & 0.10 & 28.8(28.1-30.7) & 30.5(27.7-30.5) & 0.94(0.94-0.94) & $\\surd$ \\\\\n",
            "{} & {} & 0.15 & 36.3(35.9-42.2) & 41.9(41.0-42.8) & 0.91(0.91-0.91) & $\\surd$ \\\\\n",
            "{} & {} & 0.20 & 48.3(45.1-49.8) & 51.5(49.5-53.9) & 0.89(0.88-0.90) & $\\surd$ \\\\\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0X7KJ6trNGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Save results in csv fomat\n",
        "path_csv = \"drive/My Drive/NIPS2020/results/imdb/results_{clf}_yhat{yhat}_pca{pca}.csv\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "results = pd.concat([results_fx, results_fxgx], keys=['fx', 'fxgx'], axis=1).to_csv(path_csv, index=True, header=True)\n",
        "# Save results in tex fomat\n",
        "L = [output_fx.getvalue(),output_fxgx.getvalue()]\n",
        "path_txt = \"drive/My Drive/NIPS2020/results/imdb/results_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(L) \n",
        "txt.close() #to change file access modes"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTFKkXsrQOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "ae2525cb-d847-4d08-c768-b20b7ac32b9d"
      },
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(15, 5.1), constrained_layout=False, dpi=90)\n",
        "pal = sns.color_palette('Paired')\n",
        "sns.boxplot(x=df['rho_user'], y=df['%reduction_test'], hue='label', data=df, ax=ax[0], palette=pal)\n",
        "ax[0].set_xlabel(r'budget $\\rho$')\n",
        "ax[0].set_ylabel(r'Loss reduction $r_{test}(\\%)$')\n",
        "ax[0].legend(loc='upper left')\n",
        "pal = sns.color_palette('BuGn_r')\n",
        "sns.boxplot(x=df_jaccard['rho_user'], y=df_jaccard['jaccard'], data=df_jaccard, ax=ax[1], palette=pal)\n",
        "ax[1].set_xlabel(r'budget $\\rho$')\n",
        "ax[1].set_ylabel(r'Jaccard index $J$')\n",
        "plt.tight_layout()\n",
        "path_fig_fxgx = \"drive/My Drive/NIPS2020/results/imdb/fig_fxgx_{clf}_yhat{yhat}_pca{pca}.pdf\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fxgx, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1350x459 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQtQPDGFE9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reverse lookup\n",
        "INDEX_FROM = 3\n",
        "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj6x1QVqcRgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "2a0f835e-05b1-45af-b3fa-602312378444"
      },
      "source": [
        "rho = 0.01\n",
        "rule = table_best.loc[table_best.rho_user == rho]['rule'].to_numpy()\n",
        "eta = table_best.loc[table_best.rho_user == rho]['eta'].to_numpy()[0]\n",
        "theta = crit_table_best.loc[crit_table_best.rho_user == rho]['thresh'].to_numpy()[0]\n",
        "f_test = exp_best.gpr_mean_test+rule*np.sqrt(exp_best.gpr_var_test)\n",
        "top_n = 20 # Top n selected instances in test set\n",
        "top_f_idx = np.argpartition(f_test, -top_n)[-top_n:]\n",
        "if clf=='svm':\n",
        "    crit_test = np.abs(y_test_pred_soft_best.ravel())\n",
        "    top_crit_idx = np.argpartition(crit_test, top_n)[:top_n]\n",
        "elif clf=='softmax':\n",
        "    p_test = np.concatenate((y_test_pred_soft_best,1-y_test_pred_soft_best),axis=1)\n",
        "    crit_test = entropy(p_test, axis=1, base=2)\n",
        "    top_crit_idx = np.argpartition(crit_test, -top_n)[-top_n:]\n",
        "output_text = io.StringIO()\n",
        "print('eta={:.3f}, theta={:.3f}'.format(eta,theta))"
      ],
      "execution_count": 83,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eta=0.833, theta=0.997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqv4gVyc1kF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 533
        },
        "outputId": "e2d7cb83-9189-4d24-a4c6-58535a0b1e00"
      },
      "source": [
        "output_text.write(\"top_n={}, rho_user={}, g(x)={}, addh_hat={}, PCA={}\\n\".format(top_n,rho,clf, addPredictions, applyPCA))\n",
        "output_text.write(\"|f(x)>eta|={}(eta={:.3f}), |g(x)>theta|={}(theta={:.3f})\\n\".format(np.sum(f_test>eta), eta, np.sum(crit_test<theta) if clf=='svm' else np.sum(crit_test>theta), theta))\n",
        "output_text.write(\"\\nTop misclassfied instances picked by f(x)\\n\")\n",
        "output_text.write(\"-----------------------------------------\\n\")\n",
        "for i in top_f_idx:\n",
        "  cond = f_test[i]>eta\n",
        "  if y_test_best[i] != y_test_pred_th_best[i] and cond:\n",
        "    output_text.write(' '.join([id_to_word[id] for id in X_test_best[i,:] if id!=0])+\\\n",
        "        '\\n y={}'.format(y_test_best[i])+', y_pred={}'.format(y_test_pred_th_best[i])+\\\n",
        "        ', g(x)({})={:.3f}'.format('D' if clf=='svm' else 'H', crit_test[i])+\\\n",
        "        ', f(x)={:.3f}'.format(f_test[i])+'\\n\\n')\n",
        "print(output_text.getvalue())"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top_n=20, rho_user=0.01, g(x)=softmax, addh_hat=True, PCA=True\n",
            "|f(x)>eta|=29(eta=0.833), |g(x)>theta|=30(theta=0.997)\n",
            "\n",
            "Top misclassfied instances picked by f(x)\n",
            "-----------------------------------------\n",
            "handed the job on a silver <UNK> and has to do little to <UNK> his position unlike <UNK> <UNK> hits on all 5 families in <UNK> this hand me down process may be an authentic way of <UNK> power in a mafia family but why is so much made of this boring routine certainly don vincent may <UNK> the respect of his fellow gangsters <UNK> but there's little character <UNK> in the script to give us a portrait of this young man sadly <UNK> did such a fine job of quickly <UNK> developing <UNK> character in <UNK> but <UNK> has no <UNK> in it's story telling and we suffer through drawn out <UNK> until we just want to take a <UNK> <UNK> george hamilton was also handed a <UNK> task in taking over as family lawyer after <UNK> <UNK> <UNK> turned down a 3rd installment as tom <UNK> george <UNK> <UNK> his role so it came off without damage to the actor the development of <UNK> <UNK> character is interesting but it goes too far when she takes murderous matters into her own hands she could well end up sleeping with the <UNK> if this were the real world but it's not all bad the <UNK> scene in the hotel <UNK> is <UNK> also they keep pulling me back in or words to that effect is a great line and we <UNK> some old country feel as we get to go back to <UNK> even if it's all done in <UNK> and they've got modern cars <UNK> the plot lines involving the corrupt <UNK> of the catholic church are pretty interesting since it's based on some actual financial <UNK> at <UNK> <UNK> in the <UNK> but it's brought too far again with the too spectacular death scenes etc the grand opera scenes are very dramatic and well photographed but the death scene of mary toward the end is an <UNK> attempt of emotional <UNK> at best all <UNK> need to learn that we don't always need more and more death in order to bring a mafia movie to a successful conclusion you feel sorry for kay that her daughter is dead she plays her <UNK> so well but <UNK> reaction is <UNK> <UNK> <UNK> then michael dies <UNK> in the <UNK> century in <UNK> alone on a grand estate of heart failure no <UNK> to play with before his <UNK> no wife to <UNK> for him what should have happened in the last 1 2 of this movie is michael being <UNK> down by someone like <UNK> <UNK> put on trial all his dirty family's sleazy little <UNK> and <UNK> <UNK> brought into the light of day michael then <UNK> under <UNK> <UNK> and sent to <UNK> prison for life then he can <UNK> over dead from heart failure while <UNK> a floor in <UNK> that's how mafia <UNK> were ending their careers in the late 80's and early 90's and they got much better treatment than what they deserved\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.866\n",
            "\n",
            "<START> american film makers decided to make a film they think is japanese the characters all badly represented the actors are not even japanese and the set is cheap unreal and definitely doesn't represent <UNK> in early <UNK> and <UNK> who ever read the book understand that the script writers didn't add any extra value to <UNK> the movie from the script worse they even changed the original plot line with a few <UNK> rob <UNK> is using for his two main characters two well known chinese actors who joined before in <UNK> tiger hidden dragon <UNK> probably saw one chinese movie and <UNK> they represent japanese culture seeing those two actors together again even makes the movies more ridiculous <UNK> <UNK> last scene in kill bill 1 is ten times more japanese made than that of this movie\n",
            " y=0, y_pred=1, g(x)(H)=0.974, f(x)=0.867\n",
            "\n",
            "<START> another weak third season entry <UNK> there in truth no beauty ' nonetheless has at least one key plot element that is very different and as <UNK> would say fascinating the main character is an alien who must be carried around in a black box because his appearance is so horrendous that it drives humans insane it's too bad the episode cannot live up to this incredible premise obviously i think it was a mistake to ever <UNK> the alien as its actual <UNK> in no way even <UNK> such a <UNK> build up all we get is the standard star trek <UNK> light display used for any number of things in different episodes usually when the ship is passing through a <UNK> storm or something similar in any event <UNK> appearance can at least be <UNK> by mr <UNK> and then only if <UNK> is wearing a special <UNK> for the <UNK> time i thought the <UNK> name was <UNK> ' which i found humorous but i <UNK> <UNK> is required to mind <UNK> with <UNK> at one point so that the alien can pilot the enterprise back to safety this is accomplished but when <UNK> <UNK> go back to end the mind <UNK> by <UNK> <UNK> <UNK> his <UNK> <UNK> oh he goes crazy but eventually <UNK> with the help of <UNK> assistant a blind woman with psychic powers this might have been a really bizarre excellent episode but it is poorly directed and comes across as yet one more badly executed show of the <UNK> last season\n",
            " y=0, y_pred=1, g(x)(H)=0.973, f(x)=0.900\n",
            "\n",
            "<START> sleeping with the enemy is a predictable <UNK> there <UNK> thriller that never seems to find any inspiration no matter how desperately cast and crew try i can't believe a bunch of my friends talked me into seeing this at the movies some <UNK> years ago br br the complete lack of originality from the <UNK> <UNK> screenplay based upon the nancy price novel does not help nor does the stale direction of joseph <UNK> or the very average performance from julia roberts the supporting cast including patrick <UNK> and kevin anderson do little to help br br there really isn't a lot to say just give it a miss br br sunday april 14 <UNK> <UNK> cinema <UNK> <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=0.991, f(x)=0.946\n",
            "\n",
            "<START> a friend once told me that an art house independent film ran in a cinema when upon the closing of the film audiences were so <UNK> they <UNK> to tear up the cinema <UNK> of course my imagination ran <UNK> trying to <UNK> up the <UNK> of such a piece of work well now my imagination can be put to rest br br i am a <UNK> <UNK> <UNK> fan and an <UNK> <UNK> of his work i have come across many people who thought <UNK> films are slow moving and <UNK> opinions being what they are i found this not to be true of the late director's wonderful works which are <UNK> with meaning beautiful <UNK> and complex philosophical questions upon hearing <UNK> <UNK> called the <UNK> to <UNK> i was excited to experience his films br br with the exception of the open air ride through the <UNK> <UNK> this movie has no <UNK> to anything <UNK> has done it does not seem to <UNK> the slightest meaning even on a completely mindless level it's supposedly <UNK> <UNK> cinematography is devoid of any <UNK> craft there is a no balance no <UNK> and the exposure <UNK> seems to be running low on <UNK> in the <UNK> snow the main character is so inept and <UNK> it makes you wonder whether his father might have been alive if he made up his mind <UNK> br br i am also not <UNK> to non plots or story lines that progress on multiple non <UNK> fashion but there isn't even a non story here one must surely enter the viewing of this film with a <UNK> head if one were to <UNK> it with nothing <UNK> and nothing lost as hair pulling would be the only possible answer to a pace that could make a <UNK> time <UNK> look as if jerry <UNK> had filmed a charlie chaplin short br br i won't rule out that this may be one of <UNK> <UNK> <UNK> <UNK> but to <UNK> that he is one of <UNK> <UNK> based on this film would be to call paris <UNK> the <UNK> to <UNK> <UNK> guys don't be afraid to say it no amount of big impressive words is going to <UNK> bring this corpse of celluloid back to life i don't <UNK> to fully understand russian culture and i probably don't have russian values but i immediately picked up on <UNK> work as something magical a treasure and a gift to viewers br br if it didn't have <UNK> name on it and it aired on say saturday night live i'm pretty sure nobody would read all these magnificent analysis into this wet <UNK> of a flick\n",
            " y=0, y_pred=1, g(x)(H)=0.965, f(x)=0.958\n",
            "\n",
            "<START> i grew up watching inspector gadget it was and still is one of my favorite cartoons if not my absolute favorite i learned a lot of <UNK> and history from the spin off inspector <UNK> field trip i wanted to <UNK> on a <UNK> <UNK> and become the greatest detective ever br br but the film has ruined the reputation of the wonderful cartoon br br matthew <UNK> an actor with potential was definitely not the role for inspector gadget first thing in the film inspector gadget is smart not so in the cartoon in the film gadget <UNK> the mystery mostly by himself in the cartoon it was almost always <UNK> brain and the awesome book i still want her book if gadget <UNK> the mystery it was by accident gadget in the film seems to be a competent detective but in the cartoon was pretty dumb which was where the humor came from br br another thing is that it's too much good guy v bad guy in the film it's not just meant to be a silly saturday morning cartoon also gadget never should have a love story but disney <UNK> is filled with idiots br br also i miss the true <UNK> that gadget had and especially the gadget car in the movie it was a <UNK> <UNK> in the cartoon it was a <UNK> police car and could turn into a van it also barely had any <UNK> and was mainly there to get him from place to place br br but if anything the one thing that was terrible about the movie was that it was a feature movie inspector gadget was a silly saturday morning cartoon the movie was too serious too overdone had too much of a plot and wasn't even remotely as funny br br <UNK> for those who haven't seen it never see it ever watch the cartoon it's a true classic\n",
            " y=0, y_pred=1, g(x)(H)=0.954, f(x)=0.943\n",
            "\n",
            "<START> trying to compare or represent this <UNK> as anything <UNK> is an out n out attempt to <UNK> hitchcock fans to waste 7 on this movie weak acting weak story weak script br br no real suspense no thrills you wait all through the <UNK> of this movie for the big <UNK> or even any <UNK> you're left thinking what the heck was that all about br br and please enough with the movement to make <UNK> <UNK> hip and politically correct i can't recommend this to anyone did i mention how weak the acting is williams did a better job as peter pan and <UNK> but those were much more innocent times\n",
            " y=0, y_pred=1, g(x)(H)=0.993, f(x)=1.011\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhsLRvmXO-Ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "17cc932a-9c44-4927-8f48-e9f9216e4147"
      },
      "source": [
        "output_text.write(\"\\nTop misclassfied instances picked by g(x)\\n\")\n",
        "output_text.write(\"-----------------------------------------\\n\")\n",
        "for i in top_crit_idx:\n",
        "  cond = crit_test[i]<theta if clf=='svm' else crit_test[i]>theta\n",
        "  if y_test_best[i] != y_test_pred_th_best[i]:\n",
        "    output_text.write(' '.join([id_to_word[id] for id in X_test_best[i,:] if id!=0])+\\\n",
        "        '\\n y={}'.format(y_test_best[i])+', y_pred={}'.format(y_test_pred_th_best[i])+\\\n",
        "        ', g(x)({})={:.3f}'.format('D' if clf=='svm' else 'H', crit_test[i])+\\\n",
        "        ', f(x)={:.3f}'.format(f_test[i])+'\\n\\n')\n",
        "print(output_text.getvalue())"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top_n=20, rho_user=0.01, g(x)=softmax, addh_hat=True, PCA=True\n",
            "|f(x)>eta|=29(eta=0.833), |g(x)>theta|=30(theta=0.997)\n",
            "\n",
            "Top misclassfied instances picked by f(x)\n",
            "-----------------------------------------\n",
            "handed the job on a silver <UNK> and has to do little to <UNK> his position unlike <UNK> <UNK> hits on all 5 families in <UNK> this hand me down process may be an authentic way of <UNK> power in a mafia family but why is so much made of this boring routine certainly don vincent may <UNK> the respect of his fellow gangsters <UNK> but there's little character <UNK> in the script to give us a portrait of this young man sadly <UNK> did such a fine job of quickly <UNK> developing <UNK> character in <UNK> but <UNK> has no <UNK> in it's story telling and we suffer through drawn out <UNK> until we just want to take a <UNK> <UNK> george hamilton was also handed a <UNK> task in taking over as family lawyer after <UNK> <UNK> <UNK> turned down a 3rd installment as tom <UNK> george <UNK> <UNK> his role so it came off without damage to the actor the development of <UNK> <UNK> character is interesting but it goes too far when she takes murderous matters into her own hands she could well end up sleeping with the <UNK> if this were the real world but it's not all bad the <UNK> scene in the hotel <UNK> is <UNK> also they keep pulling me back in or words to that effect is a great line and we <UNK> some old country feel as we get to go back to <UNK> even if it's all done in <UNK> and they've got modern cars <UNK> the plot lines involving the corrupt <UNK> of the catholic church are pretty interesting since it's based on some actual financial <UNK> at <UNK> <UNK> in the <UNK> but it's brought too far again with the too spectacular death scenes etc the grand opera scenes are very dramatic and well photographed but the death scene of mary toward the end is an <UNK> attempt of emotional <UNK> at best all <UNK> need to learn that we don't always need more and more death in order to bring a mafia movie to a successful conclusion you feel sorry for kay that her daughter is dead she plays her <UNK> so well but <UNK> reaction is <UNK> <UNK> <UNK> then michael dies <UNK> in the <UNK> century in <UNK> alone on a grand estate of heart failure no <UNK> to play with before his <UNK> no wife to <UNK> for him what should have happened in the last 1 2 of this movie is michael being <UNK> down by someone like <UNK> <UNK> put on trial all his dirty family's sleazy little <UNK> and <UNK> <UNK> brought into the light of day michael then <UNK> under <UNK> <UNK> and sent to <UNK> prison for life then he can <UNK> over dead from heart failure while <UNK> a floor in <UNK> that's how mafia <UNK> were ending their careers in the late 80's and early 90's and they got much better treatment than what they deserved\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.866\n",
            "\n",
            "<START> american film makers decided to make a film they think is japanese the characters all badly represented the actors are not even japanese and the set is cheap unreal and definitely doesn't represent <UNK> in early <UNK> and <UNK> who ever read the book understand that the script writers didn't add any extra value to <UNK> the movie from the script worse they even changed the original plot line with a few <UNK> rob <UNK> is using for his two main characters two well known chinese actors who joined before in <UNK> tiger hidden dragon <UNK> probably saw one chinese movie and <UNK> they represent japanese culture seeing those two actors together again even makes the movies more ridiculous <UNK> <UNK> last scene in kill bill 1 is ten times more japanese made than that of this movie\n",
            " y=0, y_pred=1, g(x)(H)=0.974, f(x)=0.867\n",
            "\n",
            "<START> another weak third season entry <UNK> there in truth no beauty ' nonetheless has at least one key plot element that is very different and as <UNK> would say fascinating the main character is an alien who must be carried around in a black box because his appearance is so horrendous that it drives humans insane it's too bad the episode cannot live up to this incredible premise obviously i think it was a mistake to ever <UNK> the alien as its actual <UNK> in no way even <UNK> such a <UNK> build up all we get is the standard star trek <UNK> light display used for any number of things in different episodes usually when the ship is passing through a <UNK> storm or something similar in any event <UNK> appearance can at least be <UNK> by mr <UNK> and then only if <UNK> is wearing a special <UNK> for the <UNK> time i thought the <UNK> name was <UNK> ' which i found humorous but i <UNK> <UNK> is required to mind <UNK> with <UNK> at one point so that the alien can pilot the enterprise back to safety this is accomplished but when <UNK> <UNK> go back to end the mind <UNK> by <UNK> <UNK> <UNK> his <UNK> <UNK> oh he goes crazy but eventually <UNK> with the help of <UNK> assistant a blind woman with psychic powers this might have been a really bizarre excellent episode but it is poorly directed and comes across as yet one more badly executed show of the <UNK> last season\n",
            " y=0, y_pred=1, g(x)(H)=0.973, f(x)=0.900\n",
            "\n",
            "<START> sleeping with the enemy is a predictable <UNK> there <UNK> thriller that never seems to find any inspiration no matter how desperately cast and crew try i can't believe a bunch of my friends talked me into seeing this at the movies some <UNK> years ago br br the complete lack of originality from the <UNK> <UNK> screenplay based upon the nancy price novel does not help nor does the stale direction of joseph <UNK> or the very average performance from julia roberts the supporting cast including patrick <UNK> and kevin anderson do little to help br br there really isn't a lot to say just give it a miss br br sunday april 14 <UNK> <UNK> cinema <UNK> <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=0.991, f(x)=0.946\n",
            "\n",
            "<START> a friend once told me that an art house independent film ran in a cinema when upon the closing of the film audiences were so <UNK> they <UNK> to tear up the cinema <UNK> of course my imagination ran <UNK> trying to <UNK> up the <UNK> of such a piece of work well now my imagination can be put to rest br br i am a <UNK> <UNK> <UNK> fan and an <UNK> <UNK> of his work i have come across many people who thought <UNK> films are slow moving and <UNK> opinions being what they are i found this not to be true of the late director's wonderful works which are <UNK> with meaning beautiful <UNK> and complex philosophical questions upon hearing <UNK> <UNK> called the <UNK> to <UNK> i was excited to experience his films br br with the exception of the open air ride through the <UNK> <UNK> this movie has no <UNK> to anything <UNK> has done it does not seem to <UNK> the slightest meaning even on a completely mindless level it's supposedly <UNK> <UNK> cinematography is devoid of any <UNK> craft there is a no balance no <UNK> and the exposure <UNK> seems to be running low on <UNK> in the <UNK> snow the main character is so inept and <UNK> it makes you wonder whether his father might have been alive if he made up his mind <UNK> br br i am also not <UNK> to non plots or story lines that progress on multiple non <UNK> fashion but there isn't even a non story here one must surely enter the viewing of this film with a <UNK> head if one were to <UNK> it with nothing <UNK> and nothing lost as hair pulling would be the only possible answer to a pace that could make a <UNK> time <UNK> look as if jerry <UNK> had filmed a charlie chaplin short br br i won't rule out that this may be one of <UNK> <UNK> <UNK> <UNK> but to <UNK> that he is one of <UNK> <UNK> based on this film would be to call paris <UNK> the <UNK> to <UNK> <UNK> guys don't be afraid to say it no amount of big impressive words is going to <UNK> bring this corpse of celluloid back to life i don't <UNK> to fully understand russian culture and i probably don't have russian values but i immediately picked up on <UNK> work as something magical a treasure and a gift to viewers br br if it didn't have <UNK> name on it and it aired on say saturday night live i'm pretty sure nobody would read all these magnificent analysis into this wet <UNK> of a flick\n",
            " y=0, y_pred=1, g(x)(H)=0.965, f(x)=0.958\n",
            "\n",
            "<START> i grew up watching inspector gadget it was and still is one of my favorite cartoons if not my absolute favorite i learned a lot of <UNK> and history from the spin off inspector <UNK> field trip i wanted to <UNK> on a <UNK> <UNK> and become the greatest detective ever br br but the film has ruined the reputation of the wonderful cartoon br br matthew <UNK> an actor with potential was definitely not the role for inspector gadget first thing in the film inspector gadget is smart not so in the cartoon in the film gadget <UNK> the mystery mostly by himself in the cartoon it was almost always <UNK> brain and the awesome book i still want her book if gadget <UNK> the mystery it was by accident gadget in the film seems to be a competent detective but in the cartoon was pretty dumb which was where the humor came from br br another thing is that it's too much good guy v bad guy in the film it's not just meant to be a silly saturday morning cartoon also gadget never should have a love story but disney <UNK> is filled with idiots br br also i miss the true <UNK> that gadget had and especially the gadget car in the movie it was a <UNK> <UNK> in the cartoon it was a <UNK> police car and could turn into a van it also barely had any <UNK> and was mainly there to get him from place to place br br but if anything the one thing that was terrible about the movie was that it was a feature movie inspector gadget was a silly saturday morning cartoon the movie was too serious too overdone had too much of a plot and wasn't even remotely as funny br br <UNK> for those who haven't seen it never see it ever watch the cartoon it's a true classic\n",
            " y=0, y_pred=1, g(x)(H)=0.954, f(x)=0.943\n",
            "\n",
            "<START> trying to compare or represent this <UNK> as anything <UNK> is an out n out attempt to <UNK> hitchcock fans to waste 7 on this movie weak acting weak story weak script br br no real suspense no thrills you wait all through the <UNK> of this movie for the big <UNK> or even any <UNK> you're left thinking what the heck was that all about br br and please enough with the movement to make <UNK> <UNK> hip and politically correct i can't recommend this to anyone did i mention how weak the acting is williams did a better job as peter pan and <UNK> but those were much more innocent times\n",
            " y=0, y_pred=1, g(x)(H)=0.993, f(x)=1.011\n",
            "\n",
            "\n",
            "Top misclassfied instances picked by g(x)\n",
            "-----------------------------------------\n",
            "<START> when i saw the movie at first i thought that it was boring because nothing was happening but when all the scary things started to happen like when church dies and is brought back to life and also <UNK> and his mom die and there idiot dad has to bring them back to life even though he <UNK> the <UNK> and <UNK> <UNK> this is not steven <UNK> best work i thought that his best work was the shining i don't think that people who see this movie and comment on how awful it was are wrong because all they think is that what were they thinking as if that person can do a better job in making a horror flick i mean making the <UNK> evil and how he kills <UNK> is genius making the most innocent most <UNK> character into one of the killers is cool people who didn't like the movie are dumb because all it is a scary movie and nothing all don't expect something from a movie that it isn't it still in a general area wasn't that good i still recommend people to watch the movie\n",
            " y=1, y_pred=0, g(x)(H)=0.999, f(x)=0.717\n",
            "\n",
            "<START> wow saw this last night and i'm still <UNK> from how good it was every character felt so real although most of them petty selfish a holes and the bizarre story middle aged widow starts <UNK> her <UNK> <UNK> boyfriend felt utterly convincing top performances all round but <UNK> off to anne <UNK> and our friends in the <UNK> daniel craig the latter coming across as the next david <UNK> br br and director roger <UNK> this is as far from <UNK> hill as it's possible to be thank god br br watch this movie\n",
            " y=1, y_pred=0, g(x)(H)=0.999, f(x)=0.830\n",
            "\n",
            "<START> a good film and one i'll watch a number of times rich the previous <UNK> is right there is much more going on here than is clear from the title <UNK> and i have to wonder how much has suffered in translation were there more in the original or was a native language audience expected to <UNK> read more or since the screenplay was written by the author of the novel on which this was based was this a currently popular story with which the audience was already very familiar in short very worth a look but it probably requires more work from contemporary viewers than the original <UNK> audience had to put into it br br the <UNK> video release <UNK> the new <UNK> score but the music is not matched to the story <UNK> in any way sure it starts <UNK> but <UNK> into a repetitive <UNK> glass like <UNK> that <UNK> nothing of the action on the screen after listening for a while i turned off the sound and simply watched much better\n",
            " y=1, y_pred=0, g(x)(H)=0.999, f(x)=0.469\n",
            "\n",
            "<START> this is the only movie i have ever seen that has <UNK> me to write a <UNK> on any internet site and that is a significant statement from someone who likes the attack of the <UNK> monsters this movie is perfect for anyone who wants an <UNK> movie it is devoid of sex and violence for example i believe that this movie is safe for children of all ages this movie is perfect for anyone who does not want to be entertained <UNK> or <UNK> in any way adults could easily catch up on their sleep in front of the tv while the kids watch this movie don't be surprise however if you <UNK> to find the kids have turned the tv off and started a board game as an adult who enjoys being entertained who enjoys everything from the mundane to the fantastic in realism drama comedy and action all of those adult things that reflect real life on earth and or <UNK> the imagination this movie has nothing to offer\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.623\n",
            "\n",
            "handed the job on a silver <UNK> and has to do little to <UNK> his position unlike <UNK> <UNK> hits on all 5 families in <UNK> this hand me down process may be an authentic way of <UNK> power in a mafia family but why is so much made of this boring routine certainly don vincent may <UNK> the respect of his fellow gangsters <UNK> but there's little character <UNK> in the script to give us a portrait of this young man sadly <UNK> did such a fine job of quickly <UNK> developing <UNK> character in <UNK> but <UNK> has no <UNK> in it's story telling and we suffer through drawn out <UNK> until we just want to take a <UNK> <UNK> george hamilton was also handed a <UNK> task in taking over as family lawyer after <UNK> <UNK> <UNK> turned down a 3rd installment as tom <UNK> george <UNK> <UNK> his role so it came off without damage to the actor the development of <UNK> <UNK> character is interesting but it goes too far when she takes murderous matters into her own hands she could well end up sleeping with the <UNK> if this were the real world but it's not all bad the <UNK> scene in the hotel <UNK> is <UNK> also they keep pulling me back in or words to that effect is a great line and we <UNK> some old country feel as we get to go back to <UNK> even if it's all done in <UNK> and they've got modern cars <UNK> the plot lines involving the corrupt <UNK> of the catholic church are pretty interesting since it's based on some actual financial <UNK> at <UNK> <UNK> in the <UNK> but it's brought too far again with the too spectacular death scenes etc the grand opera scenes are very dramatic and well photographed but the death scene of mary toward the end is an <UNK> attempt of emotional <UNK> at best all <UNK> need to learn that we don't always need more and more death in order to bring a mafia movie to a successful conclusion you feel sorry for kay that her daughter is dead she plays her <UNK> so well but <UNK> reaction is <UNK> <UNK> <UNK> then michael dies <UNK> in the <UNK> century in <UNK> alone on a grand estate of heart failure no <UNK> to play with before his <UNK> no wife to <UNK> for him what should have happened in the last 1 2 of this movie is michael being <UNK> down by someone like <UNK> <UNK> put on trial all his dirty family's sleazy little <UNK> and <UNK> <UNK> brought into the light of day michael then <UNK> under <UNK> <UNK> and sent to <UNK> prison for life then he can <UNK> over dead from heart failure while <UNK> a floor in <UNK> that's how mafia <UNK> were ending their careers in the late 80's and early 90's and they got much better treatment than what they deserved\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.866\n",
            "\n",
            "<START> this film has a lot of strong points it has one of the best horror <UNK> outside of the lugosi karloff <UNK> circle <UNK> <UNK> fay <UNK> and <UNK> <UNK> plus leading man <UNK> douglas it's got all the right ingredients <UNK> a castle with lots of stone <UNK> a mad scientist <UNK> <UNK> <UNK> and hunting vampires an <UNK> type character a beautiful girl even a goofy <UNK> <UNK> the soft focus camera work is moody and imaginative there's even some good comic relief nicely <UNK> throughout the script br br but it's not really a monster movie because there is nothing supernatural going on in <UNK> little castle the plot revolves around the generic crazy scientist nicely played by <UNK> who values his work more highly than human lives br br it's not top <UNK> material because of a ho <UNK> resolution of the plot and some <UNK> bad dialog for <UNK> <UNK> but it's worth a look if you like early b w horror pictures\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.738\n",
            "\n",
            "<START> this movie delivers the best is when the awkward teenage neighbor tries to <UNK> away from the <UNK> and in the background looks like he's never been anywhere near a <UNK> in his life as he attempts not to fall off br br but this movie doesn't stop there when less than 5 minutes later it delivers a scene of nothing but an arm reaching through a <UNK> and into a <UNK> pulling out a beer br br stereotypical <UNK> <UNK> several plot lines that go nowhere and a former <UNK> actress with a <UNK> cell phone all add up to making this the perfect saturday night at home\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.686\n",
            "\n",
            "<START> i liked this show a lot we got the first and only it would appear series in the uk on channel 4 the <UNK> was right on the money a bit like the <UNK> in that all the different <UNK> of small town <UNK> were represented br br there was no laughter track i hadn't seen this on an american tv comedy at the time except for on larry <UNK> and it really worked well here <UNK> the <UNK> that these wacky cops were really like that and not just <UNK> it up for the cameras br br all in all a quirky little number that <UNK> me just right i can't help but think that maybe it missed it's mark with certain audiences i think it would have been a cult hit in the uk had it been shown at an acceptable hour br br i'll round this off with my standard comment where the hell can i get hold of this to watch it again any ideas\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.720\n",
            "\n",
            "<START> a blair witch war movie that is as much of a <UNK> as <UNK> was the title says it all save your money and your time and spend it on a good movie such as once upon a time in america the <UNK> redemption or enemy at the <UNK> if you want to watch a great war movie etc br br this movie if it were a baseball team and the major <UNK> were the <UNK> and single a was the <UNK> then this movie was high school ball it was filmed as if it were a high school drama club filming with their <UNK> old camera sure they went into a <UNK> area to make a film but i don't call that brave around here we call that plain stupid this is a pass all the way now go watch it and then you tell me what you think\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.639\n",
            "\n",
            "<START> this picture came out in <UNK> and it was the second in the three part series of the life of sheriff <UNK> <UNK> bo <UNK> takes over the role of sheriff <UNK> <UNK> and luke <UNK> plays the role of <UNK> <UNK> <UNK> the last that we saw sheriff <UNK> he was <UNK> in a hospital bed after him and his wife who was killed in <UNK> sunday morning drive after <UNK> <UNK> he goes after the men that killed his wife is <UNK> able to complete the revenge that he's after or does the mob try to take him out before he <UNK> the only thing that bother me about this picture that this was an actual true story how could you leave in a town with this kind of crime and yet don't do anything about it since there was real no name actress in this picture i can't give it 10 <UNK> stars but i can give 8\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.640\n",
            "\n",
            "<START> in a way this film reminded me of jumping jack flash remember <UNK> goldberg at the <UNK> machine <UNK> <UNK> out <UNK> <UNK> as blind <UNK> and <UNK> <UNK> <UNK> great moments captured on film for sure but the movie still kind of sucks right that's how i feel about rich in love a man <UNK> his wife sing for the first time post <UNK> teenagers talk about the nature of love albert <UNK> <UNK> ice <UNK> out of <UNK> and in another scene has a lovely <UNK> moment regarding his absent wife <UNK> <UNK> adds another colorful character to her acting <UNK> but there's only the <UNK> of a plot here and you can't wait for it to get moving only when ex go <UNK> charlotte <UNK> the <UNK> <UNK> up a great pop song does the picture wake up and then it's over br br this picture is the equivalent of a lazy <UNK> day in the deep american south\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.864\n",
            "\n",
            "<START> story of a good for nothing <UNK> and a sidekick singer who puts his words to music director danny <UNK> has lost none of his <UNK> for <UNK> in the <UNK> of humanity for characters but he has lost in this film the edge for creating inspiring and funny films <UNK> is painful to watch and barely <UNK> by the fact that it was made for tv\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.671\n",
            "\n",
            "<START> the only words you need fear more than joe don baker if your thinking of watching a film are <UNK> clark and if they are both there run for your life however this is a very funny film because they actually take themselves seriously it starts out bad and goes downhill from there repeated scenes the good the bad and the ugly like <UNK> will have you rolling on the floor with laughter yes he's the best <UNK> sheriff in texas <UNK> a mafia hit man to <UNK> as only he can he makes his own rules does things his own way all the while wearing cowboy <UNK> and <UNK> cowboy style you want to see a bad but funny film go ahead on its your move\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.835\n",
            "\n",
            "<START> this is a wonderfully written and well acted psychological drama it is not really a horror flick so those looking for something like the ring or the <UNK> will be disappointed what really surprised me about this film was the intelligence and subtle attention to detail in the plot and the effort made to be <UNK> consistent i also appreciated the absence of dr <UNK> <UNK> or new age <UNK> rather than <UNK> an <UNK> the filmmakers just told the story told it well and let the viewer think about it the <UNK> <UNK> were reminiscent of <UNK> <UNK> and amazingly effective br br a great example of how to make a good film on a small budget without big studios star actors big name directors this was far better than many of <UNK> films special effects or clever plot twists\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.823\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-thpd8rmTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_txt = \"drive/My Drive/NIPS2020/results/imdb/instances_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(output_text.getvalue()) \n",
        "txt.close() #to change file access modes"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX7WuJijg-Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "table_by_row_index = report_table_concat.groupby(report_table_concat.index)\n",
        "report_table_mean = table_by_row_index.mean()\n",
        "report_table_std = table_by_row_index.std()\n",
        "\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "table_by_row_index = report_criteria_concat.groupby(report_criteria_concat.index)\n",
        "report_criteria_mean = table_by_row_index.mean()\n",
        "report_criteria_std = table_by_row_index.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zDE3LuFiAvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8561aab0-af6f-4211-d048-a2abb77bf548"
      },
      "source": [
        "report_table_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rule</th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>eta</th>\n",
              "      <th>p_value</th>\n",
              "      <th>check</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00560</td>\n",
              "      <td>0.14192</td>\n",
              "      <td>3.786</td>\n",
              "      <td>0.820517</td>\n",
              "      <td>1.596467e-04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.1471</td>\n",
              "      <td>3.218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02472</td>\n",
              "      <td>0.12280</td>\n",
              "      <td>16.776</td>\n",
              "      <td>0.672585</td>\n",
              "      <td>1.438608e-20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.0247</td>\n",
              "      <td>0.1273</td>\n",
              "      <td>16.284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04568</td>\n",
              "      <td>0.10184</td>\n",
              "      <td>31.026</td>\n",
              "      <td>0.448278</td>\n",
              "      <td>2.111505e-39</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.1063</td>\n",
              "      <td>30.058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06064</td>\n",
              "      <td>0.08688</td>\n",
              "      <td>41.244</td>\n",
              "      <td>0.423398</td>\n",
              "      <td>1.133743e-41</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.0634</td>\n",
              "      <td>0.0886</td>\n",
              "      <td>41.738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.07376</td>\n",
              "      <td>0.07376</td>\n",
              "      <td>50.104</td>\n",
              "      <td>0.136592</td>\n",
              "      <td>5.266706e-43</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.0745</td>\n",
              "      <td>50.980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rule  rho_user  error_val  ...  error_test  L_test  %reduction_test\n",
              "0  1.06      0.01    0.00560  ...      0.0049  0.1471            3.218\n",
              "1  1.20      0.05    0.02472  ...      0.0247  0.1273           16.284\n",
              "2  0.84      0.10    0.04568  ...      0.0457  0.1063           30.058\n",
              "3  1.26      0.15    0.06064  ...      0.0634  0.0886           41.738\n",
              "4  0.12      0.20    0.07376  ...      0.0775  0.0745           50.980\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zgyJyu4iCBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cb39105f-bc61-49e2-e1ff-25a644cd23fe"
      },
      "source": [
        "report_criteria_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>thresh</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00488</td>\n",
              "      <td>0.14264</td>\n",
              "      <td>3.306</td>\n",
              "      <td>0.998604</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.1463</td>\n",
              "      <td>3.672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02320</td>\n",
              "      <td>0.12432</td>\n",
              "      <td>15.728</td>\n",
              "      <td>0.964553</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.0242</td>\n",
              "      <td>0.1278</td>\n",
              "      <td>15.878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04408</td>\n",
              "      <td>0.10344</td>\n",
              "      <td>29.916</td>\n",
              "      <td>0.885287</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.0452</td>\n",
              "      <td>0.1068</td>\n",
              "      <td>29.700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06088</td>\n",
              "      <td>0.08664</td>\n",
              "      <td>41.324</td>\n",
              "      <td>0.774637</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.0654</td>\n",
              "      <td>0.0866</td>\n",
              "      <td>43.020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.07608</td>\n",
              "      <td>0.07144</td>\n",
              "      <td>51.648</td>\n",
              "      <td>0.668580</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>54.300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rho_user  error_val    L_val  ...  error_test  L_test  %reduction_test\n",
              "0      0.01    0.00488  0.14264  ...      0.0057  0.1463            3.672\n",
              "1      0.05    0.02320  0.12432  ...      0.0242  0.1278           15.878\n",
              "2      0.10    0.04408  0.10344  ...      0.0452  0.1068           29.700\n",
              "3      0.15    0.06088  0.08664  ...      0.0654  0.0866           43.020\n",
              "4      0.20    0.07608  0.07144  ...      0.0825  0.0695           54.300\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neU88XQkAzny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4b792c43-3fdb-48ee-d89a-e251ef90f666"
      },
      "source": [
        "report_table_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rule</th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>eta</th>\n",
              "      <th>p_value</th>\n",
              "      <th>check</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.176010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>0.797076</td>\n",
              "      <td>0.219716</td>\n",
              "      <td>3.569270e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>1.190869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.104536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003025</td>\n",
              "      <td>0.007595</td>\n",
              "      <td>2.114800</td>\n",
              "      <td>0.264298</td>\n",
              "      <td>3.187349e-20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.002564</td>\n",
              "      <td>0.009425</td>\n",
              "      <td>1.828026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.221884</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.007857</td>\n",
              "      <td>2.533146</td>\n",
              "      <td>0.322871</td>\n",
              "      <td>4.721467e-39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.006535</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>3.697022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.047855</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004495</td>\n",
              "      <td>0.010188</td>\n",
              "      <td>4.406856</td>\n",
              "      <td>0.203426</td>\n",
              "      <td>2.432024e-41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008944</td>\n",
              "      <td>0.006628</td>\n",
              "      <td>0.008569</td>\n",
              "      <td>3.804684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.178885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.009045</td>\n",
              "      <td>4.302619</td>\n",
              "      <td>0.043197</td>\n",
              "      <td>1.177661e-42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.010186</td>\n",
              "      <td>0.009670</td>\n",
              "      <td>5.632673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       rule  rho_user  error_val  ...  error_test    L_test  %reduction_test\n",
              "0  1.176010       0.0   0.001265  ...    0.001817  0.009283         1.190869\n",
              "1  1.104536       0.0   0.003025  ...    0.002564  0.009425         1.828026\n",
              "2  1.221884       0.0   0.002918  ...    0.006535  0.008526         3.697022\n",
              "3  1.047855       0.0   0.004495  ...    0.006628  0.008569         3.804684\n",
              "4  0.178885       0.0   0.005127  ...    0.010186  0.009670         5.632673\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhjaMD-kKNMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "95ed1e63-79df-4e52-e414-e7a7d519ef96"
      },
      "source": [
        "report_criteria_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>thresh</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.659454</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.004472</td>\n",
              "      <td>0.003054</td>\n",
              "      <td>0.007032</td>\n",
              "      <td>1.738094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002871</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>1.788413</td>\n",
              "      <td>0.016886</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.007023</td>\n",
              "      <td>1.419972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>2.081797</td>\n",
              "      <td>0.053454</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.005251</td>\n",
              "      <td>0.006496</td>\n",
              "      <td>2.410187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004625</td>\n",
              "      <td>0.007785</td>\n",
              "      <td>3.379797</td>\n",
              "      <td>0.081735</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.005067</td>\n",
              "      <td>0.005878</td>\n",
              "      <td>1.744506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005370</td>\n",
              "      <td>0.008341</td>\n",
              "      <td>4.106479</td>\n",
              "      <td>0.093708</td>\n",
              "      <td>0.008944</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>0.006567</td>\n",
              "      <td>2.760860</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rho_user  error_val     L_val  ...  error_test    L_test  %reduction_test\n",
              "0       0.0   0.000996  0.007357  ...    0.003054  0.007032         1.738094\n",
              "1       0.0   0.002871  0.006832  ...    0.003347  0.007023         1.419972\n",
              "2       0.0   0.002834  0.007164  ...    0.005251  0.006496         2.410187\n",
              "3       0.0   0.004625  0.007785  ...    0.005067  0.005878         1.744506\n",
              "4       0.0   0.005370  0.008341  ...    0.006225  0.006567         2.760860\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NU73I2FKP6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}