{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "imdb_lstm",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesar-claros/synergistic/blob/master/imdb_lstm_AAAI.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r4sBUso-gM_4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install torch\n",
        "!pip install gpytorch\n",
        "!pip install tensorflow-determinism"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FymsszQy2n68",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "! sudo apt-get install texlive-latex-recommended #1\n",
        "! sudo apt-get install dvipng texlive-fonts-recommended #2\n",
        "! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip #3\n",
        "! unzip type1cm.zip -d /tmp/type1cm #4\n",
        "! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins  #5\n",
        "! sudo mkdir /usr/share/texmf/tex/latex/type1cm #6\n",
        "! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm #7\n",
        "! sudo texhash #8"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O0gzdUCWgQOt",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "7175b649-b849-4e8b-8690-c0fdf4eb3c08"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pODgafakgcZ9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!cp -r \"drive/My Drive/NIPS2020/auxfunc/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/datasets/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/style/\" .\n",
        "!cp -r \"drive/My Drive/NIPS2020/runs/\" ."
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZrowTWaDjHhX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aKBZk4trqzQV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Imports\n",
        "import io #Used as buffer\n",
        "import sys\n",
        "import matplotlib\n",
        "import tensorflow as tf # Keras model for MNIST \n",
        "# matplotlib.use('qt5Agg')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import auxfunc.funcs as sgn\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from scipy.stats import entropy, spearmanr\n",
        "from sklearn import model_selection, svm, ensemble, linear_model, pipeline, \\\n",
        "      tree, neighbors, discriminant_analysis, gaussian_process, preprocessing, impute, decomposition\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
        "plt.style.use(['ggplot','style/style.mplstyle'])\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPr-qGxurI6H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Define LSTM architecture\n",
        "def LSTM_model(top_words, emb_vector_length, max_review_length, objective, reg=0.01):\n",
        "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Embedding(top_words,emb_vector_length,input_length=max_review_length))\n",
        "    model.add(tf.keras.layers.LSTM(100))\n",
        "    if objective == 'svm':\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units=1, kernel_initializer='uniform', activation='linear', kernel_regularizer=tf.keras.regularizers.l2(reg)\n",
        "        ))\n",
        "    elif objective == 'softmax':\n",
        "        model.add(tf.keras.layers.Dense(\n",
        "            units=1, kernel_initializer='uniform', activation='sigmoid'\n",
        "        ))\n",
        "    return model"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KYYvRG4AmtEJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Signaling function fitting and evaluation\n",
        "def signalingFunction(X_train, y_train, y_train_pred_th, X_val, y_val, y_val_pred_th, X_test, y_test, y_test_pred_th, kernel='exponential', norm='l01', ex_dim=1):\n",
        "    # X_train, X_val should be scaled\n",
        "    # Fit signaling function \n",
        "    exp = sgn.signaling(norm=norm) # idx = [train,test,val]\n",
        "    exp.fit(X_train, y_train, y_train_pred_th, kernel=kernel, n_iter=500, lr=0.01, ex_dim=ex_dim)\n",
        "    table_val = exp.evaluate(X_val, y_val, y_val_pred_th, rule_grid=np.linspace(0,3,30, endpoint=False), rho_grid=[0.1, 0.15])\n",
        "    table_test = exp.test(X_test, y_test, y_test_pred_th, table_val['rule'].to_numpy(), table_val['eta'].to_numpy())\n",
        "    table = pd.concat([table_val,table_test],axis=1)\n",
        "    return table, exp"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRVYBv5lxPJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Initialize model\n",
        "def init_model(input_dim, objective):\n",
        "    model = LSTM_model(top_words, emb_vector_length, max_review_length, objective)\n",
        "    if objective=='svm':\n",
        "        loss = tf.keras.losses.hinge\n",
        "        metric = ['hinge']\n",
        "\n",
        "    elif objective=='softmax':\n",
        "        loss = tf.keras.losses.binary_crossentropy\n",
        "        metric = ['accuracy']\n",
        "\n",
        "    model.compile(loss=loss,\n",
        "                optimizer=tf.keras.optimizers.Adam(),\n",
        "                metrics=metric)\n",
        "    # model.summary()\n",
        "    print('loss={}'.format(loss.__name__))\n",
        "    return model"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMK50NRixYI9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Soft and thresholded output predictions\n",
        "def pred_output(model, X, objective):\n",
        "    if objective=='svm':\n",
        "        y_pred_soft = model.predict(X)\n",
        "        y_pred_th = np.array([1 if i[0] >= 0 else 0 for i in y_pred_soft])\n",
        "    elif objective=='softmax':\n",
        "        y_pred_soft = model.predict(X)\n",
        "        y_pred_th = np.array([1 if i[0] > 0.5 else 0 for i in y_pred_soft])\n",
        "    return y_pred_soft, y_pred_th"
      ],
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8nf3lDBxjZcW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Jaccard similarity index\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vyWbCZTIjbiR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Baseline comparison\n",
        "def baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf):\n",
        "      if clf=='svm':\n",
        "          direction = 'closer'\n",
        "          crit_val = np.abs(y_val_pred_soft.ravel())\n",
        "          crit_test = np.abs(y_test_pred_soft.ravel())\n",
        "      else:\n",
        "          direction = 'further'\n",
        "          p_val = np.concatenate((y_val_pred_soft,1-y_val_pred_soft),axis=1)\n",
        "          crit_val = entropy(p_val, axis=1, base=2)\n",
        "          p_test = np.concatenate((y_test_pred_soft,1-y_test_pred_soft),axis=1)\n",
        "          crit_test = entropy(p_test, axis=1, base=2)\n",
        "      \n",
        "      critFunc = sgn.critEvaluation(norm='l01',direction=direction)\n",
        "      d_val = critFunc.evaluate(y_val, y_val_pred_th, crit_val, rho_grid=[0.1, 0.15])\n",
        "      d_test = critFunc.test(y_test, y_test_pred_th, crit_test, d_val['thresh'].to_numpy())\n",
        "      crit_table = pd.concat([d_val,d_test],axis=1)\n",
        "\n",
        "      gamma = table['rule'].to_numpy().reshape(-1,1)\n",
        "      f_test = exp.gpr_mean_test + gamma*np.sqrt(exp.gpr_var_test)\n",
        "      eta = table['eta'].to_numpy().reshape(-1,1)\n",
        "      theta = crit_table['thresh'].to_numpy().reshape(-1,1)\n",
        "      if direction == 'closer':\n",
        "        f_mask, f_idx = np.nonzero(f_test>eta)\n",
        "      else:\n",
        "        f_mask, f_idx = np.nonzero(f_test<eta)\n",
        "      crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)<theta)\n",
        "      print(list(np.unique(f_mask)))\n",
        "      print(list(np.unique(crit_mask)))\n",
        "      print(f_test.shape[0])\n",
        "      shared = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask))))\n",
        "      J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan for i in range(f_test.shape[0])]\n",
        "      # if (list(np.unique(f_mask))==list(np.unique(crit_mask))):\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) for i in np.unique(f_mask)]\n",
        "      # else:\n",
        "      #   shared = set(a).intersection(set(b))\n",
        "      #   union = set(a).union(set(b))\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan  for i in union]\n",
        "      crit_table['jaccard']=J\n",
        "      Sp = [spearmanr(f_test[i,:],crit_test)[0] for i in range(f_test.shape[0])]\n",
        "      crit_table['spearman'] = Sp\n",
        "      crit_table['gamma'] = gamma\n",
        "      return crit_table"
      ],
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W06zPRQXtMBp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# For reproducibility\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "SEED = 123456\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3VCXn3_rTYj",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "45687772-4924-4032-c355-a37b24d0f34c"
      },
      "source": [
        "# %%\n",
        "# INITIALIZATION\n",
        "# ==============\n",
        "# EXPERIMENT SETUP\n",
        "# ================\n",
        "\n",
        "top_words = 5000\n",
        "max_review_length = 500\n",
        "emb_vector_length = 32\n",
        "# Load data set\n",
        "(Data_X, Data_y), (X_test, y_test) = tf.keras.datasets.imdb.load_data(num_words=top_words)\n",
        "Data_X = tf.keras.preprocessing.sequence.pad_sequences(Data_X, maxlen=max_review_length)\n",
        "X_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=max_review_length)\n",
        "print(\"Number of original training examples:\", len(Data_X))\n",
        "Data_X, Data_X_sep, Data_y, Data_y_sep = model_selection.train_test_split(Data_X, Data_y, stratify=Data_y, test_size=0.5, random_state=SEED)\n",
        "X_test, X_test_sep, y_test, y_test_sep = model_selection.train_test_split(X_test, y_test, stratify=y_test, test_size=0.9, random_state=SEED)\n",
        "print(Data_X.shape)\n",
        "print(X_test.shape)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of original training examples: 25000\n",
            "(12500, 500)\n",
            "(2500, 500)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jsIZNLPri8D",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "d10e56ac-3228-4c45-b372-f4e8713e2378"
      },
      "source": [
        "#%%\n",
        "# Assign labels\n",
        "report_table = []\n",
        "report_criteria = []\n",
        "report_plot = []\n",
        "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "# kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=123)\n",
        "clf = 'softmax'\n",
        "addPredictions = True\n",
        "accuracy = 0\n",
        "for train, val in kf.split(Data_X, Data_y):\n",
        "# for sample, test in kf.split(Data_X):\n",
        "    X_train = Data_X[train]\n",
        "    y_train = Data_y[train]\n",
        "    # print(y_train[:10])\n",
        "    # y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    X_val = Data_X[val]\n",
        "    y_val = Data_y[val]\n",
        "    # print(y_val[:10])\n",
        "    # y_val = tf.keras.utils.to_categorical(y_val)\n",
        "\n",
        "    # TRAINING MODEL\n",
        "    model = init_model(input_dim=Data_X.shape[1:], objective=clf)\n",
        "    model.fit(X_train, y_train, batch_size=64, epochs=5, verbose=0, validation_data=(X_val, y_val))\n",
        "    # X_test = scaleX.transform(imputeX.transform(X_test))\n",
        "\n",
        "    y_train_pred_soft, y_train_pred_th = pred_output(model, X_train, clf)\n",
        "    print('accuracy(Train)={}'.format(np.sum(y_train==y_train_pred_th)/np.size(y_train)))\n",
        "    y_val_pred_soft, y_val_pred_th = pred_output(model, X_val, clf)\n",
        "    # print('accuracy(Val)={}'.format(np.sum(y_val==y_val_pred_th)/np.size(y_val)))\n",
        "    y_test_pred_soft, y_test_pred_th = pred_output(model, X_test, clf)\n",
        "\n",
        "    layer_outputs = [layer.output for layer in model.layers] \n",
        "    activation_model = tf.keras.models.Model(inputs=model.input, outputs=layer_outputs)\n",
        "    X_train_GP = activation_model.predict(X_train)[1]\n",
        "    X_val_GP = activation_model.predict(X_val)[1]\n",
        "    X_test_GP = activation_model.predict(X_test)[1]\n",
        "\n",
        "    if addPredictions:\n",
        "            # Add predictions\n",
        "            X_train_GP = np.concatenate((X_train_GP, y_train_pred_soft), axis=1)\n",
        "            X_val_GP = np.concatenate((X_val_GP, y_val_pred_soft), axis=1)\n",
        "            X_test_GP = np.concatenate((X_test_GP, y_test_pred_soft), axis=1)\n",
        "    scaleX_GP = preprocessing.StandardScaler().fit(np.concatenate((X_train_GP, X_val_GP), axis=0))\n",
        "    X_train_GP = scaleX_GP.transform(X_train_GP)\n",
        "    X_val_GP = scaleX_GP.transform(X_val_GP)\n",
        "    X_test_GP = scaleX_GP.transform(X_test_GP)\n",
        "\n",
        "    table, exp = signalingFunction(X_train_GP, y_train, y_train_pred_th,\n",
        "                                   X_val_GP, y_val, y_val_pred_th,\n",
        "                                   X_test_GP, y_test, y_test_pred_th,\n",
        "                                   kernel='e*e', ex_dim=y_train_pred_soft.shape[1])\n",
        "    report_table.append(table)\n",
        "    # Baseline for comparison\n",
        "    crit_table = baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf)\n",
        "    report_criteria.append(crit_table)\n",
        "\n",
        "    score = np.sum(y_val==y_val_pred_th)/np.size(y_val)\n",
        "    if accuracy < score:\n",
        "      accuracy = score\n",
        "      table_best = table\n",
        "      crit_table_best = crit_table\n",
        "      exp_best = exp\n",
        "      y_test_best = y_test\n",
        "      y_test_pred_soft_best = y_test_pred_soft\n",
        "      y_test_pred_th_best = y_test_pred_th\n",
        "      X_test_best = X_test\n",
        "    del(model)"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9487\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 492/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 493/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 494/500 - Loss: -0.201  noise: 0.032\n",
            "Iter 495/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 496/500 - Loss: -0.201  noise: 0.032\n",
            "Iter 497/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 498/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 499/500 - Loss: -0.202  noise: 0.032\n",
            "Iter 500/500 - Loss: -0.202  noise: 0.032\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9445\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.191  noise: 0.033\n",
            "Iter 492/500 - Loss: -0.189  noise: 0.033\n",
            "Iter 493/500 - Loss: -0.189  noise: 0.033\n",
            "Iter 494/500 - Loss: -0.190  noise: 0.033\n",
            "Iter 495/500 - Loss: -0.190  noise: 0.033\n",
            "Iter 496/500 - Loss: -0.192  noise: 0.033\n",
            "Iter 497/500 - Loss: -0.190  noise: 0.033\n",
            "Iter 498/500 - Loss: -0.191  noise: 0.033\n",
            "Iter 499/500 - Loss: -0.190  noise: 0.033\n",
            "Iter 500/500 - Loss: -0.190  noise: 0.033\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.8591\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: 0.101  noise: 0.055\n",
            "Iter 492/500 - Loss: 0.100  noise: 0.055\n",
            "Iter 493/500 - Loss: 0.099  noise: 0.055\n",
            "Iter 494/500 - Loss: 0.101  noise: 0.055\n",
            "Iter 495/500 - Loss: 0.101  noise: 0.055\n",
            "Iter 496/500 - Loss: 0.100  noise: 0.055\n",
            "Iter 497/500 - Loss: 0.100  noise: 0.055\n",
            "Iter 498/500 - Loss: 0.100  noise: 0.055\n",
            "Iter 499/500 - Loss: 0.101  noise: 0.055\n",
            "Iter 500/500 - Loss: 0.099  noise: 0.055\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9382\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.213  noise: 0.029\n",
            "Iter 492/500 - Loss: -0.214  noise: 0.029\n",
            "Iter 493/500 - Loss: -0.213  noise: 0.029\n",
            "Iter 494/500 - Loss: -0.214  noise: 0.029\n",
            "Iter 495/500 - Loss: -0.214  noise: 0.029\n",
            "Iter 496/500 - Loss: -0.212  noise: 0.029\n",
            "Iter 497/500 - Loss: -0.214  noise: 0.029\n",
            "Iter 498/500 - Loss: -0.216  noise: 0.029\n",
            "Iter 499/500 - Loss: -0.216  noise: 0.029\n",
            "Iter 500/500 - Loss: -0.214  noise: 0.029\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n",
            "loss=binary_crossentropy\n",
            "accuracy(Train)=0.9269\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.057  noise: 0.043\n",
            "Iter 492/500 - Loss: -0.057  noise: 0.043\n",
            "Iter 493/500 - Loss: -0.056  noise: 0.043\n",
            "Iter 494/500 - Loss: -0.056  noise: 0.043\n",
            "Iter 495/500 - Loss: -0.055  noise: 0.043\n",
            "Iter 496/500 - Loss: -0.057  noise: 0.043\n",
            "Iter 497/500 - Loss: -0.056  noise: 0.043\n",
            "Iter 498/500 - Loss: -0.057  noise: 0.043\n",
            "Iter 499/500 - Loss: -0.057  noise: 0.043\n",
            "Iter 500/500 - Loss: -0.057  noise: 0.043\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WWPUAct1q2Hm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "logs_base_dir = \"runs/\"\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UZzb12DMxKfq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 182
        },
        "outputId": "b7e50992-7b81-4f22-abd6-609319a9133d"
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "# report_table_concat\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "# report_criteria_concat\n",
        "# p-value column\n",
        "p_value_col = report_table_concat['p_value'] #Add\n",
        "cols_CQT = ['rho_user','corrected_test','queries_test','total_wrong_test','loss_query_test']\n",
        "cols_rholoss = ['rho_user','rho_hat_test','%loss_red_test']\n",
        "\n",
        "# Dataframes for f(x)\n",
        "df_fx_CQT = pd.DataFrame(report_table_concat[cols_CQT])\n",
        "df_fx_rholoss = pd.DataFrame(report_table_concat[cols_rholoss])\n",
        "# results_fx_CQT = df_fx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_rholoss = df_fx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_CQT = df_fx_CQT.copy()\n",
        "# results_fx_rholoss = df_fx_rholoss.copy()\n",
        "\n",
        "results_fx_rholoss_bri = results_fx_rholoss.groupby(results_fx_rholoss.index)\n",
        "fx_rholoss_median = results_fx_rholoss_bri.median()\n",
        "fx_rholoss_q1 = results_fx_rholoss_bri.quantile(q=0)\n",
        "fx_rholoss_q3 = results_fx_rholoss_bri.quantile(q=1)\n",
        "results_fx_CQT_bri = results_fx_CQT.groupby(results_fx_CQT.index)\n",
        "fx_CQT_median = results_fx_CQT_bri.median()\n",
        "fx_CQT_q1 = results_fx_CQT_bri.quantile(q=0)\n",
        "fx_CQT_q3 = results_fx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Dataframes for g(x)\n",
        "df_gx_CQT = pd.DataFrame(report_criteria_concat[cols_CQT])\n",
        "df_gx_rholoss = pd.DataFrame(report_criteria_concat[cols_rholoss])\n",
        "# results_gx_CQT = df_gx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_rholoss = df_gx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_CQT = df_gx_CQT.copy()\n",
        "# results_gx_rholoss = df_gx_rholoss.copy()\n",
        "\n",
        "results_gx_rholoss_bri = results_gx_rholoss.groupby(results_gx_rholoss.index)\n",
        "gx_rholoss_median = results_gx_rholoss_bri.median()\n",
        "gx_rholoss_q1 = results_gx_rholoss_bri.quantile(q=0)\n",
        "gx_rholoss_q3 = results_gx_rholoss_bri.quantile(q=1)\n",
        "results_gx_CQT_bri = results_gx_CQT.groupby(results_gx_CQT.index)\n",
        "gx_CQT_median = results_gx_CQT_bri.median()\n",
        "gx_CQT_q1 = results_gx_CQT_bri.quantile(q=0)\n",
        "gx_CQT_q3 = results_gx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_test = io.StringIO()\n",
        "# numRows = fx_median.shape[0]\n",
        "# numCols = fx_median.shape[1]\n",
        "output_test.write(\"results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\\n\")\n",
        "output_test.write(\"----------\\n\")\n",
        "\n",
        "for rho in [0.10,0.15]:\n",
        "  # output_test.write(\"rho={:.2f}\\\\\\\\\\n\".format(rho))\n",
        "\n",
        "  fx_CQT_filtered = results_fx_CQT.loc[results_fx_CQT['rho_user']==rho]\n",
        "  gx_CQT_filtered = results_gx_CQT.loc[results_gx_CQT['rho_user']==rho]\n",
        "  p_value_filtered = p_value_col[results_fx_CQT['rho_user']==rho]\n",
        "  # print(p_value_filtered)\n",
        "  n_folds = fx_CQT_filtered.shape[0]\n",
        "  row_fx = [' ', ' ',' ',r'{:.2f}'.format(rho),r'$f(x)$']\n",
        "  row_gx = [' ',' ',' ',' ',r'$g(x)$']\n",
        "\n",
        "  row_fx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_fx_rho = [r'{:.2f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_fx_loss = [r'{:.1f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_fx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q3.loc[fx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q1.loc[fx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_fx = row_fx + row_fx_CQT + row_fx_rho + row_fx_loss + row_fx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_fx)))\n",
        "\n",
        "  row_gx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_gx_rho = [r'{:.2f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_gx_loss = [r'{:.1f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_gx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q3.loc[gx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q1.loc[gx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_gx = row_gx + row_gx_CQT + row_gx_rho + row_gx_loss + row_gx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_gx)))\n",
        "  output_test.write(\"\\\\cline{4-9}\\n\")\n",
        "\n",
        "\n",
        "print(output_test.getvalue())"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\n",
            "----------\n",
            "  &   &   & 0.10 & $f(x)$ & (253,119,402) & 0.10 & 30.8 & 0.44(0.67-0.44)\\\\\n",
            "  &   &   &   & $g(x)$ & (258,108,402) & 0.10 & 25.4 & 0.43(0.46-0.42)\\\\\n",
            "\\cline{4-9}\n",
            "  &   &   & 0.15 & $f(x)$ & (363,156,402) & 0.15 & 40.3 & 0.42(0.60-0.41)\\\\\n",
            "  &   &   &   & $g(x)$ & (381,154,402) & 0.15 & 38.2 & 0.42(0.45-0.40)\\\\\n",
            "\\cline{4-9}\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-lNsdkAGq5Re",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "##%\n",
        "# Boxplot (loss reduction in test set)\n",
        "report_table_concat = pd.concat(report_table)\n",
        "cols_table = ['p_value','rho_user','%reduction_test']\n",
        "df_boxplot_table = pd.DataFrame(report_table_concat[cols_table])\n",
        "df_boxplot_table['label'] = df_boxplot_table.shape[0]*['$f(x)$']\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "columns_crit = ['rho_user','%reduction_test']\n",
        "df_boxplot_crit = pd.DataFrame(report_criteria_concat[columns_crit])\n",
        "df_boxplot_crit['label'] = df_boxplot_crit.shape[0]*['$g(x)$']\n",
        "# p-value median\n",
        "p_value_col = df_boxplot_table['p_value'] #Add\n",
        "p_value_by_row_index = df_boxplot_table['p_value'].groupby(df_boxplot_table.index)\n",
        "p_value_median = p_value_by_row_index.median()\n",
        "# Boxplot (jaccard index in test set)\n",
        "columns_jac = ['rho_user','jaccard']\n",
        "df_jaccard = pd.DataFrame(report_criteria_concat[columns_jac])\n",
        "# Unfiltered Result dataframes\n",
        "cols_fx = ['rho_user','%reduction_val','budget','%reduction_test']\n",
        "results_fx = pd.DataFrame(report_table_concat[cols_fx])\n",
        "cols_fxgx = ['rho_user','%reduction_test', 'jaccard']\n",
        "results_fxgx = pd.concat([df_boxplot_table[cols_fxgx[:2]], df_boxplot_crit[cols_fxgx[1]], df_jaccard[cols_fxgx[2]]], axis=1)\n",
        "# Filter experiments with p_value > 0.05\n",
        "df_boxplot_crit = df_boxplot_crit.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_jaccard = df_jaccard.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_boxplot_table = df_boxplot_table.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "# Boxplot with filtered values only\n",
        "frames = [df_boxplot_table, df_boxplot_crit]\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OumIYSVNq7Zj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Avoid plotting when median(p_value)>0.5\n",
        "for i in range(p_value_median.shape[0]):\n",
        "    if p_value_median.iloc[i]>0.05:\n",
        "      df.loc[df.index==i,'%reduction_test'] = np.nan\n",
        "      df_jaccard.loc[df_jaccard.index==i, 'jaccard'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-6d4xyQvq952",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "6c88ed3b-f0f0-487c-862e-9c2ede76beee"
      },
      "source": [
        "# Dataframe for results f(x)\n",
        "results_fx = results_fx.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_by_row_index = results_fx.groupby(results_fx.index)\n",
        "fx_median = results_fx_by_row_index.median()\n",
        "fx_q1 = results_fx_by_row_index.quantile(q=0.25)\n",
        "fx_q3 = results_fx_by_row_index.quantile(q=0.75)\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_fx = io.StringIO()\n",
        "numRows = fx_median.shape[0]\n",
        "numCols = fx_median.shape[1]\n",
        "output_fx.write(\"results_fx (\\\\rho|%reduction_val|sig_rate|%reduction_test|H0)\\n\")\n",
        "output_fx.write(\"----------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==2)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fx_median.iloc[i],fx_q1.iloc[i],fx_q3.iloc[i],range(numCols))]\n",
        "  output_fx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fx.getvalue())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results_fx (\\rho|%reduction_val|sig_rate|%reduction_test|H0)\n",
            "----------\n",
            "{} & {} & 0.01 & 3.7(3.5-3.7) & 0.01(0.01-0.01) & 3.1(3.0-3.4) & $\\surd$ \\\\\n",
            "{} & {} & 0.05 & 15.0(14.1-15.7) & 0.05(0.05-0.05) & 14.6(13.9-16.9) & $\\surd$ \\\\\n",
            "{} & {} & 0.10 & 28.9(26.8-29.2) & 0.10(0.09-0.10) & 28.8(26.2-29.1) & $\\surd$ \\\\\n",
            "{} & {} & 0.15 & 38.8(37.0-40.9) & 0.15(0.15-0.16) & 39.0(36.9-41.7) & $\\surd$ \\\\\n",
            "{} & {} & 0.20 & 46.3(45.4-48.1) & 0.20(0.20-0.20) & 47.5(46.4-48.5) & $\\surd$ \\\\\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RjL-JtRhrC6e",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 164
        },
        "outputId": "51d98ab1-0290-4384-ecc5-bf29bbfbf3c3"
      },
      "source": [
        "# Dataframe for comparison f(x)-g(x)\n",
        "results_fxgx = results_fxgx.loc[p_value_col <= 0.05].copy()\n",
        "results_fxgx_by_row_index = results_fxgx.groupby(results_fxgx.index)\n",
        "fxgx_median = results_fxgx_by_row_index.median()\n",
        "fxgx_q1 = results_fxgx_by_row_index.quantile(q=0.25)\n",
        "fxgx_q3 = results_fxgx_by_row_index.quantile(q=0.75)\n",
        "# Baseline comparison statistics (median(q1-q3)) LaTex\n",
        "output_fxgx = io.StringIO()\n",
        "numRows = fxgx_median.shape[0]\n",
        "numCols = fxgx_median.shape[1]\n",
        "output_fxgx.write(\"results_fxgx (\\\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\\n\")\n",
        "output_fxgx.write(\"------------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==3)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fxgx_median.iloc[i],fxgx_q1.iloc[i],fxgx_q3.iloc[i],range(numCols))]\n",
        "  output_fxgx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fxgx.getvalue())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "results_fxgx (\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\n",
            "------------\n",
            "{} & {} & 0.01 & 3.1(3.0-3.4) & 3.0(2.8-3.7) & 0.98(0.98-0.98) & $\\surd$ \\\\\n",
            "{} & {} & 0.05 & 14.6(13.9-16.9) & 14.7(12.9-17.9) & 0.96(0.94-0.96) & $\\surd$ \\\\\n",
            "{} & {} & 0.10 & 28.8(26.2-29.1) & 27.6(26.1-28.2) & 0.95(0.92-0.96) & $\\surd$ \\\\\n",
            "{} & {} & 0.15 & 39.0(36.9-41.7) & 40.5(37.2-41.1) & 0.92(0.90-0.93) & $\\surd$ \\\\\n",
            "{} & {} & 0.20 & 47.5(46.4-48.5) & 48.3(44.9-48.8) & 0.90(0.89-0.90) & $\\surd$ \\\\\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G0X7KJ6trNGc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#%%\n",
        "# Save results in csv fomat\n",
        "path_csv = \"drive/My Drive/NIPS2020/results/imdb/results_{clf}_yhat{yhat}_pca{pca}.csv\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "results = pd.concat([results_fx, results_fxgx, p_value_col.loc[p_value_col <= 0.05]], keys=['fx', 'fxgx', ''], axis=1).to_csv(path_csv, index=True, header=True)\n",
        "# modified output\n",
        "# Save results in tex fomat\n",
        "L = [output_fx.getvalue(),output_fxgx.getvalue()]\n",
        "path_txt = \"drive/My Drive/NIPS2020/results/imdb/results_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(L) \n",
        "txt.close() #to change file access modes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0LTFKkXsrQOl",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 419
        },
        "outputId": "390c6bd5-939c-4023-bc9f-b7312fbce918"
      },
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(15, 5.1), constrained_layout=False, dpi=90)\n",
        "pal = sns.color_palette('Paired')\n",
        "sns.boxplot(x=df['rho_user'], y=df['%reduction_test'], hue='label', data=df, ax=ax[0], palette=pal)\n",
        "ax[0].set_xlabel(r'budget $\\rho$')\n",
        "ax[0].set_ylabel(r'Loss reduction $r_{test}(\\%)$')\n",
        "ax[0].legend(loc='upper left')\n",
        "pal = sns.color_palette('BuGn_r')\n",
        "sns.boxplot(x=df_jaccard['rho_user'], y=df_jaccard['jaccard'], data=df_jaccard, ax=ax[1], palette=pal)\n",
        "ax[1].set_xlabel(r'budget $\\rho$')\n",
        "ax[1].set_ylabel(r'Jaccard index $J$')\n",
        "plt.tight_layout()\n",
        "path_fig_fxgx = \"drive/My Drive/NIPS2020/results/imdb/fig_fxgx_{clf}_yhat{yhat}_pca{pca}.pdf\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fxgx, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 1350x459 with 2 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SlQtQPDGFE9w",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#reverse lookup\n",
        "INDEX_FROM = 3\n",
        "word_to_id = tf.keras.datasets.imdb.get_word_index()\n",
        "word_to_id = {k:(v+INDEX_FROM) for k,v in word_to_id.items()}\n",
        "word_to_id[\"<PAD>\"] = 0\n",
        "word_to_id[\"<START>\"] = 1\n",
        "word_to_id[\"<UNK>\"] = 2\n",
        "id_to_word = {value:key for key,value in word_to_id.items()}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj6x1QVqcRgO",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "b7e448f1-a6d8-4bb3-c4ab-94da5324e0f8"
      },
      "source": [
        "rho = 0.15\n",
        "rule = table_best.loc[table_best.rho_user == rho]['rule'].to_numpy()\n",
        "eta = table_best.loc[table_best.rho_user == rho]['eta'].to_numpy()[0]\n",
        "theta = crit_table_best.loc[crit_table_best.rho_user == rho]['thresh'].to_numpy()[0]\n",
        "f_test = exp_best.gpr_mean_test+rule*np.sqrt(exp_best.gpr_var_test)\n",
        "top_n = 10 # Top n selected instances in test set\n",
        "top_f_idx = np.argpartition(f_test, -top_n)[-top_n:]\n",
        "top_f_idx = top_f_idx[np.argsort(f_test[top_f_idx])]# Added\n",
        "if clf=='svm':\n",
        "    crit_test = np.abs(y_test_pred_soft_best.ravel())\n",
        "    top_crit_idx = np.argpartition(crit_test, top_n)[:top_n]\n",
        "    top_crit_idx = top_crit_idx[np.argsort(crit_test[top_crit_idx])]# Added\n",
        "elif clf=='softmax':\n",
        "    p_test = np.concatenate((y_test_pred_soft_best,1-y_test_pred_soft_best),axis=1)\n",
        "    crit_test = entropy(p_test, axis=1, base=2)\n",
        "    top_crit_idx = np.argpartition(crit_test, -top_n)[-top_n:]\n",
        "    top_crit_idx = top_crit_idx[np.argsort(crit_test[top_crit_idx])]# Added\n",
        "output_text = io.StringIO()\n",
        "print('eta={:.3f}, theta={:.3f}'.format(eta,theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "eta=0.149, theta=0.767\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xGqv4gVyc1kF",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 368
        },
        "outputId": "803cb48d-ddce-4e06-8f46-dad731fa2f9a"
      },
      "source": [
        "output_text.write(\"top_n={}, rho_user={}, g(x)={}, addh_hat={}, PCA={}\\n\".format(top_n,rho,clf, addPredictions, applyPCA))\n",
        "output_text.write(\"|f(x)>eta|={}(eta={:.3f}), |g(x)>theta|={}(theta={:.3f})\\n\".format(np.sum(f_test>eta), eta, np.sum(crit_test<theta) if clf=='svm' else np.sum(crit_test>theta), theta))\n",
        "output_text.write(\"\\nTop misclassfied instances picked by f(x)\\n\")\n",
        "output_text.write(\"-----------------------------------------\\n\")\n",
        "for i in top_f_idx:\n",
        "  cond = f_test[i]>eta\n",
        "  if y_test_best[i] != y_test_pred_th_best[i] and cond:\n",
        "    output_text.write(' '.join([id_to_word[id] for id in X_test_best[i,:] if id!=0])+\\\n",
        "        '\\n y={}'.format(y_test_best[i])+', y_pred={}'.format(y_test_pred_th_best[i])+\\\n",
        "        ', g(x)({})={:.3f}'.format('D' if clf=='svm' else 'H', crit_test[i])+\\\n",
        "        ', f(x)={:.3f}'.format(f_test[i])+'\\n\\n')\n",
        "print(output_text.getvalue())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top_n=10, rho_user=0.15, g(x)=softmax, addh_hat=True, PCA=True\n",
            "|f(x)>eta|=309(eta=0.149), |g(x)>theta|=320(theta=0.767)\n",
            "\n",
            "Top misclassfied instances picked by f(x)\n",
            "-----------------------------------------\n",
            "<START> i <UNK> to see the film at the recent <UNK> film festival because it had been <UNK> to the <UNK> film <UNK> <UNK> competition section i was surprised that <UNK> could be so off the mark in judging quality br br the film some reviewers have noted does not have too much of <UNK> but the <UNK> violence is <UNK> imagine killing your enemy <UNK> in front of your young son or <UNK> someone to eat a <UNK> <UNK> to prove <UNK> there are some hints of the <UNK> <UNK> sons in <UNK> godfather that seem to <UNK> here in this chinese hong kong film but the quality of the two are as <UNK> different as <UNK> and cheese br br this film is only recommended for violence <UNK> there is no great cinema here at best it might be considered to be better than the usual run run <UNK> production for production values\n",
            " y=0, y_pred=1, g(x)(H)=0.999, f(x)=0.585\n",
            "\n",
            "<START> i found this film to be quite an <UNK> from the very get go i found it extremely hard to like this movie and now after a little thinking about it i can pretty much <UNK> the reason why jean <UNK> <UNK> although i love him to bits i think <UNK> is one of the best movies ever made is quite miscast here and although i can't figure for the life of me who would be better i am sure someone could have taken his place quite easily and make this film work everything else is fine except for the <UNK> at weak comedy a meet the parents joke is not really needed filmmakers and i really like richard e grant as the british major it just suffers from one thing jean <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=0.975, f(x)=0.594\n",
            "\n",
            "and proud of his success in mainstream hollywood br br in another hand i see most of the reviews focuses on their favorite and not so short films but we are <UNK> that there is a subtle bottom line that <UNK> the whole <UNK> and maybe it will not be so pleasant for american people even if that was not the main purpose of the producers br br what i'm talking about is that most of the short films does not show the suffering that <UNK> people went through because the terrorist attack on september <UNK> but the suffering of the other people br br do you need <UNK> about what i'm saying look in the <UNK> short film the message is you cry because of the people who died in the <UNK> but we the others east <UNK> are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you br br even though the <UNK> <UNK> story is more in comedy there is a the same thought you are angry because <UNK> bin <UNK> <UNK> you in an evil way but we the others <UNK> should be more angry because our people is dying of <UNK> poverty and aids long time ago and nobody pay attention to us like the whole world has done to you br br look now at the sean <UNK> short the fall of the twin <UNK> makes happy to a lonely and <UNK> man so the message is that the power and the greed <UNK> by the <UNK> must fall for letting the people see the sun rise and the <UNK> <UNK> it is remarkable that this terrible bottom line has been <UNK> by an american there is so much irony in this short film that it is close to be <UNK> br br well the ken <UNK> very know because his anti <UNK> <UNK> is much more clearly and <UNK> in going straight to the point you are angry because your country has been attacked by evil forces but we the others latin americans suffered at a similar date something worst and nobody <UNK> our <UNK> as the whole world has done to you br br it is like if the creative of this project wanted to say to americans you see now america you are not the only that have become victim of the world violence you are not alone in your pain and by the way we the others the non americans have been suffering a lot more than you from long time ago so we are in <UNK> with you in your pain and by the way we are sorry because you have had some taste of your own <UNK> only the mexican and the french short films showed some <UNK> and sympathy for american people the others are like a slap on the face for the american state that is not equal to american people\n",
            " y=1, y_pred=0, g(x)(H)=0.995, f(x)=0.595\n",
            "\n",
            "<START> when i saw that this movie was being shown on tv i was really looking forward to it i grew up in the <UNK> and like everyone else who has grown up in that era have seen every 80's teen and summer camp movie out there so i couldn't wait to see this movie that totally <UNK> that film genre what a disappointment the movie was nothing but a bunch of really bad jokes and gags over and over with hardly any plot and no substance and the filmmakers attempts at dark humor totally failed some of these so called jokes didn't come across as anything but downright cruel and offensive the only good things about this film were the <UNK> music and acting it was nice to go on a nostalgia trip and see all of the summer clothing styles from the 80's and the same goes for the music and the acting was top notch throughout almost all of hollywood's best <UNK> were present too bad they didn't have better material to work with\n",
            " y=0, y_pred=1, g(x)(H)=0.999, f(x)=0.651\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YhsLRvmXO-Ik",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 643
        },
        "outputId": "230b3be9-ea95-4199-c05c-f1b3ed29df28"
      },
      "source": [
        "output_text.write(\"\\nTop misclassfied instances picked by g(x)\\n\")\n",
        "output_text.write(\"-----------------------------------------\\n\")\n",
        "for i in top_crit_idx:\n",
        "  cond = crit_test[i]<theta if clf=='svm' else crit_test[i]>theta\n",
        "  if y_test_best[i] != y_test_pred_th_best[i]:\n",
        "    output_text.write(' '.join([id_to_word[id] for id in X_test_best[i,:] if id!=0])+\\\n",
        "        '\\n y={}'.format(y_test_best[i])+', y_pred={}'.format(y_test_pred_th_best[i])+\\\n",
        "        ', g(x)({})={:.3f}'.format('D' if clf=='svm' else 'H', crit_test[i])+\\\n",
        "        ', f(x)={:.3f}'.format(f_test[i])+'\\n\\n')\n",
        "print(output_text.getvalue())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "top_n=10, rho_user=0.15, g(x)=softmax, addh_hat=True, PCA=True\n",
            "|f(x)>eta|=309(eta=0.149), |g(x)>theta|=320(theta=0.767)\n",
            "\n",
            "Top misclassfied instances picked by f(x)\n",
            "-----------------------------------------\n",
            "<START> i <UNK> to see the film at the recent <UNK> film festival because it had been <UNK> to the <UNK> film <UNK> <UNK> competition section i was surprised that <UNK> could be so off the mark in judging quality br br the film some reviewers have noted does not have too much of <UNK> but the <UNK> violence is <UNK> imagine killing your enemy <UNK> in front of your young son or <UNK> someone to eat a <UNK> <UNK> to prove <UNK> there are some hints of the <UNK> <UNK> sons in <UNK> godfather that seem to <UNK> here in this chinese hong kong film but the quality of the two are as <UNK> different as <UNK> and cheese br br this film is only recommended for violence <UNK> there is no great cinema here at best it might be considered to be better than the usual run run <UNK> production for production values\n",
            " y=0, y_pred=1, g(x)(H)=0.999, f(x)=0.585\n",
            "\n",
            "<START> i found this film to be quite an <UNK> from the very get go i found it extremely hard to like this movie and now after a little thinking about it i can pretty much <UNK> the reason why jean <UNK> <UNK> although i love him to bits i think <UNK> is one of the best movies ever made is quite miscast here and although i can't figure for the life of me who would be better i am sure someone could have taken his place quite easily and make this film work everything else is fine except for the <UNK> at weak comedy a meet the parents joke is not really needed filmmakers and i really like richard e grant as the british major it just suffers from one thing jean <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=0.975, f(x)=0.594\n",
            "\n",
            "and proud of his success in mainstream hollywood br br in another hand i see most of the reviews focuses on their favorite and not so short films but we are <UNK> that there is a subtle bottom line that <UNK> the whole <UNK> and maybe it will not be so pleasant for american people even if that was not the main purpose of the producers br br what i'm talking about is that most of the short films does not show the suffering that <UNK> people went through because the terrorist attack on september <UNK> but the suffering of the other people br br do you need <UNK> about what i'm saying look in the <UNK> short film the message is you cry because of the people who died in the <UNK> but we the others east <UNK> are crying long ago for the crimes committed against our women and nobody pay attention to us like the whole world has done to you br br even though the <UNK> <UNK> story is more in comedy there is a the same thought you are angry because <UNK> bin <UNK> <UNK> you in an evil way but we the others <UNK> should be more angry because our people is dying of <UNK> poverty and aids long time ago and nobody pay attention to us like the whole world has done to you br br look now at the sean <UNK> short the fall of the twin <UNK> makes happy to a lonely and <UNK> man so the message is that the power and the greed <UNK> by the <UNK> must fall for letting the people see the sun rise and the <UNK> <UNK> it is remarkable that this terrible bottom line has been <UNK> by an american there is so much irony in this short film that it is close to be <UNK> br br well the ken <UNK> very know because his anti <UNK> <UNK> is much more clearly and <UNK> in going straight to the point you are angry because your country has been attacked by evil forces but we the others latin americans suffered at a similar date something worst and nobody <UNK> our <UNK> as the whole world has done to you br br it is like if the creative of this project wanted to say to americans you see now america you are not the only that have become victim of the world violence you are not alone in your pain and by the way we the others the non americans have been suffering a lot more than you from long time ago so we are in <UNK> with you in your pain and by the way we are sorry because you have had some taste of your own <UNK> only the mexican and the french short films showed some <UNK> and sympathy for american people the others are like a slap on the face for the american state that is not equal to american people\n",
            " y=1, y_pred=0, g(x)(H)=0.995, f(x)=0.595\n",
            "\n",
            "<START> when i saw that this movie was being shown on tv i was really looking forward to it i grew up in the <UNK> and like everyone else who has grown up in that era have seen every 80's teen and summer camp movie out there so i couldn't wait to see this movie that totally <UNK> that film genre what a disappointment the movie was nothing but a bunch of really bad jokes and gags over and over with hardly any plot and no substance and the filmmakers attempts at dark humor totally failed some of these so called jokes didn't come across as anything but downright cruel and offensive the only good things about this film were the <UNK> music and acting it was nice to go on a nostalgia trip and see all of the summer clothing styles from the 80's and the same goes for the music and the acting was top notch throughout almost all of hollywood's best <UNK> were present too bad they didn't have better material to work with\n",
            " y=0, y_pred=1, g(x)(H)=0.999, f(x)=0.651\n",
            "\n",
            "\n",
            "Top misclassfied instances picked by g(x)\n",
            "-----------------------------------------\n",
            "<START> forget that this is a b movie forget that it is in many ways <UNK> instead give writer director <UNK> <UNK> much deserved credit for <UNK> a subject which at the time <UNK> was <UNK> in hollywood to my knowledge this was the first film to <UNK> the subject of rape and the emotional and mental effects that that crime has upon its victims br br although much of the <UNK> acting is <UNK> at best <UNK> powers who at the time was <UNK> or <UNK> gives an excellent performance throughout as the <UNK> young woman ann who tries to run away from her shame based on her work in this film i'm surprised that she did not have a more successful acting career <UNK> andrews too has some fine moments as the <UNK> who reaches out to help her br br ms <UNK> obviously working on a limited budget was still able to create some memorable scenes such as the pursuit through the streets and <UNK> leading to the rape and the police <UNK> following it and she created a <UNK> ending which left me wondering if ann really could ever have a normal life again\n",
            " y=1, y_pred=0, g(x)(H)=1.000, f(x)=0.332\n",
            "\n",
            "<START> i was honestly surprised by alone in the dark it was so bad i could hardly believe what i was seeing there are no characters just a few stereotypes wandering around and getting killed the extent of the character development was giving each character a name and an <UNK> and that's about it there was no real plot and none of the characters seemed to have any motivation in fact many action scenes just began on their own coming from nowhere with a <UNK> <UNK> track while i was watching this movie i kept asking where is this happening what's going on the acting was high school drama quality with stiff wooden delivery as though the actors were reading from <UNK> cards without <UNK> their lines their trouble delivering lines was made even more obvious by horrible sound design <UNK> sounded like it was recorded in an open room the actors were constantly taking obvious care to hit their marks looking almost <UNK> in their movements so these <UNK> <UNK> are <UNK> through a series of implausible and confusing <UNK> often without even the benefit of transition scenes they were here now they're there this was happening now that's happening random scenes with little <UNK> or reason i had a lot of fun watching it definitely not worth nine bucks though\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.386\n",
            "\n",
            "<START> plot synopsis hong kong <UNK> paul <UNK> the man who built the victoria <UNK> is murdered along with his wife by his <UNK> his twin sons <UNK> alex are split apart 25 years later <UNK> a <UNK> <UNK> in los angeles alex a <UNK> living in hong kong join forces to <UNK> their <UNK> murder <UNK> claim the <UNK> br br this is the second time that jean <UNK> van <UNK> <UNK> <UNK> have worked together having previously done <UNK> this is also the first of three films to feature van <UNK> playing <UNK> roles <UNK> risk <UNK> are the others the plot is a very simplistic take on the revenge story the film's sole redeeming feature being van <UNK> performance as two very different people  the <UNK> rich kid the rough <UNK> <UNK> <UNK> tough guy as it goes van <UNK> doesn't do a very good job in either role although his take on alex is mildly amusing it is <UNK> as to have the brothers mistaken for each other with them wearing different clothes having different <UNK> <UNK> <UNK> makes a very worthy <UNK> for the <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.449\n",
            "\n",
            "<START> sleeping with the enemy is a predictable <UNK> there <UNK> thriller that never seems to find any inspiration no matter how desperately cast and crew try i can't believe a bunch of my friends talked me into seeing this at the movies some <UNK> years ago br br the complete lack of originality from the <UNK> <UNK> screenplay based upon the nancy price novel does not help nor does the stale direction of joseph <UNK> or the very average performance from julia roberts the supporting cast including patrick <UNK> and kevin anderson do little to help br br there really isn't a lot to say just give it a miss br br sunday april 14 <UNK> <UNK> cinema <UNK> <UNK>\n",
            " y=0, y_pred=1, g(x)(H)=1.000, f(x)=0.581\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xH-thpd8rmTC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "path_txt = \"drive/My Drive/NIPS2020/results/imdb/instances_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(output_text.getvalue()) \n",
        "txt.close() #to change file access modes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gX7WuJijg-Ho",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "table_by_row_index = report_table_concat.groupby(report_table_concat.index)\n",
        "report_table_mean = table_by_row_index.mean()\n",
        "report_table_std = table_by_row_index.std()\n",
        "\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "table_by_row_index = report_criteria_concat.groupby(report_criteria_concat.index)\n",
        "report_criteria_mean = table_by_row_index.mean()\n",
        "report_criteria_std = table_by_row_index.std()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4zDE3LuFiAvg",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "8561aab0-af6f-4211-d048-a2abb77bf548"
      },
      "source": [
        "report_table_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rule</th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>eta</th>\n",
              "      <th>p_value</th>\n",
              "      <th>check</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.06</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00560</td>\n",
              "      <td>0.14192</td>\n",
              "      <td>3.786</td>\n",
              "      <td>0.820517</td>\n",
              "      <td>1.596467e-04</td>\n",
              "      <td>0.01</td>\n",
              "      <td>0.010</td>\n",
              "      <td>0.0049</td>\n",
              "      <td>0.1471</td>\n",
              "      <td>3.218</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02472</td>\n",
              "      <td>0.12280</td>\n",
              "      <td>16.776</td>\n",
              "      <td>0.672585</td>\n",
              "      <td>1.438608e-20</td>\n",
              "      <td>0.05</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.0247</td>\n",
              "      <td>0.1273</td>\n",
              "      <td>16.284</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.84</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04568</td>\n",
              "      <td>0.10184</td>\n",
              "      <td>31.026</td>\n",
              "      <td>0.448278</td>\n",
              "      <td>2.111505e-39</td>\n",
              "      <td>0.10</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.0457</td>\n",
              "      <td>0.1063</td>\n",
              "      <td>30.058</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.26</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06064</td>\n",
              "      <td>0.08688</td>\n",
              "      <td>41.244</td>\n",
              "      <td>0.423398</td>\n",
              "      <td>1.133743e-41</td>\n",
              "      <td>0.15</td>\n",
              "      <td>0.154</td>\n",
              "      <td>0.0634</td>\n",
              "      <td>0.0886</td>\n",
              "      <td>41.738</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.12</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.07376</td>\n",
              "      <td>0.07376</td>\n",
              "      <td>50.104</td>\n",
              "      <td>0.136592</td>\n",
              "      <td>5.266706e-43</td>\n",
              "      <td>0.20</td>\n",
              "      <td>0.200</td>\n",
              "      <td>0.0775</td>\n",
              "      <td>0.0745</td>\n",
              "      <td>50.980</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rule  rho_user  error_val  ...  error_test  L_test  %reduction_test\n",
              "0  1.06      0.01    0.00560  ...      0.0049  0.1471            3.218\n",
              "1  1.20      0.05    0.02472  ...      0.0247  0.1273           16.284\n",
              "2  0.84      0.10    0.04568  ...      0.0457  0.1063           30.058\n",
              "3  1.26      0.15    0.06064  ...      0.0634  0.0886           41.738\n",
              "4  0.12      0.20    0.07376  ...      0.0775  0.0745           50.980\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1zgyJyu4iCBQ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "cb39105f-bc61-49e2-e1ff-25a644cd23fe"
      },
      "source": [
        "report_criteria_mean"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>thresh</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.01</td>\n",
              "      <td>0.00488</td>\n",
              "      <td>0.14264</td>\n",
              "      <td>3.306</td>\n",
              "      <td>0.998604</td>\n",
              "      <td>0.012</td>\n",
              "      <td>0.0057</td>\n",
              "      <td>0.1463</td>\n",
              "      <td>3.672</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.05</td>\n",
              "      <td>0.02320</td>\n",
              "      <td>0.12432</td>\n",
              "      <td>15.728</td>\n",
              "      <td>0.964553</td>\n",
              "      <td>0.054</td>\n",
              "      <td>0.0242</td>\n",
              "      <td>0.1278</td>\n",
              "      <td>15.878</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.10</td>\n",
              "      <td>0.04408</td>\n",
              "      <td>0.10344</td>\n",
              "      <td>29.916</td>\n",
              "      <td>0.885287</td>\n",
              "      <td>0.102</td>\n",
              "      <td>0.0452</td>\n",
              "      <td>0.1068</td>\n",
              "      <td>29.700</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.15</td>\n",
              "      <td>0.06088</td>\n",
              "      <td>0.08664</td>\n",
              "      <td>41.324</td>\n",
              "      <td>0.774637</td>\n",
              "      <td>0.152</td>\n",
              "      <td>0.0654</td>\n",
              "      <td>0.0866</td>\n",
              "      <td>43.020</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.20</td>\n",
              "      <td>0.07608</td>\n",
              "      <td>0.07144</td>\n",
              "      <td>51.648</td>\n",
              "      <td>0.668580</td>\n",
              "      <td>0.206</td>\n",
              "      <td>0.0825</td>\n",
              "      <td>0.0695</td>\n",
              "      <td>54.300</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rho_user  error_val    L_val  ...  error_test  L_test  %reduction_test\n",
              "0      0.01    0.00488  0.14264  ...      0.0057  0.1463            3.672\n",
              "1      0.05    0.02320  0.12432  ...      0.0242  0.1278           15.878\n",
              "2      0.10    0.04408  0.10344  ...      0.0452  0.1068           29.700\n",
              "3      0.15    0.06088  0.08664  ...      0.0654  0.0866           43.020\n",
              "4      0.20    0.07608  0.07144  ...      0.0825  0.0695           54.300\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "neU88XQkAzny",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "4b792c43-3fdb-48ee-d89a-e251ef90f666"
      },
      "source": [
        "report_table_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rule</th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>eta</th>\n",
              "      <th>p_value</th>\n",
              "      <th>check</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.176010</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.001265</td>\n",
              "      <td>0.007034</td>\n",
              "      <td>0.797076</td>\n",
              "      <td>0.219716</td>\n",
              "      <td>3.569270e-04</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.001817</td>\n",
              "      <td>0.009283</td>\n",
              "      <td>1.190869</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.104536</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.003025</td>\n",
              "      <td>0.007595</td>\n",
              "      <td>2.114800</td>\n",
              "      <td>0.264298</td>\n",
              "      <td>3.187349e-20</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.002564</td>\n",
              "      <td>0.009425</td>\n",
              "      <td>1.828026</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.221884</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002918</td>\n",
              "      <td>0.007857</td>\n",
              "      <td>2.533146</td>\n",
              "      <td>0.322871</td>\n",
              "      <td>4.721467e-39</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.006535</td>\n",
              "      <td>0.008526</td>\n",
              "      <td>3.697022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>1.047855</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004495</td>\n",
              "      <td>0.010188</td>\n",
              "      <td>4.406856</td>\n",
              "      <td>0.203426</td>\n",
              "      <td>2.432024e-41</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.008944</td>\n",
              "      <td>0.006628</td>\n",
              "      <td>0.008569</td>\n",
              "      <td>3.804684</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.178885</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005127</td>\n",
              "      <td>0.009045</td>\n",
              "      <td>4.302619</td>\n",
              "      <td>0.043197</td>\n",
              "      <td>1.177661e-42</td>\n",
              "      <td>0.0</td>\n",
              "      <td>0.010000</td>\n",
              "      <td>0.010186</td>\n",
              "      <td>0.009670</td>\n",
              "      <td>5.632673</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       rule  rho_user  error_val  ...  error_test    L_test  %reduction_test\n",
              "0  1.176010       0.0   0.001265  ...    0.001817  0.009283         1.190869\n",
              "1  1.104536       0.0   0.003025  ...    0.002564  0.009425         1.828026\n",
              "2  1.221884       0.0   0.002918  ...    0.006535  0.008526         3.697022\n",
              "3  1.047855       0.0   0.004495  ...    0.006628  0.008569         3.804684\n",
              "4  0.178885       0.0   0.005127  ...    0.010186  0.009670         5.632673\n",
              "\n",
              "[5 rows x 12 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MhjaMD-kKNMT",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "outputId": "95ed1e63-79df-4e52-e414-e7a7d519ef96"
      },
      "source": [
        "report_criteria_std"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>rho_user</th>\n",
              "      <th>error_val</th>\n",
              "      <th>L_val</th>\n",
              "      <th>%reduction_val</th>\n",
              "      <th>thresh</th>\n",
              "      <th>budget</th>\n",
              "      <th>error_test</th>\n",
              "      <th>L_test</th>\n",
              "      <th>%reduction_test</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.000996</td>\n",
              "      <td>0.007357</td>\n",
              "      <td>0.659454</td>\n",
              "      <td>0.000779</td>\n",
              "      <td>0.004472</td>\n",
              "      <td>0.003054</td>\n",
              "      <td>0.007032</td>\n",
              "      <td>1.738094</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002871</td>\n",
              "      <td>0.006832</td>\n",
              "      <td>1.788413</td>\n",
              "      <td>0.016886</td>\n",
              "      <td>0.005477</td>\n",
              "      <td>0.003347</td>\n",
              "      <td>0.007023</td>\n",
              "      <td>1.419972</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.002834</td>\n",
              "      <td>0.007164</td>\n",
              "      <td>2.081797</td>\n",
              "      <td>0.053454</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.005251</td>\n",
              "      <td>0.006496</td>\n",
              "      <td>2.410187</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.004625</td>\n",
              "      <td>0.007785</td>\n",
              "      <td>3.379797</td>\n",
              "      <td>0.081735</td>\n",
              "      <td>0.008367</td>\n",
              "      <td>0.005067</td>\n",
              "      <td>0.005878</td>\n",
              "      <td>1.744506</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "      <td>0.005370</td>\n",
              "      <td>0.008341</td>\n",
              "      <td>4.106479</td>\n",
              "      <td>0.093708</td>\n",
              "      <td>0.008944</td>\n",
              "      <td>0.006225</td>\n",
              "      <td>0.006567</td>\n",
              "      <td>2.760860</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   rho_user  error_val     L_val  ...  error_test    L_test  %reduction_test\n",
              "0       0.0   0.000996  0.007357  ...    0.003054  0.007032         1.738094\n",
              "1       0.0   0.002871  0.006832  ...    0.003347  0.007023         1.419972\n",
              "2       0.0   0.002834  0.007164  ...    0.005251  0.006496         2.410187\n",
              "3       0.0   0.004625  0.007785  ...    0.005067  0.005878         1.744506\n",
              "4       0.0   0.005370  0.008341  ...    0.006225  0.006567         2.760860\n",
              "\n",
              "[5 rows x 9 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1NU73I2FKP6B",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}