{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "CIFAR10_cnn.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesar-claros/synergistic/blob/SocialAds/CIFAR10_cnn.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TFGGSTpW_RbD"
      },
      "source": [
        "# CIFAR 10"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ayq3B9TI_Kdm"
      },
      "source": [
        "! git clone https://github.com/cesar-claros/synergistic\n",
        "% cd synergistic/"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f-lBcXow_Zz4"
      },
      "source": [
        "## Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4NehHQNB_bjw"
      },
      "source": [
        "#%%\n",
        "# Command line instalation\n",
        "# ---------------------------\n",
        "!pip install torch\n",
        "!pip install gpytorch\n",
        "!pip install tensorflow-determinism\n",
        "\n",
        "# Imports\n",
        "# ---------------------------\n",
        "import io #Used as buffer\n",
        "import sys\n",
        "import matplotlib\n",
        "import tensorflow as tf # Keras model for MNIST \n",
        "# matplotlib.use('qt5Agg')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import auxfunc.funcs as sgn\n",
        "import auxfunc.trustscore as trs\n",
        "import seaborn as sns\n",
        "import torch\n",
        "from scipy.stats import entropy, spearmanr\n",
        "from sklearn import model_selection, svm, ensemble, linear_model, pipeline, metrics,\\\n",
        "      tree, neighbors, discriminant_analysis, gaussian_process, preprocessing, impute, decomposition\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
        "plt.style.use(['ggplot','style/style.mplstyle'])\n",
        "import os\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JAZez0Yd_sar"
      },
      "source": [
        "## Auxiliar Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPr-qGxurI6H"
      },
      "source": [
        "#%%\n",
        "# Define classic MLP architecture\n",
        "def CNN_model(input_dim):\n",
        "    # A simple model based off LeNet from https://keras.io/examples/mnist_cnn/\n",
        "    model = tf.keras.Sequential()\n",
        "    model.add(tf.keras.layers.Conv2D(32, [3, 3], activation='relu', input_shape=(32,32,3))) \n",
        "    model.add(tf.keras.layers.Conv2D(64, [3, 3], activation='relu'))\n",
        "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Flatten())\n",
        "    model.add(tf.keras.layers.Dense(128, activation='relu'))\n",
        "    model.add(tf.keras.layers.Dropout(0.5))\n",
        "    model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
        "    return model"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q3WjH5CGADeK"
      },
      "source": [
        "#%%\n",
        "# Signaling function fitting and evaluation\n",
        "def signalingFunction(X_train, y_train, y_train_pred_th, X_val, y_val, y_val_pred_th, X_test, y_test, y_test_pred_th, kernel='exponential', norm='l01', ex_dim=1):\n",
        "    # X_train, X_val should be scaled. ex_dim=1 by default on sgn.signailing\n",
        "    # Fit signaling function \n",
        "    exp = sgn.signaling(norm=norm) # idx = [train,test,val]\n",
        "    exp.fit(X_train, y_train, y_train_pred_th, kernel=kernel, n_iter=500, lr=0.01, ex_dim=ex_dim)\n",
        "    table_val = exp.evaluate(X_val, y_val, y_val_pred_th, rule_grid=np.linspace(0,3,30, endpoint=False), rho_grid=[0.1, 0.15])\n",
        "    table_test = exp.test(X_test, y_test, y_test_pred_th, table_val['rule'].to_numpy(), table_val['eta'].to_numpy())\n",
        "    table = pd.concat([table_val,table_test],axis=1)\n",
        "    return table, exp"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nRVYBv5lxPJk"
      },
      "source": [
        "#%%\n",
        "# Initialize model\n",
        "def init_model(input_dim):\n",
        "    # svm = False\n",
        "    # model = MLP_model(input_dim=Data_X.shape[1], svm_obj=svm)\n",
        "    model  = CNN_model(input_dim=input_dim)\n",
        "    loss   = tf.keras.losses.categorical_crossentropy\n",
        "    metric = ['accuracy']\n",
        "    model.compile(loss=loss,\n",
        "                  optimizer=tf.keras.optimizers.Adam(learning_rate=1e-3, beta_1=0.9, \n",
        "                                                     beta_2=0.999, epsilon=1e-07, amsgrad=False,\n",
        "                                                     name='Adam'),\n",
        "                  metrics=metric)\n",
        "    return model"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pMK50NRixYI9"
      },
      "source": [
        "#%%\n",
        "# Soft and thresholded output predictions\n",
        "def pred_output(model, X):\n",
        "    y_pred_soft = model.predict(X)\n",
        "    y_pred_th   = np.argmax(y_pred_soft, axis=1)\n",
        "    return y_pred_soft, y_pred_th"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t174yG8_f1_2"
      },
      "source": [
        "#%%\n",
        "# Jaccard similarity index\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n6VM8w4vf4lN"
      },
      "source": [
        "#%%\n",
        "# Baseline comparison\n",
        "def baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp,trust_val, trust_test):\n",
        "      direction = 'further'\n",
        "      # p_val = np.concatenate(y_val_pred_soft,axis=1)\n",
        "      crit_val   = entropy(y_val_pred_soft, axis=1, base=10)\n",
        "      # p_test = np.concatenate(y_test_pred_soft,axis=1)\n",
        "      crit_test  = entropy(y_test_pred_soft, axis=1, base=10)\n",
        "      # Criteria 1\n",
        "      critFunc   = sgn.critEvaluation(norm='l01',direction=direction)\n",
        "      d_val      = critFunc.evaluate(y_val, y_val_pred_th, crit_val, rho_grid=[0.1, 0.15])\n",
        "      d_test     = critFunc.test(y_test, y_test_pred_th, crit_test, d_val['thresh'].to_numpy())\n",
        "      crit_table = pd.concat([d_val,d_test],axis=1)\n",
        "      # Best rules from signailing function on val are used to get UCBs on test\n",
        "      gamma      = table['rule'].to_numpy().reshape(-1,1)\n",
        "      f_test     = exp.gpr_mean_test + gamma*np.sqrt(exp.gpr_var_test)\n",
        "      # Threshold values on val data. UCB Signailing (eta) and new criteria (theta) \n",
        "      eta        = table['eta'].to_numpy().reshape(-1,1)\n",
        "      theta      = crit_table['thresh'].to_numpy().reshape(-1,1)\n",
        "      if direction == 'closer':\n",
        "        crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)<theta)\n",
        "      else:\n",
        "        crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)>theta)\n",
        "      f_mask, f_idx = np.nonzero(f_test>eta)      \n",
        "      shared = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask))))\n",
        "      # Jaccard index btw signaled instances using both methods for ith rule-threshold\n",
        "      J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan for i in range(f_test.shape[0])]\n",
        "      crit_table['jaccard']=J\n",
        "      Sp = [spearmanr(f_test[i,:],crit_test)[0] for i in range(f_test.shape[0])]\n",
        "      crit_table['spearman'] = Sp\n",
        "      crit_table['gamma']    = gamma\n",
        "\n",
        "      # Criteria 2\n",
        "      critFuncSc = sgn.critEvaluation(norm='l01',direction='closer')\n",
        "      s_val      = critFuncSc.evaluate(y_val, y_val_pred_th, trust_val, rho_grid=[0.1, 0.15])\n",
        "      s_test     = critFuncSc.test(y_test, y_test_pred_th, trust_test, s_val['thresh'].to_numpy())\n",
        "      score_table= pd.concat([s_val,s_test],axis=1)\n",
        "      # Threshold values on val data. TrustScore (theta0) \n",
        "      theta0     = score_table['thresh'].to_numpy().reshape(-1,1) \n",
        "      crit_mask0, crit_idx0 = np.nonzero(trust_test.reshape(1,-1)<theta0)\n",
        "      # Jaccard index btw signaled instances using both methods for ith rule-threshold  \n",
        "      shared0    = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask0))))\n",
        "      J0         = [jaccard_similarity(crit_idx0[crit_mask0==i],f_idx[f_mask==i]) if i in shared0 else np.nan for i in range(f_test.shape[0])]\n",
        "      Sp0        = [spearmanr(f_test[i,:],trust_test)[0] for i in range(f_test.shape[0])]\n",
        "      score_table['jaccard']  = J0 \n",
        "      score_table['spearman'] = Sp0  \n",
        "\n",
        "      return crit_table,score_table,crit_test"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wbo4e_6RA8_b"
      },
      "source": [
        "## Signailing function and baselines"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ICCVCQKDU00u"
      },
      "source": [
        "# For reproducibility\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "SEED = 12345\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R3VCXn3_rTYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b1c1c45b-e6a2-4887-b620-f6287e335be4"
      },
      "source": [
        "# %%\n",
        "# INITIALIZATION\n",
        "# ==============\n",
        "# EXPERIMENT SETUP\n",
        "# ================\n",
        "# Load data set\n",
        "(Data_X, Data_y), (X_test, y_test) = tf.keras.datasets.cifar10.load_data()\n",
        "# y, y_test = y.astype('int8'), y_test.astype('int8')\n",
        "# Rescale the images from [0,255] to the [0.0,1.0] range.\n",
        "Data_X, X_test = Data_X[...]/255.0, X_test[...]/255.0\n",
        "n_cat  = np.unique(np.concatenate([Data_y,y_test]), return_counts=False)\n",
        "n_cat  = n_cat.size\n",
        "print(\"Number of original training examples:\", len(Data_X))\n",
        "print(\"Number of original testing examples:\", len(X_test))\n",
        "print(\"---------------\")\n",
        "# Subsample data (30% of original data both in training and testing)\n",
        "Data_X, Data_X_sep, Data_y, Data_y_sep = model_selection.train_test_split(Data_X, Data_y, stratify=Data_y, test_size=0.75, random_state=SEED)\n",
        "X_test, X_test_sep, y_test, y_test_sep = model_selection.train_test_split(X_test, y_test, stratify=y_test, test_size=0.75, random_state=SEED)\n",
        "print(\"Number of subsampled training examples:\", len(Data_X))\n",
        "print(\"Number of subsampled testing examples:\", len(X_test))"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170500096/170498071 [==============================] - 13s 0us/step\n",
            "170508288/170498071 [==============================] - 13s 0us/step\n",
            "Number of original training examples: 50000\n",
            "Number of original testing examples: 10000\n",
            "---------------\n",
            "Number of subsampled training examples: 12500\n",
            "Number of subsampled testing examples: 2500\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0jsIZNLPri8D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "716206b8-e44f-4e09-96c9-44eb7bffec16"
      },
      "source": [
        "#%%\n",
        "# Assign labels\n",
        "report_table    = []\n",
        "trust_criteria  = []\n",
        "report_criteria = []\n",
        "report_plot     = []\n",
        "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "clf = 'softmax_act'\n",
        "addPredictions = True\n",
        "accuracy = 0\n",
        "fold     = 1\n",
        "es = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='auto', verbose=1)\n",
        "y_test = tf.keras.utils.to_categorical(y_test)\n",
        "for train, val in kf.split(Data_X, Data_y):\n",
        "    X_train = Data_X[train]\n",
        "    y_train = Data_y[train]\n",
        "    y_train = tf.keras.utils.to_categorical(y_train)\n",
        "    X_val   = Data_X[val]\n",
        "    y_val   = Data_y[val]\n",
        "    y_val   = tf.keras.utils.to_categorical(y_val)\n",
        "\n",
        "    # ML Model fit & prediction\n",
        "    model = init_model(input_dim=Data_X.shape[1:])\n",
        "    # model.fit(X_train, y_train, batch_size=128, epochs=30, verbose=0, validation_data=(X_val, y_val), callbacks=[es])\n",
        "    model.fit(X_train, y_train, batch_size=128, epochs=10, verbose=0, validation_data=(X_val, y_val))\n",
        "    y_train_pred_soft, y_train_pred_th = pred_output(model, X_train) # y_train_pred_soft&y_train_pred_th: 10-d vector  \n",
        "    y_val_pred_soft, y_val_pred_th     = pred_output(model, X_val)   # y_val_pred_soft&y_val_pred_th    : 10-d vector\n",
        "    y_test_pred_soft, y_test_pred_th   = pred_output(model, X_test)  # y_test_pred_soft&y_test_pred_th  : 10-d vector\n",
        "    print('accuracy(Train)={}'.format(np.sum(y_train_pred_th==np.argmax(y_train,axis=1))/y_train_pred_th.size))\n",
        "\n",
        "    # layer_outputs = [layer.output for layer in model.layers[4]]\n",
        "    if clf == 'softmax_act':\n",
        "            activation_model = tf.keras.models.Model(inputs=model.input, outputs=model.layers[5].output) # Early feture extraction: 128 dense vector\n",
        "            X_train_GP       = activation_model.predict(X_train)\n",
        "            X_val_GP         = activation_model.predict(X_val)\n",
        "            X_test_GP        = activation_model.predict(X_test)\n",
        "    elif clf == 'softmax':\n",
        "            X_train_GP       = X_train.reshape(-1,np.prod(X_train.shape[1:]))\n",
        "            X_val_GP         = X_val.reshape(-1,np.prod(X_val.shape[1:]))\n",
        "            X_test_GP        = X_test.reshape(-1,np.prod(X_test.shape[1:]))\n",
        "\n",
        "    if addPredictions:\n",
        "            # Add predictions\n",
        "            X_train_GP = np.concatenate((X_train_GP, y_train_pred_soft), axis=1)\n",
        "            X_val_GP   = np.concatenate((X_val_GP, y_val_pred_soft), axis=1)\n",
        "            X_test_GP  = np.concatenate((X_test_GP, y_test_pred_soft), axis=1)\n",
        "    scaleX_GP  = preprocessing.StandardScaler().fit(X_train_GP)\n",
        "    X_train_GP = scaleX_GP.transform(X_train_GP)\n",
        "    X_val_GP   = scaleX_GP.transform(X_val_GP)\n",
        "    X_test_GP  = scaleX_GP.transform(X_test_GP)\n",
        "\n",
        "    # Signailing function. Call\n",
        "    table, exp = signalingFunction(X_train_GP, np.argmax(y_train, axis=1), y_train_pred_th, \\\n",
        "                                   X_val_GP, np.argmax(y_val, axis=1), y_val_pred_th,\\\n",
        "                                   X_test_GP, np.argmax(y_test, axis=1), y_test_pred_th,\\\n",
        "                                   kernel='e*e', ex_dim=y_train_pred_soft.shape[1])\n",
        "\n",
        "    report_table.append(pd.concat([pd.DataFrame({'fold':[fold]*table.shape[0]}),table],axis=1))\n",
        "\n",
        "    # Trust Score fitted on train data to evaluate loss reduction on val-test data\n",
        "    trust_model = trs.TrustScore()\n",
        "    trust_model.fit(X=X_train_GP,Y=y_train,classes=n_cat)\n",
        "    trust_val,class_val   = trust_model.score(X_val_GP, y_val_pred_th)\n",
        "    trust_test,class_test = trust_model.score(X_test_GP, y_test_pred_th)\n",
        "    \n",
        "    # Baseline for comparison\n",
        "    crit_table,score_table,crit_test = baselineCriteria(np.argmax(y_val, axis=1), y_val_pred_soft, y_val_pred_th,\\\n",
        "                                                        np.argmax(y_test, axis=1), y_test_pred_soft, y_test_pred_th,\\\n",
        "                                                        table, exp,\\\n",
        "                                                        trust_val, trust_test)\n",
        "   \n",
        "    report_criteria.append(pd.concat([pd.DataFrame({'fold':[fold]*crit_table.shape[0]}),crit_table],axis=1))\n",
        "    trust_criteria.append(pd.concat([pd.DataFrame({'fold':[fold]*score_table.shape[0]}),score_table],axis=1))\n",
        "\n",
        "    score = np.sum(np.argmax(y_val, axis=1)==y_val_pred_th)/np.size(np.argmax(y_val, axis=1))\n",
        "    if accuracy < score:\n",
        "      accuracy = score\n",
        "      table_best = table\n",
        "      crit_table_best = crit_table\n",
        "      exp_best = exp\n",
        "      y_test_best = y_test\n",
        "      y_test_pred_soft_best = y_test_pred_soft\n",
        "      y_test_pred_th_best = y_test_pred_th\n",
        "      X_test_best = X_test\n",
        "    fold +=1\n",
        "    del(model)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "accuracy(Train)=0.7681\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 491/500 - Loss: 0.389  noise: 0.072\n",
            "Iter 492/500 - Loss: 0.389  noise: 0.072\n",
            "Iter 493/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 494/500 - Loss: 0.390  noise: 0.071\n",
            "Iter 495/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 496/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 497/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 498/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 499/500 - Loss: 0.389  noise: 0.071\n",
            "Iter 500/500 - Loss: 0.388  noise: 0.071\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "accuracy(Train)=0.7254\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 491/500 - Loss: 0.470  noise: 0.091\n",
            "Iter 492/500 - Loss: 0.469  noise: 0.091\n",
            "Iter 493/500 - Loss: 0.470  noise: 0.091\n",
            "Iter 494/500 - Loss: 0.470  noise: 0.091\n",
            "Iter 495/500 - Loss: 0.470  noise: 0.091\n",
            "Iter 496/500 - Loss: 0.471  noise: 0.091\n",
            "Iter 497/500 - Loss: 0.471  noise: 0.091\n",
            "Iter 498/500 - Loss: 0.470  noise: 0.091\n",
            "Iter 499/500 - Loss: 0.470  noise: 0.090\n",
            "Iter 500/500 - Loss: 0.471  noise: 0.090\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "accuracy(Train)=0.7724\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 491/500 - Loss: 0.398  noise: 0.077\n",
            "Iter 492/500 - Loss: 0.399  noise: 0.077\n",
            "Iter 493/500 - Loss: 0.398  noise: 0.077\n",
            "Iter 494/500 - Loss: 0.398  noise: 0.077\n",
            "Iter 495/500 - Loss: 0.399  noise: 0.077\n",
            "Iter 496/500 - Loss: 0.398  noise: 0.077\n",
            "Iter 497/500 - Loss: 0.399  noise: 0.077\n",
            "Iter 498/500 - Loss: 0.399  noise: 0.076\n",
            "Iter 499/500 - Loss: 0.398  noise: 0.076\n",
            "Iter 500/500 - Loss: 0.398  noise: 0.076\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "accuracy(Train)=0.7582\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 491/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 492/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 493/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 494/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 495/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 496/500 - Loss: 0.406  noise: 0.076\n",
            "Iter 497/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 498/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 499/500 - Loss: 0.408  noise: 0.076\n",
            "Iter 500/500 - Loss: 0.407  noise: 0.076\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n",
            "accuracy(Train)=0.7622\n",
            "initializing cuda...\n",
            "lr=0.01, n_iterations=500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/gpytorch/utils/linear_cg.py:234: UserWarning: An output with one or more elements was resized since it had shape [11], which does not match the required output shape [1, 11].This behavior is deprecated, and in a future PyTorch release outputs will not be resized unless they have zero elements. You can explicitly reuse an out tensor t by resizing it, inplace, to zero elements with t.resize_(0). (Triggered internally at  ../aten/src/ATen/native/Resize.cpp:23.)\n",
            "  torch.sum(mul_storage, -2, keepdim=True, out=alpha)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iter 491/500 - Loss: 0.413  noise: 0.077\n",
            "Iter 492/500 - Loss: 0.414  noise: 0.077\n",
            "Iter 493/500 - Loss: 0.413  noise: 0.077\n",
            "Iter 494/500 - Loss: 0.414  noise: 0.077\n",
            "Iter 495/500 - Loss: 0.412  noise: 0.076\n",
            "Iter 496/500 - Loss: 0.413  noise: 0.076\n",
            "Iter 497/500 - Loss: 0.414  noise: 0.076\n",
            "Iter 498/500 - Loss: 0.413  noise: 0.076\n",
            "Iter 499/500 - Loss: 0.414  noise: 0.076\n",
            "Iter 500/500 - Loss: 0.412  noise: 0.076\n",
            "evaluating with cuda...\n",
            "evaluating with cuda...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-059zsY9UwAK"
      },
      "source": [
        "## Summary"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "nhvxiQi7UzCf",
        "outputId": "11ed68f2-ff21-4161-8c69-ecdf88f3d7d2"
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "report_table_concat = report_table_concat.groupby('rho_user').agg({\"rho_hat_test\":[np.median],\"%loss_red_test\": [np.median,min, max]})\n",
        "report_table_concat"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>rho_hat_test</th>\n",
              "      <th colspan=\"3\" halign=\"left\">%loss_red_test</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>median</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho_user</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.10</td>\n",
              "      <td>18.07</td>\n",
              "      <td>15.51</td>\n",
              "      <td>19.49</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.15</th>\n",
              "      <td>0.15</td>\n",
              "      <td>25.84</td>\n",
              "      <td>25.07</td>\n",
              "      <td>29.35</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         rho_hat_test %loss_red_test              \n",
              "               median         median    min    max\n",
              "rho_user                                          \n",
              "0.10             0.10          18.07  15.51  19.49\n",
              "0.15             0.15          25.84  25.07  29.35"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "cniEnorRU0qz",
        "outputId": "ccd07368-dc06-4de3-eb45-33234c7bbd2d"
      },
      "source": [
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "report_criteria_concat = report_criteria_concat.groupby('rho_user').agg({\"rho_hat_test\":[np.median],\"%loss_red_test\":[np.median,min, max],\"jaccard\":[np.median,min, max]})\n",
        "report_criteria_concat"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>rho_hat_test</th>\n",
              "      <th colspan=\"3\" halign=\"left\">%loss_red_test</th>\n",
              "      <th colspan=\"3\" halign=\"left\">jaccard</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>median</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho_user</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.11</td>\n",
              "      <td>18.93</td>\n",
              "      <td>16.75</td>\n",
              "      <td>19.40</td>\n",
              "      <td>0.257709</td>\n",
              "      <td>0.246445</td>\n",
              "      <td>0.333333</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.15</th>\n",
              "      <td>0.15</td>\n",
              "      <td>25.93</td>\n",
              "      <td>24.87</td>\n",
              "      <td>26.84</td>\n",
              "      <td>0.351449</td>\n",
              "      <td>0.301325</td>\n",
              "      <td>0.390476</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         rho_hat_test %loss_red_test         ...   jaccard                    \n",
              "               median         median    min  ...    median       min       max\n",
              "rho_user                                     ...                              \n",
              "0.10             0.11          18.93  16.75  ...  0.257709  0.246445  0.333333\n",
              "0.15             0.15          25.93  24.87  ...  0.351449  0.301325  0.390476\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 175
        },
        "id": "BY6k0vPPU2I1",
        "outputId": "52217bdd-9f9d-43c3-c43a-a9e44d8c5cca"
      },
      "source": [
        "report_trust_concat    = pd.concat(trust_criteria)\n",
        "report_trust_concat    = report_trust_concat.groupby('rho_user').agg({\"rho_hat_test\":[np.median],\"%loss_red_test\":[np.median,min, max],\"jaccard\":[np.median,min, max]})\n",
        "report_trust_concat"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr th {\n",
              "        text-align: left;\n",
              "    }\n",
              "\n",
              "    .dataframe thead tr:last-of-type th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>rho_hat_test</th>\n",
              "      <th colspan=\"3\" halign=\"left\">%loss_red_test</th>\n",
              "      <th colspan=\"3\" halign=\"left\">jaccard</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th></th>\n",
              "      <th>median</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "      <th>median</th>\n",
              "      <th>min</th>\n",
              "      <th>max</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>rho_user</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0.10</th>\n",
              "      <td>0.09</td>\n",
              "      <td>15.84</td>\n",
              "      <td>14.24</td>\n",
              "      <td>16.98</td>\n",
              "      <td>0.238462</td>\n",
              "      <td>0.195000</td>\n",
              "      <td>0.263027</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>0.15</th>\n",
              "      <td>0.14</td>\n",
              "      <td>24.58</td>\n",
              "      <td>20.91</td>\n",
              "      <td>26.23</td>\n",
              "      <td>0.299145</td>\n",
              "      <td>0.238532</td>\n",
              "      <td>0.341969</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "         rho_hat_test %loss_red_test         ...   jaccard                    \n",
              "               median         median    min  ...    median       min       max\n",
              "rho_user                                     ...                              \n",
              "0.10             0.09          15.84  14.24  ...  0.238462  0.195000  0.263027\n",
              "0.15             0.14          24.58  20.91  ...  0.299145  0.238532  0.341969\n",
              "\n",
              "[2 rows x 7 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NnV3S5ntjoRB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "20cb08c2-d640-4e5e-b0ba-bdaee43474e1"
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "# report_table_concat\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "# report_criteria_concat\n",
        "# p-value column\n",
        "p_value_col = report_table_concat['p_value'] #Add\n",
        "cols_CQT = ['rho_user','corrected_test','queries_test','total_wrong_test','loss_query_test']\n",
        "cols_rholoss = ['rho_user','rho_hat_test','%loss_red_test']\n",
        "\n",
        "# Dataframes for f(x)\n",
        "df_fx_CQT = pd.DataFrame(report_table_concat[cols_CQT])\n",
        "df_fx_rholoss = pd.DataFrame(report_table_concat[cols_rholoss])\n",
        "# results_fx_CQT = df_fx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_rholoss = df_fx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_CQT = df_fx_CQT.copy()\n",
        "# results_fx_rholoss = df_fx_rholoss.copy()\n",
        "\n",
        "results_fx_rholoss_bri = results_fx_rholoss.groupby(results_fx_rholoss.index)\n",
        "fx_rholoss_median = results_fx_rholoss_bri.median()\n",
        "fx_rholoss_q1 = results_fx_rholoss_bri.quantile(q=0)\n",
        "fx_rholoss_q3 = results_fx_rholoss_bri.quantile(q=1)\n",
        "results_fx_CQT_bri = results_fx_CQT.groupby(results_fx_CQT.index)\n",
        "fx_CQT_median = results_fx_CQT_bri.median()\n",
        "fx_CQT_q1 = results_fx_CQT_bri.quantile(q=0)\n",
        "fx_CQT_q3 = results_fx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Dataframes for g(x)\n",
        "df_gx_CQT = pd.DataFrame(report_criteria_concat[cols_CQT])\n",
        "df_gx_rholoss = pd.DataFrame(report_criteria_concat[cols_rholoss])\n",
        "# results_gx_CQT = df_gx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_rholoss = df_gx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_CQT = df_gx_CQT.copy()\n",
        "# results_gx_rholoss = df_gx_rholoss.copy()\n",
        "\n",
        "results_gx_rholoss_bri = results_gx_rholoss.groupby(results_gx_rholoss.index)\n",
        "gx_rholoss_median = results_gx_rholoss_bri.median()\n",
        "gx_rholoss_q1 = results_gx_rholoss_bri.quantile(q=0)\n",
        "gx_rholoss_q3 = results_gx_rholoss_bri.quantile(q=1)\n",
        "results_gx_CQT_bri = results_gx_CQT.groupby(results_gx_CQT.index)\n",
        "gx_CQT_median = results_gx_CQT_bri.median()\n",
        "gx_CQT_q1 = results_gx_CQT_bri.quantile(q=0)\n",
        "gx_CQT_q3 = results_gx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_test = io.StringIO()\n",
        "# numRows = fx_median.shape[0]\n",
        "# numCols = fx_median.shape[1]\n",
        "output_test.write(\"results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\\n\")\n",
        "output_test.write(\"----------\\n\")\n",
        "\n",
        "for rho in [0.10,0.15]:\n",
        "  # output_test.write(\"rho={:.2f}\\\\\\\\\\n\".format(rho))\n",
        "\n",
        "  fx_CQT_filtered = results_fx_CQT.loc[results_fx_CQT['rho_user']==rho]\n",
        "  gx_CQT_filtered = results_gx_CQT.loc[results_gx_CQT['rho_user']==rho]\n",
        "  p_value_filtered = p_value_col[results_fx_CQT['rho_user']==rho]\n",
        "  # print(p_value_filtered)\n",
        "  n_folds = fx_CQT_filtered.shape[0]\n",
        "  row_fx = [' ', ' ',' ',r'{:.2f}'.format(rho),r'$f(x)$']\n",
        "  row_gx = [' ',' ',' ',' ',r'$g(x)$']\n",
        "\n",
        "  row_fx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_fx_rho = [r'{:.2f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_fx_loss = [r'{:.1f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_fx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q3.loc[fx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q1.loc[fx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_fx = row_fx + row_fx_CQT + row_fx_rho + row_fx_loss + row_fx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_fx)))\n",
        "\n",
        "  row_gx_CQT = [r'({:.0f},{:.0f},{:.0f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_gx_rho = [r'{:.2f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_gx_loss = [r'{:.1f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_gx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q3.loc[gx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q1.loc[gx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_gx = row_gx + row_gx_CQT + row_gx_rho + row_gx_loss + row_gx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_gx)))\n",
        "  output_test.write(\"\\\\cline{4-9}\\n\")\n",
        "\n",
        "\n",
        "print(output_test.getvalue())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\n",
            "----------\n",
            "  &   &   & 0.10 & $f(x)$ & (242,186,1021) & 0.10 & 17.6 & 0.74(0.77-0.72)\\\\\n",
            "  &   &   &   & $g(x)$ & (264,190,1021) & 0.11 & 18.6 & 0.71(0.72-0.65)\\\\\n",
            "\\cline{4-9}\n",
            "  &   &   & 0.15 & $f(x)$ & (386,281,1021) & 0.15 & 27.5 & 0.71(0.75-0.69)\\\\\n",
            "  &   &   &   & $g(x)$ & (383,263,1021) & 0.15 & 25.2 & 0.69(0.69-0.63)\\\\\n",
            "\\cline{4-9}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQeWpMFeoR5I"
      },
      "source": [
        "##%\n",
        "# Boxplot (loss reduction in test set)\n",
        "report_table_concat = pd.concat(report_table)\n",
        "cols_table = ['p_value','rho_user','%reduction_test']\n",
        "df_boxplot_table = pd.DataFrame(report_table_concat[cols_table])\n",
        "df_boxplot_table['label'] = df_boxplot_table.shape[0]*['$f(x)$']\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "columns_crit = ['rho_user','%reduction_test']\n",
        "df_boxplot_crit = pd.DataFrame(report_criteria_concat[columns_crit])\n",
        "df_boxplot_crit['label'] = df_boxplot_crit.shape[0]*['$g(x)$']\n",
        "# p-value median\n",
        "p_value_col = df_boxplot_table['p_value'] #Add\n",
        "p_value_by_row_index = df_boxplot_table['p_value'].groupby(df_boxplot_table.index)\n",
        "p_value_median = p_value_by_row_index.median()\n",
        "# Boxplot (jaccard index in test set)\n",
        "columns_jac = ['rho_user','jaccard']\n",
        "df_jaccard = pd.DataFrame(report_criteria_concat[columns_jac])\n",
        "# Unfiltered Result dataframes\n",
        "cols_fx = ['rho_user','%reduction_val','budget','%reduction_test']\n",
        "results_fx = pd.DataFrame(report_table_concat[cols_fx])\n",
        "cols_fxgx = ['rho_user','%reduction_test', 'jaccard']\n",
        "results_fxgx = pd.concat([df_boxplot_table[cols_fxgx[:2]], df_boxplot_crit[cols_fxgx[1]], df_jaccard[cols_fxgx[2]]], axis=1)\n",
        "# Filter experiments with p_value > 0.05\n",
        "df_boxplot_crit = df_boxplot_crit.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_jaccard = df_jaccard.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_boxplot_table = df_boxplot_table.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "# Boxplot with filtered values only\n",
        "frames = [df_boxplot_table, df_boxplot_crit]\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U9UhCBCvoXT0"
      },
      "source": [
        "# Avoid plotting when median(p_value)>0.5\n",
        "for i in range(p_value_median.shape[0]):\n",
        "    if p_value_median.iloc[i]>0.05:\n",
        "      df.loc[df.index==i,'%reduction_test'] = np.nan\n",
        "      df_jaccard.loc[df_jaccard.index==i, 'jaccard'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-zU6_XqoZ9N"
      },
      "source": [
        "# Dataframe for results f(x)\n",
        "results_fx = results_fx.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_by_row_index = results_fx.groupby(results_fx.index)\n",
        "fx_median = results_fx_by_row_index.median()\n",
        "fx_q1 = results_fx_by_row_index.quantile(q=0.25)\n",
        "fx_q3 = results_fx_by_row_index.quantile(q=0.75)\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_fx = io.StringIO()\n",
        "numRows = fx_median.shape[0]\n",
        "numCols = fx_median.shape[1]\n",
        "output_fx.write(\"results_fx (\\\\rho|%reduction_val|sig_rate|%reduction_test|H0)\\n\")\n",
        "output_fx.write(\"----------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==2)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fx_median.iloc[i],fx_q1.iloc[i],fx_q3.iloc[i],range(numCols))]\n",
        "  output_fx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fx.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4obWyq2UodSY"
      },
      "source": [
        "# Dataframe for comparison f(x)-g(x)\n",
        "results_fxgx = results_fxgx.loc[p_value_col <= 0.05].copy()\n",
        "results_fxgx_by_row_index = results_fxgx.groupby(results_fxgx.index)\n",
        "fxgx_median = results_fxgx_by_row_index.median()\n",
        "fxgx_q1 = results_fxgx_by_row_index.quantile(q=0.25)\n",
        "fxgx_q3 = results_fxgx_by_row_index.quantile(q=0.75)\n",
        "# Baseline comparison statistics (median(q1-q3)) LaTex\n",
        "output_fxgx = io.StringIO()\n",
        "numRows = fxgx_median.shape[0]\n",
        "numCols = fxgx_median.shape[1]\n",
        "output_fxgx.write(\"results_fxgx (\\\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\\n\")\n",
        "output_fxgx.write(\"------------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==3)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fxgx_median.iloc[i],fxgx_q1.iloc[i],fxgx_q3.iloc[i],range(numCols))]\n",
        "  output_fxgx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fxgx.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "worAyJtSopME"
      },
      "source": [
        "#%%\n",
        "# Save results in csv fomat\n",
        "path_csv = \"drive/My Drive/NIPS2020/results/cifar10/results_{clf}_yhat{yhat}_pca{pca}.csv\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "results = pd.concat([results_fx, results_fxgx, p_value_col.loc[p_value_col <= 0.05]], keys=['fx', 'fxgx', ''], axis=1).to_csv(path_csv, index=True, header=True)\n",
        "# modified output\n",
        "# Save results in tex fomat\n",
        "L = [output_fx.getvalue(),output_fxgx.getvalue()]\n",
        "path_txt = \"drive/My Drive/NIPS2020/results/cifar10/results_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(L) \n",
        "txt.close() #to change file access modes"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dt4yHhb1o6Ck"
      },
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(15, 5.1), constrained_layout=False, dpi=90)\n",
        "pal = sns.color_palette('Paired')\n",
        "sns.boxplot(x=df['rho_user'], y=df['%reduction_test'], hue='label', data=df, ax=ax[0], palette=pal)\n",
        "ax[0].set_xlabel(r'budget $\\rho$')\n",
        "ax[0].set_ylabel(r'Loss reduction $r_{test}(\\%)$')\n",
        "ax[0].legend(loc='upper left')\n",
        "pal = sns.color_palette('BuGn_r')\n",
        "sns.boxplot(x=df_jaccard['rho_user'], y=df_jaccard['jaccard'], data=df_jaccard, ax=ax[1], palette=pal)\n",
        "ax[1].set_xlabel(r'budget $\\rho$')\n",
        "ax[1].set_ylabel(r'Jaccard index $J$')\n",
        "plt.tight_layout()\n",
        "path_fig_fxgx = \"drive/My Drive/NIPS2020/results/cifar10/fig_fxgx_{clf}_yhat{yhat}_pca{pca}.pdf\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fxgx, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mj6x1QVqcRgO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0d807049-4657-49c4-f7eb-24e551a39d27"
      },
      "source": [
        "rho = 0.15\n",
        "X_test_best = X_test_best.squeeze()\n",
        "rule = table_best.loc[table_best.rho_user == rho]['rule'].to_numpy()\n",
        "eta = table_best.loc[table_best.rho_user == rho]['eta'].to_numpy()[0]\n",
        "theta = crit_table_best.loc[crit_table_best.rho_user == rho]['thresh'].to_numpy()[0]\n",
        "f_test = exp_best.gpr_mean_test+rule*np.sqrt(exp_best.gpr_var_test)\n",
        "top_n = 5 # Top n selected instances in test set\n",
        "top_f_idx = np.argpartition(f_test, -top_n)[-top_n:]\n",
        "top_f_idx = top_f_idx[np.argsort(f_test[top_f_idx])[::-1]]# Added\n",
        "# p_test = np.concatenate((y_test_pred_soft_best,1-y_test_pred_soft_best),axis=1)\n",
        "crit_test = entropy(y_test_pred_soft_best, axis=1, base=10)\n",
        "top_crit_idx = np.argpartition(crit_test, -top_n)[-top_n:]\n",
        "top_crit_idx = top_crit_idx[np.argsort(crit_test[top_crit_idx])[::-1]]# Added\n",
        "\n",
        "output_text = io.StringIO()\n",
        "print('eta={:.3f},theta={:.3f}'.format(eta,theta))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "eta=0.538,theta=0.722\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BAt0-8IaWG2c"
      },
      "source": [
        "roc_f = metrics.roc_auc_score(exp_best.L_test, f_test)\n",
        "roc_crit = metrics.roc_auc_score(exp_best.L_test, crit_test)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PJlMUtf_wTWa"
      },
      "source": [
        "# Plot selected instances\n",
        "# initialize the label \n",
        "labelNames = ['airplane', 'auto', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "# X_test_best = X_test.reshape(-1,28,28)\n",
        "# y_hat_best = y_test_pred_th[top_f_idx]\n",
        "# y_th_best = np.argmax(y_test[top_f_idx], axis=1)\n",
        "# Plot instances\n",
        "row,col = 2,top_n\n",
        "fig1, ax = plt.subplots(row, col, figsize=(6.0, 3.5), constrained_layout=True, dpi=120)\n",
        "# fig1.subplots_adjust(wspace=0.1, hspace=0.35, top=0.5)\n",
        "# for i in top_f_idx:\n",
        "j = 0\n",
        "for i,k in zip(top_f_idx, top_crit_idx):\n",
        "    if np.argmax(y_test_best, axis=1)[i] != y_test_pred_th_best[i]:\n",
        "        color = 'red'\n",
        "    else:\n",
        "        color = 'green'\n",
        "    # ax = fig1.add_subplot(3, 4, j+1)\n",
        "    ax[0][j].imshow(X_test_best[i, :, :, :])\n",
        "    ax[0][j].set_title(r'$y=${s1}'.format(s1=labelNames[np.argmax(y_test_best, axis=1)[i]])+',\\n'+\\\n",
        "                    r'$\\hat{{y}}=${s2}'.format(s2=labelNames[y_test_pred_th_best[i]]), color=color, fontsize=13)\n",
        "    ax[0][j].set_xlabel(r\"$g(x)=${:.2f}\".format(crit_test[i])+'\\n$f(x)=${:.2f}'.format(f_test[i]), fontsize=13)\n",
        "    ax[0][j].set_xticks([])\n",
        "    ax[0][j].set_yticks([])\n",
        "\n",
        "    if np.argmax(y_test_best, axis=1)[k] != y_test_pred_th_best[k]:\n",
        "        color = 'red'\n",
        "    else:\n",
        "        color = 'green'\n",
        "    # ax = fig1.add_subplot(3, 4, j+1)\n",
        "    ax[1][j].imshow(X_test_best[k, :, :, :])\n",
        "    ax[1][j].set_title(r'$y=${s1}'.format(s1=labelNames[np.argmax(y_test_best, axis=1)[k]])+',\\n'+\\\n",
        "                    r'$\\hat{{y}}=${s2}'.format(s2=labelNames[y_test_pred_th_best[k]]), color=color, fontsize=13)\n",
        "    ax[1][j].set_xlabel(r\"$g(x)=${:.2f}\".format(crit_test[k])+'\\n$f(x)=${:.2f}'.format(f_test[k]), fontsize=13)\n",
        "    ax[1][j].set_xticks([])\n",
        "    ax[1][j].set_yticks([])\n",
        "    j = j + 1\n",
        "fig1.set_constrained_layout_pads(w_pad=0, h_pad=0, hspace=0.01, wspace=-.5)\n",
        "ax[0][0].set_ylabel(r\"$f(x)>\\eta$\"+\"\\n\"+r\"$(AUC={:.2f})$\".format(roc_f), fontsize=13)\n",
        "# ax[0][0].set_ylabel(r\"$f(x)>\\eta$\", fontsize=14)\n",
        "ax[1][0].set_ylabel(r\"$g(x)>\\theta$\"+\"\\n\"+r\"$(AUC={:.2f})$\".format(roc_crit), fontsize=13)\n",
        "# ax[1][0].set_ylabel(r\"$g(x)>\\theta$\", fontsize=14)\n",
        "\n",
        "# fig1.text(0.5, 0.01, r'$\\rho={},~|f(x)>\\eta|={},~|g(x)>\\theta|={}$'.format(rho,np.sum(f_test>eta),np.sum(crit_test>theta)), ha='center', fontsize = 12)\n",
        "# plt.suptitle(r'Top {} selected instances'.format(top_n), fontsize=15)\n",
        "# plt.tight_layout()\n",
        "path_fig_fxgx_test = \"drive/My Drive/NIPS2020/results/cifar10/fig_fxgx_test_{clf}_yhat{yhat}_pca{pca}.svg\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fxgx_test, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}