{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "anaconda-cloud": {},
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.1"
    },
    "colab": {
      "name": "Copy of socialadsnet-NOTAS.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cesar-claros/synergistic/blob/master/socialadsnet_NOTAS.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iRCkuTVcGMHA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f0e83c59-ed01-4122-e326-e4d902908379"
      },
      "source": [
        "! pip install torch\n",
        "! pip install gpytorch"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch) (3.7.4.3)\n",
            "Requirement already satisfied: gpytorch in /usr/local/lib/python3.7/dist-packages (1.5.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from gpytorch) (0.22.2.post1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.4.1)\n",
            "Requirement already satisfied: torch>=1.8.1 in /usr/local/lib/python3.7/dist-packages (from gpytorch) (1.9.0+cu102)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.8.1->gpytorch) (3.7.4.3)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.0.1)\n",
            "Requirement already satisfied: numpy>=1.11.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->gpytorch) (1.19.5)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cDjK9jQGhIk7"
      },
      "source": [
        "#! sudo apt-get install texlive-latex-recommended #1\n",
        "#! sudo apt-get install dvipng texlive-fonts-recommended #2\n",
        "#! wget http://mirrors.ctan.org/macros/latex/contrib/type1cm.zip #3\n",
        "#! unzip type1cm.zip -d /tmp/type1cm #4\n",
        "#! cd /tmp/type1cm/type1cm/ && sudo latex type1cm.ins  #5\n",
        "#! sudo mkdir /usr/share/texmf/tex/latex/type1cm #6\n",
        "#! sudo cp /tmp/type1cm/type1cm/type1cm.sty /usr/share/texmf/tex/latex/type1cm #7\n",
        "#! sudo texhash #8"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZYazArWBNu7h",
        "outputId": "4ebc66d2-0015-4196-b97b-f1e89c334bd7"
      },
      "source": [
        "# NL - Trust Score for python 3; Change xrange for range on methods fit and get_score\n",
        "! git clone https://github.com/google/TrustScore\n",
        "% cd TrustScore/\n",
        "from sklearn.neighbors import KDTree\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from trustscore import TrustScore\n",
        "\n",
        "class trust_score(TrustScore): # NL\n",
        "    def __init__(self,k=10, alpha=0., filtering=\"none\", min_dist=1e-12):\n",
        "        super().__init__(k,alpha,filtering,min_dist)\n",
        "    def fit(self, X, y):\n",
        "      \"\"\"Initialize trust score precomputations with training data.\n",
        "      WARNING: assumes that the labels are 0-indexed (i.e.\n",
        "      0, 1,..., n_labels-1).\n",
        "      Args:\n",
        "      X: an array of sample points.\n",
        "      y: corresponding labels.\n",
        "      \"\"\"\n",
        "      self.n_labels = np.max(y) + 1\n",
        "      self.kdtrees = [None] * self.n_labels\n",
        "      if self.filtering == \"uncertainty\":\n",
        "        X_filtered, y_filtered = self.filter_by_uncertainty(X, y)\n",
        "      for label in range(self.n_labels):\n",
        "        if self.filtering == \"none\":\n",
        "          X_to_use = X[np.where(y == label)[0]]\n",
        "          self.kdtrees[label] = KDTree(X_to_use)\n",
        "        elif self.filtering == \"density\":\n",
        "          X_to_use = self.filter_by_density(X[np.where(y == label)[0]])\n",
        "          self.kdtrees[label] = KDTree(X_to_use)\n",
        "        elif self.filtering == \"uncertainty\":\n",
        "          X_to_use = X_filtered[np.where(y_filtered == label)[0]]\n",
        "          self.kdtrees[label] = KDTree(X_to_use)\n",
        "\n",
        "        if len(X_to_use) == 0:\n",
        "          print(\n",
        "              \"Filtered too much or missing examples from a label! Please lower \"\n",
        "              \"alpha or check data.\")\n",
        "\n",
        "    def get_score(self, X, y_pred):\n",
        "      \"\"\"Compute the trust scores.\n",
        "      Given a set of points, determines the distance to each class.\n",
        "      Args:\n",
        "      X: an array of sample points.\n",
        "      y_pred: The predicted labels for these points.\n",
        "      Returns:\n",
        "      The trust score, which is ratio of distance to closest class that was not\n",
        "      the predicted class to the distance to the predicted class.\n",
        "      \"\"\"\n",
        "      d = np.tile(None, (X.shape[0], self.n_labels))\n",
        "      for label_idx in range(self.n_labels):\n",
        "        d[:, label_idx] = self.kdtrees[label_idx].query(X, k=2)[0][:, -1]\n",
        "\n",
        "      sorted_d = np.sort(d, axis=1)\n",
        "      d_to_pred = d[range(d.shape[0]), y_pred]\n",
        "      d_to_closest_not_pred = np.where(sorted_d[:, 0] != d_to_pred,\n",
        "                                      sorted_d[:, 0], sorted_d[:, 1])\n",
        "      return d_to_closest_not_pred / (d_to_pred + self.min_dist)\n",
        "% cd .. \n",
        "# help(trust_score)"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'TrustScore'...\n",
            "remote: Enumerating objects: 14, done.\u001b[K\n",
            "Unpacking objects:   7% (1/14)   \rUnpacking objects:  14% (2/14)   \rUnpacking objects:  21% (3/14)   \rUnpacking objects:  28% (4/14)   \rUnpacking objects:  35% (5/14)   \rUnpacking objects:  42% (6/14)   \rUnpacking objects:  50% (7/14)   \rUnpacking objects:  57% (8/14)   \rUnpacking objects:  64% (9/14)   \rUnpacking objects:  71% (10/14)   \rUnpacking objects:  78% (11/14)   \rUnpacking objects:  85% (12/14)   \rUnpacking objects:  92% (13/14)   \rremote: Total 14 (delta 0), reused 0 (delta 0), pack-reused 14\u001b[K\n",
            "Unpacking objects: 100% (14/14)   \rUnpacking objects: 100% (14/14), done.\n",
            "/content/synergistic/TrustScore\n",
            "/content/synergistic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BqKVCUlTk4jf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "12320d9f-74a7-4bdd-fa78-956d0eab22a6"
      },
      "source": [
        "! git clone https://github.com/cesar-claros/synergistic\n",
        "% cd synergistic/"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Cloning into 'synergistic'...\n",
            "remote: Enumerating objects: 185, done.\u001b[K\n",
            "remote: Counting objects: 100% (185/185), done.\u001b[K\n",
            "remote: Compressing objects: 100% (182/182), done.\u001b[K\n",
            "remote: Total 185 (delta 80), reused 10 (delta 0), pack-reused 0\u001b[K\n",
            "Receiving objects: 100% (185/185), 11.46 MiB | 7.83 MiB/s, done.\n",
            "Resolving deltas: 100% (80/80), done.\n",
            "/content/synergistic/synergistic\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wAamzzLphhQg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5cd97e9-cadd-4d0b-b1c9-9c839833e21f"
      },
      "source": [
        "%load_ext tensorboard"
      ],
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The tensorboard extension is already loaded. To reload it, use:\n",
            "  %reload_ext tensorboard\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ml6Vk437FAHd"
      },
      "source": [
        "#%%\n",
        "# Imports\n",
        "import io #Used as buffer\n",
        "import sys\n",
        "import matplotlib\n",
        "# matplotlib.use('qt5Agg')\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import auxfunc.funcs as sgn\n",
        "import seaborn as sns\n",
        "import torch\n",
        "import tensorflow as tf\n",
        "from scipy.stats import entropy, spearmanr\n",
        "from sklearn import model_selection, svm, ensemble, linear_model, pipeline, decomposition,\\\n",
        "     tree, neighbors, discriminant_analysis, gaussian_process, preprocessing\n",
        "from sklearn.gaussian_process.kernels import ConstantKernel, RBF, Matern\n",
        "plt.style.use(['ggplot','style/style.mplstyle'])\n",
        "import os\n",
        "import random"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWoZGvD-QNwr"
      },
      "source": [
        "#%%\n",
        "# MODELS\n",
        "# ====================\n",
        "# Grid search for parameters and classifiers\n",
        "\n",
        "models = { \n",
        "        'predictor':\n",
        "            [\n",
        "            svm.SVC(),\n",
        "            gaussian_process.GaussianProcessClassifier(),\n",
        "            linear_model.LinearRegression(),\n",
        "            linear_model.Lasso(),\n",
        "            svm.SVR()],\n",
        "        'name':\n",
        "            [\n",
        "            'SVM',\n",
        "            'GPClassifier',\n",
        "            'LinReg',\n",
        "            'Lasso',\n",
        "            'SVR']}\n",
        "parameters = [\n",
        "            {'SVM__kernel':['poly'],'SVM__degree':[3,4,5]},\n",
        "            {'GPClassifier__kernel':[]},\n",
        "            {},\n",
        "            {'Lasso__alpha':np.linspace(0.01,1,10)},\n",
        "            {'SVR__kernel':['linear'], 'SVR__C':np.logspace(-1, 1, 10), 'SVR__epsilon':np.logspace(-2, 2, 10)} ]\n"
      ],
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P5kgx4mqidzz"
      },
      "source": [
        "#%%\n",
        "# Signaling function fitting and evaluation\n",
        "def signalingFunction(X_train, y_train, y_train_pred_th, X_val, y_val, y_val_pred_th, X_test, y_test, y_test_pred_th, kernel='exponential', norm='l01'):\n",
        "    # X_train, X_val should be scaled\n",
        "    # Fit signaling function \n",
        "    exp = sgn.signaling(norm=norm) # idx = [train,test,val]\n",
        "    exp.fit(X_train, y_train, y_train_pred_th, kernel=kernel, n_iter=500, lr=0.01)\n",
        "    table_val = exp.evaluate(X_val, y_val, y_val_pred_th, rule_grid=np.linspace(0,3,30, endpoint=False), rho_grid=[0.1, 0.15])\n",
        "    table_test = exp.test(X_test, y_test, y_test_pred_th, table_val['rule'].to_numpy(), table_val['eta'].to_numpy())\n",
        "    table = pd.concat([table_val,table_test],axis=1)\n",
        "    return table, exp"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyfzTXQnhoR_"
      },
      "source": [
        "#%%\n",
        "# Signaling function fitting and evaluation\n",
        "def patchFunction(X_train, y_train, y_train_pred_soft, y_train_pred_th, \n",
        "                  X_val, y_val, y_val_pred_soft, y_val_pred_th, \n",
        "                  X_test, y_test, y_test_pred_soft, y_test_pred_th, \n",
        "                  kernel='e*e', ex_dim=1, norm='res'):\n",
        "    # X_train, X_val should be scaled\n",
        "    # Add predictions\n",
        "    X_train_GP = np.concatenate((X_train, y_train_pred_soft), axis=1)\n",
        "    X_val_GP = np.concatenate((X_val, y_val_pred_soft), axis=1)\n",
        "    X_test_GP = np.concatenate((X_test, y_test_pred_soft), axis=1)\n",
        "    # Scale features\n",
        "    scaleX_GP = preprocessing.StandardScaler().fit(np.concatenate((X_train_GP, X_val_GP), axis=0))\n",
        "    X_train_GP = scaleX_GP.transform(X_train_GP)\n",
        "    X_val_GP = scaleX_GP.transform(X_val_GP)\n",
        "    X_test_GP = scaleX_GP.transform(X_test_GP)\n",
        "    # Fit signaling function \n",
        "    patch = sgn.patching(norm=norm) # idx = [train,test,val]\n",
        "    patch.fit(X_train_GP, y_train, y_train_pred_soft[:,1], kernel=kernel, n_iter=500, lr=0.01, ex_dim=ex_dim)\n",
        "    table_patch_only = patch.apply(X_val_GP, y_val, y_val_pred_soft, y_val_pred_th, \n",
        "                    X_test_GP, y_test, y_test_pred_soft, y_test_pred_th)\n",
        "    table_patch_val = patch.evaluate(y_val, y_val_pred_soft, y_val_pred_th)\n",
        "    table_patch_test = patch.test(y_test, y_test_pred_soft, y_test_pred_th, table_patch_val['thresh'].to_numpy())\n",
        "    table_patch = pd.concat([table_patch_val, table_patch_test], axis=1)\n",
        "    return table_patch_only, table_patch"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T_2y7G-HiujE"
      },
      "source": [
        "#%%\n",
        "# Initialize model\n",
        "def init_model(input_dim, models, parameters, clf):\n",
        "    \n",
        "    if clf=='svm':\n",
        "      i = 0\n",
        "    elif clf=='gpc':\n",
        "      i = 1\n",
        "      kernel = 1.0 * RBF(length_scale=1.0*np.ones(input_dim)) # RFB = Squared Exponental Kernel. As length scale = [1,...,1] (vector), it is the anisotropic variant of the kernel (diferentes sigma para c dimension, también para GP regression)\n",
        "      parameters[i]['GPClassifier__kernel'].append(kernel) # Se añade a la lista de parmámetros del 'GPClassifier' el kernel recién definido\n",
        "      \n",
        "    scaler = preprocessing.StandardScaler() # Iniciación de estandarización de datos\n",
        "    steps = [('scaler', scaler), (models['name'][i], models['predictor'][i])] # Definicion de pipeline 1)Scaler, 2) (Nombre_Modelo,Llamada_Modelo)\n",
        "    ppline = pipeline.Pipeline(steps) # define the pipeline object.\n",
        "\n",
        "    clf = model_selection.GridSearchCV(ppline, param_grid=parameters[i], cv=5, iid=False) # Iniciación de Búsqueda de parámetros de optimización para el modelo seleccionado, i =0 o i=1\n",
        "    return clf"
      ],
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rYgvy9cRs1m1"
      },
      "source": [
        "#%%\n",
        "# Soft and thresholded output predictions\n",
        "def pred_output(model, X):\n",
        "    if hasattr(model, \"decision_function\"): # Si tiene el atributo \"decision_function\" (SVM lo tienen y GPC no)\n",
        "      y_pred_soft = model.best_estimator_.decision_function(X)[:,None] # Evalua la decision function en X\n",
        "      y_pred_th = model.best_estimator_.predict(X)          # Para el modelo de validación cruzada con menor error de CV (?check) predice de manera discreta el resultado purchase/no purchase\n",
        "    else:\n",
        "      y_pred_soft = model.best_estimator_.predict_proba(X)  # Para el modelo de validación cruzada con menor error de CV (?check) predice de manera continua el resultado purchase/no purchase\n",
        "      y_pred_th = model.best_estimator_.predict(X)          # Para el modelo de validación cruzada con menor error de CV (?check) predice de manera discreta el resultado purchase/no purchase\n",
        "    return y_pred_soft, y_pred_th"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZPco9Y0aHUpL"
      },
      "source": [
        "#%%\n",
        "# Jaccard similarity index\n",
        "def jaccard_similarity(list1, list2):\n",
        "    s1 = set(list1)\n",
        "    s2 = set(list2)\n",
        "    return len(s1.intersection(s2)) / len(s1.union(s2))"
      ],
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dHrKyMLGq_Is"
      },
      "source": [
        "#%%\n",
        "# Baseline comparison\n",
        "def baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf,trust_val,trust_test): #NL\n",
        "      if clf=='svm':           # SVM es un modelo en R. g(x) - Margin\n",
        "          direction = 'closer'\n",
        "          crit_val = np.abs(y_val_pred_soft.ravel())\n",
        "          crit_test = np.abs(y_test_pred_soft.ravel())\n",
        "      else:\n",
        "          direction = 'further' # Los demás son modelos en [0,1] - entropy\n",
        "          p_val = np.concatenate((y_val_pred_soft,1-y_val_pred_soft),axis=1)\n",
        "          crit_val = entropy(p_val, axis=1, base=2)\n",
        "          p_test = np.concatenate((y_test_pred_soft,1-y_test_pred_soft),axis=1)\n",
        "          crit_test = entropy(p_test, axis=1, base=2)\n",
        "      # crit_val  - valor de la signailing function alternativa correspondiente (en val)\n",
        "      # crit_test - valor de la signailing function alternativa correspondiente (en test)\n",
        "      critFunc   = sgn.critEvaluation(norm='l01',direction=direction)\n",
        "      d_val      = critFunc.evaluate(y_val, y_val_pred_th, crit_val, rho_grid=[0.1, 0.15]) # same rho grid as in 'signalingFunction'\n",
        "      d_test     = critFunc.test(y_test, y_test_pred_th, crit_test, d_val['thresh'].to_numpy())\n",
        "      # TrustScore\n",
        "      critFuncSc = sgn.critEvaluation(norm='l01',direction='closer') #NL\n",
        "      score_val  = critFuncSc.evaluate(y_val, y_val_pred_th, trust_val, rho_grid=[0.1, 0.15]) # NL\n",
        "      score_test = critFuncSc.test(y_test, y_test_pred_th, trust_test, score_val['thresh'].to_numpy()) #NL \n",
        "\n",
        "      crit_table = pd.concat([d_val,d_test],axis=1)\n",
        "      score_table= pd.concat([score_val,score_test],axis=1) #NL\n",
        "\n",
        "      gamma = table['rule'].to_numpy().reshape(-1,1) #Trae la columna de best rules en val\n",
        "      f_test = exp.gpr_mean_test + gamma*np.sqrt(exp.gpr_var_test)#Trae\n",
        "      eta = table['eta'].to_numpy().reshape(-1,1)\n",
        "      theta = crit_table['thresh'].to_numpy().reshape(-1,1)\n",
        "      if direction == 'closer':\n",
        "        f_mask, f_idx = np.nonzero(f_test>eta)\n",
        "      else:\n",
        "        f_mask, f_idx = np.nonzero(f_test<eta)\n",
        "      crit_mask, crit_idx = np.nonzero(crit_test.reshape(1,-1)<theta)\n",
        "      print(list(np.unique(f_mask)))\n",
        "      print(list(np.unique(crit_mask)))\n",
        "      print(f_test.shape[0])\n",
        "      shared = set(list(np.unique(f_mask))).intersection(set(list(np.unique(crit_mask))))\n",
        "      J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan for i in range(f_test.shape[0])]\n",
        "      # if (list(np.unique(f_mask))==list(np.unique(crit_mask))):\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) for i in np.unique(f_mask)]\n",
        "      # else:\n",
        "      #   shared = set(a).intersection(set(b))\n",
        "      #   union = set(a).union(set(b))\n",
        "      #   J = [jaccard_similarity(crit_idx[crit_mask==i],f_idx[f_mask==i]) if i in shared else np.nan  for i in union]\n",
        "      crit_table['jaccard']=J\n",
        "      Sp = [spearmanr(f_test[i,:],crit_test)[0] for i in range(f_test.shape[0])]\n",
        "      crit_table['spearman'] = Sp\n",
        "      crit_table['gamma'] = gamma\n",
        "      return crit_table"
      ],
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dkeB_8BQi1im"
      },
      "source": [
        "# For reproducibility\n",
        "os.environ['TF_DETERMINISTIC_OPS'] = '1'\n",
        "SEED = 123\n",
        "os.environ['PYTHONHASHSEED']=str(SEED)\n",
        "random.seed(SEED)\n",
        "np.random.seed(SEED)\n",
        "tf.random.set_seed(SEED)\n",
        "torch.manual_seed(SEED)\n",
        "torch.cuda.manual_seed_all(SEED)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NgYak1INQS7t"
      },
      "source": [
        "#%%\n",
        "# INITIALIZATION\n",
        "# ==============\n",
        "# EXPERIMENT SETUP\n",
        "# ================\n",
        "# Load data\n",
        "# -------------\n",
        "df = pd.read_table('datasets/Social_Network_Ads.csv')\n",
        "Data_X = df.iloc[:,[2,3]]\n",
        "Data_y = df.iloc[:,4]\n",
        "Data_X = Data_X.to_numpy()\n",
        "Data_y = Data_y.to_numpy()"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRWPIuqwRLoD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f08ab1d9-796c-48f8-a340-ed8cb599fc4f"
      },
      "source": [
        "# NL. Adición de trust score\n",
        "# Assign labels\n",
        "report_table = []\n",
        "report_criteria = []\n",
        "report_table_patch_only = []\n",
        "report_table_patch = []\n",
        "kf = model_selection.StratifiedKFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "# kf = model_selection.KFold(n_splits=5, shuffle=True, random_state=SEED)\n",
        "clf = 'gpc'\n",
        "addPredictions = True\n",
        "evaluatePatch = False\n",
        "accuracy = 0\n",
        "for sample, test in kf.split(Data_X, Data_y):\n",
        "# for sample, test in kf.split(Data_X):\n",
        "    X = Data_X[sample]    # Toma 80% de los 400 datos = 320 sujetos para las 2 covariables. Esta matriz se usa para entrenar el modelo para el trust score\n",
        "    y = Data_y[sample]    # Toma 80% de los 400 datos = 320 respuestas en {0,1}. Este vector se usa para entrenar el modelo para el trust score\n",
        "    X_train, X_val, y_train, y_val = model_selection.train_test_split(X, y, test_size=0.20, random_state=SEED) # Los 320 datos del fold actual son divididos en training (80% - 256) y testing-validation (20% - 64)\n",
        "    X_test = Data_X[test] # Toma 20% de los 400 datos = 80\n",
        "    y_test = Data_y[test] # Toma 20% de los 400 datos = 80\n",
        "\n",
        "    # TRAINING MODEL - Modelo de entrenamiento de h. En training data\n",
        "    model = init_model(input_dim=X.shape[1], models=models, parameters=parameters, clf=clf) # Determinación de objeto de validación cruzada para seleccion de modelo. En este caso, seleccion **asintropic** de Modelo clf = GPC (definido arriba)\n",
        "    model.fit(X_train, y_train) # Actual fit del modelo. Ejecución del objeto clf = model_selection.GridSearchCV(...)\n",
        "\n",
        "    y_train_pred_soft, y_train_pred_th = pred_output(model, X_train) # train - Predicción del modelo entrenado en 'train' - th {0,1}, soft [0,1] o [0,inf]\n",
        "    print('accuracy(Train)={}'.format(np.sum(y_train==y_train_pred_th)/np.size(y_train)))\n",
        "    y_val_pred_soft, y_val_pred_th = pred_output(model, X_val)       # val   - Predicción del modelo entrenado en 'train' - th {0,1}, soft [0,1] o [0,inf]\n",
        "    y_test_pred_soft, y_test_pred_th = pred_output(model, X_test)    # test  - Predicción del modelo entrenado en 'train' - th {0,1}, soft [0,1] o [0,inf]\n",
        "\n",
        "    # PATCH MODEL\n",
        "    if evaluatePatch:\n",
        "        table_patch_only, table_patch = patchFunction(X_train, y_train, y_train_pred_soft, y_train_pred_th, \n",
        "                                                               X_val, y_val, y_val_pred_soft, y_val_pred_th, \n",
        "                                                               X_test, y_test, y_test_pred_soft, y_test_pred_th,\n",
        "                                                               kernel='RBF+RBF', ex_dim=y_train_pred_soft.shape[1])\n",
        "        report_table_patch_only.append(table_patch_only)\n",
        "        report_table_patch.append(table_patch)\n",
        "\n",
        "    X_train_GP = X_train\n",
        "    X_val_GP = X_val\n",
        "    X_test_GP = X_test\n",
        "    if addPredictions: # Se añaden las predicciones del modelo en la matriz diseño para configurar \\Delta^{**}\n",
        "        # Add predictions\n",
        "        X_train_GP = np.concatenate((X_train_GP, y_train_pred_soft), axis=1) # matriz diseño train + pronostico continuo\n",
        "        X_val_GP = np.concatenate((X_val_GP, y_val_pred_soft), axis=1)       # matriz diseño val   + pronostico continuo\n",
        "        X_test_GP = np.concatenate((X_test_GP, y_test_pred_soft), axis=1)    # matriz diseño test  + pronostico continuo\n",
        "    scaleX_GP = preprocessing.StandardScaler().fit(X_train_GP) # NL - Xq concatenate Mean and std dev of [train,val] data to standarize the other data\n",
        "    X_train_GP = scaleX_GP.transform(X_train_GP)\n",
        "    X_val_GP = scaleX_GP.transform(X_val_GP)\n",
        "    X_test_GP = scaleX_GP.transform(X_test_GP)\n",
        "    \n",
        "    table, exp = signalingFunction(X_train_GP, y_train, y_train_pred_th, X_val_GP, y_val, y_val_pred_th, X_test_GP, y_test, y_test_pred_th)\n",
        "    report_table.append(table)\n",
        "    \n",
        "    # Baseline for comparison\n",
        "    # Trust Score fitted on train+val data to evaluate loss reduction on test data\n",
        "    trust_model = trust_score()\n",
        "    trust_model.fit(X=X_train_GP,y=y_train) #NL\n",
        "    trust_val  = trust_model.get_score(X_val_GP, y_val_pred_th) #NL\n",
        "    trust_test = trust_model.get_score(X_test_GP, y_test_pred_th) #NL\n",
        "                                \n",
        "    crit_table = baselineCriteria(y_val, y_val_pred_soft, y_val_pred_th, y_test, y_test_pred_soft, y_test_pred_th, table, exp, clf,trust_val,trust_test)\n",
        "    report_criteria.append(crit_table)\n",
        "\n",
        "    if accuracy < model.best_estimator_.score(X_val,y_val):\n",
        "        accuracy = model.best_estimator_.score(X_val,y_val)\n",
        "        classifier = model.best_estimator_\n",
        "        X_test_surface_plot = X_test\n",
        "        y_test_surface_plot = y_test\n",
        "        X_train_surface_plot = X_train\n",
        "        y_train_surface_plot = y_train\n",
        "        X_val_surface_plot = X_val\n",
        "        y_val_surface_plot = y_val\n",
        "        scaler_surface_plot = scaleX_GP\n",
        "        exp_surface_plot = exp\n",
        "        table_surface_plot = table\n",
        "\n",
        "    del(model)\n"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy(Train)=0.9140625\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: 0.033  noise: 0.045\n",
            "Iter 492/500 - Loss: 0.033  noise: 0.045\n",
            "Iter 493/500 - Loss: 0.033  noise: 0.045\n",
            "Iter 494/500 - Loss: 0.033  noise: 0.045\n",
            "Iter 495/500 - Loss: 0.032  noise: 0.045\n",
            "Iter 496/500 - Loss: 0.032  noise: 0.045\n",
            "Iter 497/500 - Loss: 0.033  noise: 0.044\n",
            "Iter 498/500 - Loss: 0.032  noise: 0.044\n",
            "Iter 499/500 - Loss: 0.032  noise: 0.044\n",
            "Iter 500/500 - Loss: 0.032  noise: 0.044\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy(Train)=0.921875\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: 0.001  noise: 0.041\n",
            "Iter 492/500 - Loss: 0.001  noise: 0.041\n",
            "Iter 493/500 - Loss: 0.001  noise: 0.041\n",
            "Iter 494/500 - Loss: 0.001  noise: 0.041\n",
            "Iter 495/500 - Loss: 0.001  noise: 0.041\n",
            "Iter 496/500 - Loss: 0.000  noise: 0.040\n",
            "Iter 497/500 - Loss: 0.000  noise: 0.040\n",
            "Iter 498/500 - Loss: 0.000  noise: 0.040\n",
            "Iter 499/500 - Loss: 0.000  noise: 0.040\n",
            "Iter 500/500 - Loss: -0.000  noise: 0.040\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy(Train)=0.9140625\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 492/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 493/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 494/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 495/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 496/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 497/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 498/500 - Loss: -0.056  noise: 0.032\n",
            "Iter 499/500 - Loss: -0.057  noise: 0.032\n",
            "Iter 500/500 - Loss: -0.056  noise: 0.032\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy(Train)=0.921875\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: -0.014  noise: 0.039\n",
            "Iter 492/500 - Loss: -0.014  noise: 0.038\n",
            "Iter 493/500 - Loss: -0.015  noise: 0.038\n",
            "Iter 494/500 - Loss: -0.016  noise: 0.038\n",
            "Iter 495/500 - Loss: -0.016  noise: 0.038\n",
            "Iter 496/500 - Loss: -0.017  noise: 0.038\n",
            "Iter 497/500 - Loss: -0.018  noise: 0.038\n",
            "Iter 498/500 - Loss: -0.018  noise: 0.038\n",
            "Iter 499/500 - Loss: -0.019  noise: 0.037\n",
            "Iter 500/500 - Loss: -0.020  noise: 0.037\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_search.py:823: FutureWarning: The parameter 'iid' is deprecated in 0.22 and will be removed in 0.24.\n",
            "  \"removed in 0.24.\", FutureWarning\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "accuracy(Train)=0.90234375\n",
            "lr=0.01, n_iterations=500\n",
            "Iter 491/500 - Loss: 0.130  noise: 0.054\n",
            "Iter 492/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 493/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 494/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 495/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 496/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 497/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 498/500 - Loss: 0.129  noise: 0.054\n",
            "Iter 499/500 - Loss: 0.129  noise: 0.053\n",
            "Iter 500/500 - Loss: 0.129  noise: 0.053\n",
            "[0, 1]\n",
            "[0, 1]\n",
            "2\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o47vhh_haXU9"
      },
      "source": [
        "logs_base_dir = \"runs/\"\n",
        "%tensorboard --logdir {logs_base_dir}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a7ovVre-TUCC"
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "report_table_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "R5Nui6Gqs8-z"
      },
      "source": [
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "report_criteria_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aknS5GwDyv2m"
      },
      "source": [
        "# p-value column\n",
        "p_value_col = report_table_concat['p_value'] #Add\n",
        "cols_CQT = ['rho_user','corrected_test','queries_test','total_wrong_test','loss_query_test']\n",
        "cols_rholoss = ['rho_user','rho_hat_test','%loss_red_test']\n",
        "\n",
        "# Dataframes for f(x)\n",
        "df_fx_CQT = pd.DataFrame(report_table_concat[cols_CQT])\n",
        "df_fx_rholoss = pd.DataFrame(report_table_concat[cols_rholoss])\n",
        "# results_fx_CQT = df_fx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_rholoss = df_fx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_CQT = df_fx_CQT.copy()\n",
        "# results_fx_rholoss = df_fx_rholoss.copy()\n",
        "\n",
        "results_fx_rholoss_bri = results_fx_rholoss.groupby(results_fx_rholoss.index)\n",
        "fx_rholoss_median = results_fx_rholoss_bri.median()\n",
        "fx_rholoss_q1 = results_fx_rholoss_bri.quantile(q=0)\n",
        "fx_rholoss_q3 = results_fx_rholoss_bri.quantile(q=1)\n",
        "results_fx_CQT_bri = results_fx_CQT.groupby(results_fx_CQT.index)\n",
        "fx_CQT_median = results_fx_CQT_bri.median()\n",
        "fx_CQT_q1 = results_fx_CQT_bri.quantile(q=0)\n",
        "fx_CQT_q3 = results_fx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Dataframes for g(x)\n",
        "df_gx_CQT = pd.DataFrame(report_criteria_concat[cols_CQT])\n",
        "df_gx_rholoss = pd.DataFrame(report_criteria_concat[cols_rholoss])\n",
        "# results_gx_CQT = df_gx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_rholoss = df_gx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_CQT = df_gx_CQT.copy()\n",
        "# results_gx_rholoss = df_gx_rholoss.copy()\n",
        "\n",
        "results_gx_rholoss_bri = results_gx_rholoss.groupby(results_gx_rholoss.index)\n",
        "gx_rholoss_median = results_gx_rholoss_bri.median()\n",
        "gx_rholoss_q1 = results_gx_rholoss_bri.quantile(q=0)\n",
        "gx_rholoss_q3 = results_gx_rholoss_bri.quantile(q=1)\n",
        "results_gx_CQT_bri = results_gx_CQT.groupby(results_gx_CQT.index)\n",
        "gx_CQT_median = results_gx_CQT_bri.median()\n",
        "gx_CQT_q1 = results_gx_CQT_bri.quantile(q=0)\n",
        "gx_CQT_q3 = results_gx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_test = io.StringIO()\n",
        "# numRows = fx_median.shape[0]\n",
        "# numCols = fx_median.shape[1]\n",
        "output_test.write(\"results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\\n\")\n",
        "output_test.write(\"----------\\n\")\n",
        "\n",
        "for rho in [0.10,0.15]:\n",
        "  # output_test.write(\"rho={:.2f}\\\\\\\\\\n\".format(rho))\n",
        "\n",
        "  fx_CQT_filtered = results_fx_CQT.loc[results_fx_CQT['rho_user']==rho]\n",
        "  gx_CQT_filtered = results_gx_CQT.loc[results_gx_CQT['rho_user']==rho]\n",
        "  p_value_filtered = p_value_col[results_fx_CQT['rho_user']==rho]\n",
        "  # print(p_value_filtered)\n",
        "  n_folds = fx_CQT_filtered.shape[0]\n",
        "\n",
        "  row_fx = [' ',' ',r'{:.2f}'.format(rho),r'$f(x)$']\n",
        "  row_fx_CQT = []\n",
        "  row_gx = [' ',' ',' ',r'$g(x)$']\n",
        "  row_gx_CQT = []\n",
        "  for i in range(n_folds):\n",
        "    if p_value_filtered.iloc[i] <= 0.05:\n",
        "      row_fx_CQT.append('({Q:.0f}/{C:.0f}/{T:.0f})'.format(\n",
        "                                                  Q=fx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "                                                  C=fx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "                                                  T=fx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "      row_gx_CQT.append('({Q:.0f}/{C:.0f}/{T:.0f})'.format(\n",
        "                                                  Q=gx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "                                                  C=gx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "                                                  T=gx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "    else:\n",
        "      row_fx_CQT.append('\\\\textit{{({Q:.0f}/{C:.0f}/{T:.0f})}}'.format(\n",
        "                                                  Q=fx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "                                                  C=fx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "                                                  T=fx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "      row_gx_CQT.append('\\\\textit{{({Q:.0f}/{C:.0f}/{T:.0f})}}'.format(\n",
        "                                                  Q=gx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "                                                  C=gx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "                                                  T=gx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "  row_fx.append(\"{:s}\".format(\", \".join(row_fx_CQT)))\n",
        "  row_gx.append(\"{:s}\".format(\", \".join(row_gx_CQT)))\n",
        "\n",
        "  row_fx_rho = [r'{:.2f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_fx_loss = [r'{:.1f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_fx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q3.loc[fx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q1.loc[fx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_fx = row_fx + row_fx_rho + row_fx_loss + row_fx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_fx)))\n",
        "\n",
        "  row_gx_rho = [r'{:.2f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_gx_loss = [r'{:.1f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_gx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q3.loc[gx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q1.loc[gx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_gx = row_gx + row_gx_rho + row_gx_loss + row_gx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_gx)))\n",
        "  output_test.write(\"\\\\cline{3-8}\\n\")\n",
        "\n",
        "\n",
        "print(output_test.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tlkfkkN463um"
      },
      "source": [
        "# p-value column\n",
        "p_value_col = report_table_concat['p_value'] #Add\n",
        "cols_CQT = ['rho_user','corrected_test','queries_test','total_wrong_test','loss_query_test']\n",
        "cols_rholoss = ['rho_user','rho_hat_test','%loss_red_test']\n",
        "\n",
        "# Dataframes for f(x)\n",
        "df_fx_CQT = pd.DataFrame(report_table_concat[cols_CQT])\n",
        "df_fx_rholoss = pd.DataFrame(report_table_concat[cols_rholoss])\n",
        "# results_fx_CQT = df_fx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_rholoss = df_fx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_CQT = df_fx_CQT.copy()\n",
        "# results_fx_rholoss = df_fx_rholoss.copy()\n",
        "\n",
        "results_fx_rholoss_bri = results_fx_rholoss.groupby(results_fx_rholoss.index)\n",
        "fx_rholoss_median = results_fx_rholoss_bri.median()\n",
        "fx_rholoss_q1 = results_fx_rholoss_bri.quantile(q=0)\n",
        "fx_rholoss_q3 = results_fx_rholoss_bri.quantile(q=1)\n",
        "results_fx_CQT_bri = results_fx_CQT.groupby(results_fx_CQT.index)\n",
        "fx_CQT_median = results_fx_CQT_bri.median()\n",
        "fx_CQT_q1 = results_fx_CQT_bri.quantile(q=0)\n",
        "fx_CQT_q3 = results_fx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Dataframes for g(x)\n",
        "df_gx_CQT = pd.DataFrame(report_criteria_concat[cols_CQT])\n",
        "df_gx_rholoss = pd.DataFrame(report_criteria_concat[cols_rholoss])\n",
        "# results_gx_CQT = df_gx_CQT.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_rholoss = df_gx_rholoss.loc[p_value_col <= 0.05].copy()\n",
        "results_gx_CQT = df_gx_CQT.copy()\n",
        "# results_gx_rholoss = df_gx_rholoss.copy()\n",
        "\n",
        "results_gx_rholoss_bri = results_gx_rholoss.groupby(results_gx_rholoss.index)\n",
        "gx_rholoss_median = results_gx_rholoss_bri.median()\n",
        "gx_rholoss_q1 = results_gx_rholoss_bri.quantile(q=0)\n",
        "gx_rholoss_q3 = results_gx_rholoss_bri.quantile(q=1)\n",
        "results_gx_CQT_bri = results_gx_CQT.groupby(results_gx_CQT.index)\n",
        "gx_CQT_median = results_gx_CQT_bri.median()\n",
        "gx_CQT_q1 = results_gx_CQT_bri.quantile(q=0)\n",
        "gx_CQT_q3 = results_gx_CQT_bri.quantile(q=1)\n",
        "\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_test = io.StringIO()\n",
        "# numRows = fx_median.shape[0]\n",
        "# numCols = fx_median.shape[1]\n",
        "output_test.write(\"results_test (dataset|method|(Q,C,T)|\\hat{rho_test}|%loss_red_test\\n\")\n",
        "output_test.write(\"----------\\n\")\n",
        "\n",
        "for rho in [0.10,0.15]:\n",
        "  # output_test.write(\"rho={:.2f}\\\\\\\\\\n\".format(rho))\n",
        "\n",
        "  fx_CQT_filtered = results_fx_CQT.loc[results_fx_CQT['rho_user']==rho]\n",
        "  gx_CQT_filtered = results_gx_CQT.loc[results_gx_CQT['rho_user']==rho]\n",
        "  p_value_filtered = p_value_col[results_fx_CQT['rho_user']==rho]\n",
        "  # print(p_value_filtered)\n",
        "  n_folds = fx_CQT_filtered.shape[0]\n",
        "\n",
        "  row_fx = [' ',' ',r'{:.2f}'.format(rho),r'$f(x)$']\n",
        "  row_gx = [' ',' ',' ',r'$g(x)$']\n",
        "  # for i in range(n_folds):\n",
        "  #   if p_value_filtered.iloc[i] <= 0.05:\n",
        "  #     row_fx_CQT.append('({Q:.0f}/{C:.0f}/{T:.0f})'.format(\n",
        "  #                                                 Q=fx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "  #                                                 C=fx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "  #                                                 T=fx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "  #     row_gx_CQT.append('({Q:.0f}/{C:.0f}/{T:.0f})'.format(\n",
        "  #                                                 Q=gx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "  #                                                 C=gx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "  #                                                 T=gx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "  #   else:\n",
        "  #     row_fx_CQT.append('\\\\textit{{({Q:.0f}/{C:.0f}/{T:.0f})}}'.format(\n",
        "  #                                                 Q=fx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "  #                                                 C=fx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "  #                                                 T=fx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "  #     row_gx_CQT.append('\\\\textit{{({Q:.0f}/{C:.0f}/{T:.0f})}}'.format(\n",
        "  #                                                 Q=gx_CQT_filtered['queries_test'].iloc[i],\\\n",
        "  #                                                 C=gx_CQT_filtered['corrected_test'].iloc[i],\\\n",
        "  #                                                 T=gx_CQT_filtered['total_wrong_test'].iloc[i]))\n",
        "  # row_fx.append(\"{:s}\".format(\", \".join(row_fx_CQT)))\n",
        "  # row_gx.append(\"{:s}\".format(\", \".join(row_gx_CQT)))\n",
        "\n",
        "  row_fx_CQT = [r'({:.2f},{:.2f},{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      fx_CQT_median.loc[fx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_fx_rho = [r'{:.2f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_fx_loss = [r'{:.1f}'.format(val1) for val1 in fx_rholoss_median.loc[fx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_fx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(fx_CQT_median.loc[fx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q3.loc[fx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      fx_CQT_q1.loc[fx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_fx = row_fx + row_fx_CQT + row_fx_rho + row_fx_loss + row_fx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_fx)))\n",
        "\n",
        "  row_gx_CQT = [r'({:.2f},{:.2f},{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'queries_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q3['rho_user']==rho,'corrected_test'],\\\n",
        "                                                      gx_CQT_median.loc[gx_CQT_q1['rho_user']==rho,'total_wrong_test'])]\n",
        "  row_gx_rho = [r'{:.2f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'rho_hat_test']]\n",
        "  row_gx_loss = [r'{:.1f}'.format(val1) for val1 in gx_rholoss_median.loc[gx_rholoss_median['rho_user']==rho,'%loss_red_test']]\n",
        "  row_gx_lq = [r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) for val1,val2,val3 in zip(gx_CQT_median.loc[gx_CQT_median['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q3.loc[gx_CQT_q3['rho_user']==rho,'loss_query_test'],\\\n",
        "                                                      gx_CQT_q1.loc[gx_CQT_q1['rho_user']==rho,'loss_query_test'])]\n",
        "  row_gx = row_gx + row_gx_CQT + row_gx_rho + row_gx_loss + row_gx_lq\n",
        "  output_test.write(\"{:s}\\\\\\\\\\n\".format(\" & \".join(row_gx)))\n",
        "  output_test.write(\"\\\\cline{3-8}\\n\")\n",
        "\n",
        "\n",
        "print(output_test.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-FDWh7K2liJm"
      },
      "source": [
        "# report_table_patch_only\n",
        "report_table_patch_only_concat = pd.concat(report_table_patch_only)\n",
        "report_table_patch_only_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_SeNIW53ljJA"
      },
      "source": [
        "# report_table_patch\n",
        "report_table_patch_concat = pd.concat(report_table_patch)\n",
        "report_table_patch_concat"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lwtO-FSKiFJk"
      },
      "source": [
        "p_value_col"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Iph2NBAn59EW"
      },
      "source": [
        "##%\n",
        "# Boxplot (loss reduction in test set)\n",
        "report_table_concat = pd.concat(report_table)\n",
        "cols_table = ['p_value','rho_user','%reduction_test']\n",
        "df_boxplot_table = pd.DataFrame(report_table_concat[cols_table])\n",
        "df_boxplot_table['label'] = df_boxplot_table.shape[0]*['$f(x)$']\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "columns_crit = ['rho_user','%reduction_test']\n",
        "df_boxplot_crit = pd.DataFrame(report_criteria_concat[columns_crit])\n",
        "df_boxplot_crit['label'] = df_boxplot_crit.shape[0]*['$g(x)$']\n",
        "# p-value median\n",
        "p_value_col = df_boxplot_table['p_value'] #Add\n",
        "p_value_by_row_index = df_boxplot_table['p_value'].groupby(df_boxplot_table.index)\n",
        "p_value_median = p_value_by_row_index.median()\n",
        "# Boxplot (jaccard index in test set)\n",
        "columns_jac = ['rho_user','jaccard']\n",
        "df_jaccard = pd.DataFrame(report_criteria_concat[columns_jac])\n",
        "# Unfiltered Result dataframes\n",
        "cols_fx = ['rho_user','%reduction_val','budget','%reduction_test']\n",
        "results_fx = pd.DataFrame(report_table_concat[cols_fx])\n",
        "cols_fxgx = ['rho_user','%reduction_test', 'jaccard']\n",
        "results_fxgx = pd.concat([df_boxplot_table[cols_fxgx[:2]], df_boxplot_crit[cols_fxgx[1]], \\\n",
        "                          df_jaccard[cols_fxgx[2]]], axis=1)\n",
        "# Filter experiments with p_value > 0.05\n",
        "df_boxplot_table = df_boxplot_table.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_boxplot_crit = df_boxplot_crit.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "df_jaccard = df_jaccard.loc[df_boxplot_table['p_value'] <= 0.05]\n",
        "# Boxplot with filtered values only\n",
        "frames = [df_boxplot_table, df_boxplot_crit]\n",
        "df = pd.concat(frames)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hUAysqka1Rzo"
      },
      "source": [
        "# Avoid plotting when median(p_value)>0.5\n",
        "for i in range(p_value_median.shape[0]):\n",
        "    if p_value_median.iloc[i]>0.05:\n",
        "      df.loc[df.index==i,'%reduction_test'] = np.nan\n",
        "      df_jaccard.loc[df_jaccard.index==i, 'jaccard'] = np.nan"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oTcWIjp_ZN6U"
      },
      "source": [
        "# Dataframe for results f(x)\n",
        "results_fx = results_fx.loc[p_value_col <= 0.05].copy()\n",
        "results_fx_by_row_index = results_fx.groupby(results_fx.index)\n",
        "fx_median = results_fx_by_row_index.median()\n",
        "fx_q1 = results_fx_by_row_index.quantile(q=0.25)\n",
        "fx_q3 = results_fx_by_row_index.quantile(q=0.75)\n",
        "# Signaling function statistics (median(q1-q3)) LaTex\n",
        "output_fx = io.StringIO()\n",
        "numRows = fx_median.shape[0]\n",
        "numCols = fx_median.shape[1]\n",
        "output_fx.write(\"results_fx (\\\\rho|%reduction_val|sig_rate|%reduction_test|H0)\\n\")\n",
        "output_fx.write(\"----------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==2)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fx_median.iloc[i],fx_q1.iloc[i],fx_q3.iloc[i],range(numCols))]\n",
        "  output_fx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fx.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4nVVPf7G2Gpf"
      },
      "source": [
        "# Dataframe for comparison f(x)-g(x)\n",
        "results_fxgx = results_fxgx.loc[p_value_col <= 0.05]\n",
        "results_fxgx_by_row_index = results_fxgx.groupby(results_fxgx.index)\n",
        "fxgx_median = results_fxgx_by_row_index.median()\n",
        "fxgx_q1 = results_fxgx_by_row_index.quantile(q=0.25)\n",
        "fxgx_q3 = results_fxgx_by_row_index.quantile(q=0.75)\n",
        "# Baseline comparison statistics (median(q1-q3)) LaTex\n",
        "output_fxgx = io.StringIO()\n",
        "numRows = fxgx_median.shape[0]\n",
        "numCols = fxgx_median.shape[1]\n",
        "output_fxgx.write(\"results_fxgx (\\\\rho|%reduction_test(fx)|%reduction_test(fxgx)|Jaccard|H0\\n\")\n",
        "output_fxgx.write(\"------------\\n\")\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.2f}'.format(val1) if p_value_median[i]>0.05 and j==0 else r'{}' if p_value_median[i]>0.05 and j!=0\\\n",
        "         else r'{:.2f}'.format(val1) if (j==0) else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3) if (j==3)\\\n",
        "         else r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) for val1,val2,val3,j in zip(fxgx_median.iloc[i],fxgx_q1.iloc[i],fxgx_q3.iloc[i],range(numCols))]\n",
        "  output_fxgx.write(\"{{}} & {{}} & %s & {H0} \\\\\\\\\\n\".format(H0=r'$\\surd$' if p_value_median[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output_fxgx.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d7W1Ln5-dm5t"
      },
      "source": [
        "#%%\n",
        "# Save results in csv fomat\n",
        "path_csv = \"drive/My Drive/NIPS2020/results/socialadsnet/results_{clf}_yhat{yhat}_pca{pca}.csv\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "results = pd.concat([results_fx, results_fxgx, p_value_col.loc[p_value_col <= 0.05]], keys=['fx', 'fxgx', ''], axis=1).to_csv(path_csv, index=True, header=True)\n",
        "# modified output\n",
        "# Save results in tex fomat\n",
        "L = [output_fx.getvalue(),output_fxgx.getvalue()]\n",
        "path_txt = \"drive/My Drive/NIPS2020/results/socialadsnet/results_{clf}_yhat{yhat}_pca{pca}.txt\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "txt = open(path_txt, \"w\") \n",
        "txt.writelines(L) \n",
        "txt.close() #to change file access modes "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WCiI4764Haf8"
      },
      "source": [
        "fig, ax = plt.subplots(1,2,figsize=(15, 5.1), constrained_layout=False, dpi=90)\n",
        "pal = sns.color_palette('Paired')\n",
        "sns.boxplot(x=df['rho_user'], y=df['%reduction_test'], hue='label', data=df, ax=ax[0], palette=pal)\n",
        "ax[0].set_xlabel(r'budget $\\rho$')\n",
        "ax[0].set_ylabel(r'Loss reduction $r_{test}(\\%)$')\n",
        "ax[0].legend(loc='upper left')\n",
        "pal = sns.color_palette('BuGn_r')\n",
        "sns.boxplot(x=df_jaccard['rho_user'], y=df_jaccard['jaccard'], data=df_jaccard, ax=ax[1], palette=pal)\n",
        "ax[1].set_xlabel(r'budget $\\rho$')\n",
        "ax[1].set_ylabel(r'Jaccard index $J$')\n",
        "plt.tight_layout()\n",
        "path_fig_fxgx = \"drive/My Drive/NIPS2020/results/socialadsnet/fig_fxgx_{clf}_yhat{yhat}_pca{pca}.pdf\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fxgx, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Rw8ao37-ymu6"
      },
      "source": [
        "  #%%\n",
        "# PLOT DECISION SURFACE\n",
        "# ==================\n",
        "# Plot test instances and decision surface\n",
        "# ----------------------------------------------\n",
        "# Visualising the Train set results\n",
        "fig1 = plt.figure(figsize=(10,8),dpi=120)\n",
        "ax1 = fig1.add_subplot(111)\n",
        "X_set, y_set = X_train_surface_plot, y_train_surface_plot\n",
        "y_set[y_set==0] = -1\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d_set = 1-y_set*classifier.decision_function(X_set)\n",
        "else:\n",
        "    pred_x = classifier.predict(X_set)\n",
        "    pred_x[pred_x==0] = -1\n",
        "    d_set = y_set*pred_x\n",
        "xi_set = np.max([[np.zeros(d_set.size)],[d_set]],axis=0).ravel()\n",
        "\n",
        "aranged_ages = np.arange(start = X_set[:, 0].min()-5, stop = X_set[:, 0].max()+5, step = 0.025)\n",
        "aranged_salaries = np.arange(start = X_set[:, 1].min()-4000, stop = X_set[:, 1].max()+4000, step = 500)\n",
        "\n",
        "X1, X2 = np.meshgrid(aranged_ages, aranged_salaries)\n",
        "Z = classifier.predict(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d = classifier.decision_function(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "    ax1.contourf(X1, X2, np.where((np.abs(d)<1),np.abs(d),np.nan), alpha = 0.6, cmap='gist_gray', label='margin region')\n",
        "    ax1.contour(X1, X2, d, levels=[-1, 0, 1], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set>1,0]\n",
        "    y = X_set[xi_set>1,1]\n",
        "else:\n",
        "    d = classifier.predict_proba(np.array([X1.ravel(), X2.ravel()]).T)[:, 1].reshape(X1.shape)-0.5\n",
        "    ax1.contourf(X1, X2, np.where((np.abs(d)<0.25),np.abs(d),np.nan), alpha = 0.6, cmap='gist_gray', label='margin region')\n",
        "    ax1.contour(X1, X2, d, levels=[-0.25, 0.0, 0.25], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set==0,0]\n",
        "    y = X_set[xi_set==0,1]\n",
        "dots = ['red','blue']\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    ax1.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                color = dots[i], label = '{}'.format('no purchase' if i==0 else 'purchase'),alpha=0.7, s=65, linewidths=5)\n",
        "# Plot slack variables magnitudes\n",
        "ax1.scatter(x, y, s=150, facecolors='none', edgecolors='g', label='misclassified', linewidths=5)\n",
        "\n",
        "ax1.set_xlabel('Age', fontsize=40)\n",
        "ax1.set_ylabel('Salary', fontsize=40)\n",
        "ax1.set_title('Training set', fontsize=40)\n",
        "ax1.legend(loc='lower left', framealpha=0.5, prop={'size': 30}, labelspacing=0.0)\n",
        "formatter0 = matplotlib.ticker.EngFormatter()\n",
        "ax1.yaxis.set_major_formatter(formatter0)\n",
        "X1_min, X1_max = X1.min(), X1.max()\n",
        "X2_min, X2_max = X2.min(), X2.max()\n",
        "ax1.set_xlim(X1_min, X1_max)\n",
        "ax1.set_ylim(X2_min, X2_max)\n",
        "plt.tight_layout()\n",
        "path_fig_fx_train = \"drive/My Drive/NIPS2020/results/socialadsnet/fig_fx_train_{clf}_yhat{yhat}_pca{pca}.svg\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fx_train, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DwpQgHNpA6zm"
      },
      "source": [
        "#%%\n",
        "# PLOT DECISION SURFACE\n",
        "# ==================\n",
        "# Plot test instances and decision surface\n",
        "# ----------------------------------------------\n",
        "# Visualising the Train set results\n",
        "fig1 = plt.figure(figsize=(8,8),dpi=120)\n",
        "ax1 = fig1.add_subplot(111)\n",
        "X_set, y_set = X_val_surface_plot, y_val_surface_plot\n",
        "y_set[y_set==0] = -1\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d_set = 1-y_set*classifier.decision_function(X_set)\n",
        "else:\n",
        "    pred_x = classifier.predict(X_set)\n",
        "    pred_x[pred_x==0] = -1\n",
        "    d_set = y_set*pred_x\n",
        "xi_set = np.max([[np.zeros(d_set.size)],[d_set]],axis=0).ravel()\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d = classifier.decision_function(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "    ax1.contourf(X1, X2, np.where((np.abs(d)<1),np.abs(d),np.nan), alpha = 0.6, cmap='gist_gray', label='margin region')\n",
        "    ax1.contour(X1, X2, d, levels=[-1,0,1], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set>1,0]\n",
        "    y = X_set[xi_set>1,1]\n",
        "else:\n",
        "    d = classifier.predict_proba(np.array([X1.ravel(), X2.ravel()]).T)[:, 1].reshape(X1.shape)-0.5\n",
        "    ax1.contourf(X1, X2, np.where((np.abs(d)<0.25),np.abs(d),np.nan), alpha = 0.6, cmap='gist_gray', label='margin region')\n",
        "    ax1.contour(X1, X2, d, levels=[-0.25, 0, 0.25], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set==0,0]\n",
        "    y = X_set[xi_set==0,1]\n",
        "\n",
        "dots = ['red','blue']\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    ax1.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                color = dots[i], label = '{}'.format('no purchase' if i==0 else 'purchase'),alpha=0.7,s=150, marker='*', linewidths=5)\n",
        "# Plot slack variables magnitudes\n",
        "ax1.scatter(x, y, s=150, facecolors='none', edgecolors='g', label='misclassified', linewidths=5)\n",
        " \n",
        "ax1.set_xlabel('Age',fontsize=40)\n",
        "ax1.set_yticks([])\n",
        "ax1.set_title('Validation set',fontsize=40)\n",
        "ax1.legend(loc='lower left', framealpha=0.5, prop={'size': 30},labelspacing=0.0)\n",
        "ax1.set_xlim(X1_min, X1_max)\n",
        "ax1.set_ylim(X2_min, X2_max)\n",
        "plt.tight_layout()\n",
        "path_fig_fx_val = \"drive/My Drive/NIPS2020/results/socialadsnet/fig_fx_val_{clf}_yhat{yhat}_pca{pca}.svg\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fx_val, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OMj3AeGfHm0J"
      },
      "source": [
        "t = np.stack((X1.ravel(), X2.ravel()), axis=1)\n",
        "t_scaled = scaler_surface_plot.transform(t)\n",
        "f,v = exp_surface_plot.gpr.predict(t_scaled)\n",
        "f = f.reshape(X1.shape)\n",
        "# PLOT DECISION SURFACE\n",
        "# ==================\n",
        "# Plot test instances and decision surface\n",
        "# ----------------------------------------------\n",
        "# Visualising the Test set results\n",
        "fig1 = plt.figure(figsize=(10,8),dpi=120)\n",
        "ax1 = fig1.add_subplot(111)\n",
        "X_set, y_set = X_test_surface_plot, y_test_surface_plot\n",
        "y_set[y_set==0] = -1\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d_set = 1-y_set*classifier.decision_function(X_set)\n",
        "else:\n",
        "    pred_x = classifier.predict(X_set)\n",
        "    pred_x[pred_x==0] = -1\n",
        "    d_set = y_set*pred_x\n",
        "xi_set = np.max([[np.zeros(d_set.size)],[d_set]],axis=0).ravel()\n",
        "if hasattr(classifier, \"decision_function\"):\n",
        "    d = classifier.decision_function(np.array([X1.ravel(), X2.ravel()]).T).reshape(X1.shape)\n",
        "    ax1.contour(X1, X2, d, levels=[0], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set>1,0]\n",
        "    y = X_set[xi_set>1,1]\n",
        "else:\n",
        "    d = classifier.predict_proba(np.array([X1.ravel(), X2.ravel()]).T)[:, 1].reshape(X1.shape)\n",
        "    ax1.contour(X1, X2, d, levels=[0.5], colors='black', linestyles='dashed')\n",
        "    ax1.contourf(X1, X2, Z, alpha = 0.3, cmap = matplotlib.colors.ListedColormap(('red', 'blue')))\n",
        "    x = X_set[xi_set==0,0]\n",
        "    y = X_set[xi_set==0,1]\n",
        "\n",
        "f_set = table_surface_plot['eta'].to_numpy()[::-1]\n",
        "f_set = np.unique(np.around(np.append(f_set, f.max()), decimals=2))\n",
        "cs = ax1.contourf(X1, X2, f, f_set, origin='upper', cmap='gray', alpha=0.5)\n",
        "ax1.contour(X1, X2, f, f_set, colors='black')\n",
        "cbar = fig1.colorbar(cs, pad=0.0, shrink=0.80)\n",
        "cbar.ax.set_title(r'$f$', fontsize=40, loc='left')\n",
        "cbar.set_label(r'$\\eta$', labelpad=-10, y=1.10, rotation=0, fontsize=30)\n",
        "dots = ['red','blue']\n",
        "for i, j in enumerate(np.unique(y_set)):\n",
        "    ax1.scatter(X_set[y_set == j, 0], X_set[y_set == j, 1],\n",
        "                color = dots[i], label = '{}'.format('no purchase' if i==0 else 'purchase'), alpha=0.7, s=150,marker='x', linewidth=5)\n",
        "# Plot slack variables magnitudes\n",
        "ax1.scatter(x, y, s=150, facecolors='none', edgecolors='g', label='misclassified', linewidths=5)\n",
        "\n",
        "ax1.set_xlabel('Age',fontsize=40)\n",
        "ax1.set_yticks([])\n",
        "ax1.set_title('Test set',fontsize=40)\n",
        "ax1.legend(loc='lower left', framealpha=0.5, prop={'size': 30}, labelspacing=0.0)\n",
        "ax1.set_xlim(X1_min, X1_max)\n",
        "ax1.set_ylim(X2_min, X2_max)\n",
        "plt.tight_layout()\n",
        "path_fig_fx_test = \"drive/My Drive/NIPS2020/results/socialadsnet/fig_fx_test_{clf}_yhat{yhat}_pca{pca}.svg\".format(clf=clf, pca=applyPCA, yhat=addPredictions)\n",
        "plt.savefig(path_fig_fx_test, bbox_inches='tight', facecolor='w')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x737XLArx4ev"
      },
      "source": [
        "report_table_concat = pd.concat(report_table)\n",
        "table_by_row_index = report_table_concat.groupby(report_table_concat.index)\n",
        "report_table_mean = table_by_row_index.mean()\n",
        "report_table_std = table_by_row_index.std()\n",
        "report_table_median = table_by_row_index.median()\n",
        "report_table_q1 = table_by_row_index.quantile(q=0.25)\n",
        "report_table_q3 = table_by_row_index.quantile(q=0.75)\n",
        "\n",
        "report_criteria_concat = pd.concat(report_criteria)\n",
        "table_by_row_index = report_criteria_concat.groupby(report_criteria_concat.index)\n",
        "report_criteria_mean = table_by_row_index.mean()\n",
        "report_criteria_std = table_by_row_index.std()\n",
        "report_criteria_median = table_by_row_index.median()\n",
        "report_criteria_q1 = table_by_row_index.quantile(q=0.25)\n",
        "report_criteria_q3 = table_by_row_index.quantile(q=0.75)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8MSpqF8V0Vao"
      },
      "source": [
        "##%\n",
        "# Signaling function statistics (median(q1-q3))\n",
        "output = io.StringIO()\n",
        "columns = ['%reduction_val','budget','%reduction_test']\n",
        "df1 = report_table_median[columns]\n",
        "df2 = report_table_q1[columns]\n",
        "df3 = report_table_q3[columns]\n",
        "pval = report_table_mean['p_value']\n",
        "rho = report_table_mean['rho_user']\n",
        "assert(df1.shape[0]==df2.shape[0])\n",
        "numRows = df1.shape[0]\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) if pval[i]<=0.05 and j!=1 else r'{:.2f}({:.2f}-{:.2f})'.format(val1,val2,val3)\\\n",
        "       if pval[i]<=0.05 and j==1 else r'{}' for val1,val2,val3,j in zip(df1.iloc[i],df2.iloc[i],df3.iloc[i],range(len(columns)))]\n",
        "  output.write(\"{{}} & {{}} & {rho:.2f} & %s & {H0} \\\\\\\\\\n\".format(rho=rho[i], H0=r'$\\surd$' if pval[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBDXnXB92-o1"
      },
      "source": [
        "##%\n",
        "# Signaling function statistics (mean+/-std)\n",
        "output = io.StringIO()\n",
        "columns = ['%reduction_val','budget','%reduction_test']\n",
        "df1 = report_table_mean[columns]\n",
        "df2 = report_table_std[columns]\n",
        "pval = report_table_mean['p_value']\n",
        "rho = report_table_mean['rho_user']\n",
        "assert(df1.shape[0]==df2.shape[0])\n",
        "numRows = df1.shape[0]\n",
        "for i in range(numRows):\n",
        "  row = [r'{:.1f}$\\pm${:.1f}'.format(val1,val2) if pval[i]<=0.05 and j!=1 else r'{:.2f}$\\pm${:.2f}'.format(val1,val2)\\\n",
        "       if pval[i]<=0.05 and j==1 else r'{}' for val1,val2,j in zip(df1.iloc[i],df2.iloc[i],range(len(columns)))]\n",
        "  output.write(\"{{}} & {{}} & {rho:.2f} & %s & {H0} \\\\\\\\\\n\".format(rho=rho[i], H0=r'$\\surd$' if pval[i]<=0.05 else r'$\\times$')%(\" & \".join(row)))\n",
        "print(output.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vThbbY8aLOHb"
      },
      "source": [
        "##%\n",
        "# Comparison statistics (median(q1-q3))\n",
        "output = io.StringIO()\n",
        "shared = ['%reduction_test']\n",
        "df1 = report_table_median[shared]\n",
        "df2 = report_table_q1[shared]\n",
        "df3 = report_table_q3[shared]\n",
        "shared.append('jaccard')\n",
        "df4 = report_criteria_median[shared]\n",
        "df5 = report_criteria_q1[shared]\n",
        "df6 = report_criteria_q3[shared]\n",
        "pval = report_table_mean['p_value']\n",
        "rho = report_table_mean['rho_user']\n",
        "assert(df1.shape[0]==df3.shape[0])\n",
        "numRows = df1.shape[0]\n",
        "for i in range(numRows):\n",
        "  row_table = [r'{:.1f}({:.1f}-{:.1f})'.format(val1,val2,val3) if pval[i]<=0.05 else r'{}' for val1,val2,val3 in zip(df1.iloc[i],df2.iloc[i],df3.iloc[i])]\n",
        "  row_criteria = [r'{:.1f}({:.1f}-{:.1f})'.format(val4,val5,val6) if pval[i]<=0.05 and j==0 else r'{:.2f}({:.2f}-{:.2f})'.format(val4,val5,val6)\\\n",
        "                  if pval[i]<=0.05 and j==1 else r'{}' for val4,val5,val6,j in zip(df4.iloc[i],df5.iloc[i],df6.iloc[i],range(len(columns)))]\n",
        "  # row =  []\n",
        "  output.write(\"{{}} & {{}} & {rho:.2f} & %s & %s & {H0} \\\\\\\\\\n\".format(rho=rho[i], \\\n",
        "                H0=r'$\\surd$' if pval[i]<=0.05 else r'$\\times$')%(\" & \".join(row_table), \" & \".join(row_criteria)))\n",
        "print(output.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "p0ji9vNU2AWu"
      },
      "source": [
        "##%\n",
        "# Comparison statistics (mean+/-std)\n",
        "output = io.StringIO()\n",
        "shared = ['%reduction_test']\n",
        "df1 = report_table_mean[shared]\n",
        "df2 = report_table_std[shared]\n",
        "shared.append('jaccard')\n",
        "df3 = report_criteria_mean[shared]\n",
        "df4 = report_criteria_std[shared]\n",
        "pval = report_table_mean['p_value']\n",
        "rho = report_table_mean['rho_user']\n",
        "assert(df1.shape[0]==df3.shape[0])\n",
        "numRows = df1.shape[0]\n",
        "for i in range(numRows):\n",
        "  row_table = [r'{:.1f}$\\pm${:.1f}'.format(val1,val2) if pval[i]<=0.05 else r'{}' for val1,val2 in zip(df1.iloc[i],df2.iloc[i])]\n",
        "  row_criteria = [r'{:.1f}$\\pm${:.1f}'.format(val3,val4) if pval[i]<=0.05 and j==0 else r'{:.2f}$\\pm${:.2f}'.format(val3,val4)\\\n",
        "                  if pval[i]<=0.05 and j==1 else r'{}' for val3,val4,j in zip(df3.iloc[i],df4.iloc[i],range(len(columns)))]\n",
        "  # row =  []\n",
        "  output.write(\"{{}} & {{}} & {rho:.2f} & %s & %s & {H0} \\\\\\\\\\n\".format(rho=rho[i], \\\n",
        "                H0=r'$\\surd$' if pval[i]<=0.05 else r'$\\times$')%(\" & \".join(row_table), \" & \".join(row_criteria)))\n",
        "print(output.getvalue())"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MtjIyrczSDqX"
      },
      "source": [
        "# data to plot\n",
        "means_table = report_table_mean['%reduction_test'].to_numpy()\n",
        "means_table[pval>0.05]=0\n",
        "std_table = report_table_std['%reduction_test'].to_numpy()\n",
        "std_table[pval>0.05]=0\n",
        "means_crit = report_criteria_mean['%reduction_test'].to_numpy()\n",
        "means_crit[pval>0.05]=0\n",
        "std_crit = report_criteria_std['%reduction_test'].to_numpy()\n",
        "std_crit[pval>0.05]=0\n",
        "n_groups = means_table.size\n",
        "\n",
        "# create plot\n",
        "fig, ax = plt.subplots(figsize=(5.4, 5.1), constrained_layout=False, dpi=120)\n",
        "index = np.arange(n_groups)\n",
        "bar_width = 0.35\n",
        "opacity = 0.6\n",
        "\n",
        "rects1 = plt.bar(index, means_table, bar_width, yerr=std_table, capsize=5,\n",
        "alpha=opacity, color='b', label=r'$f(x)>\\eta$')\n",
        "\n",
        "rects2 = plt.bar(index + bar_width, means_crit, bar_width, yerr=std_crit, capsize=5,\n",
        "alpha=opacity, color='g', label=r'$g(x)>\\theta$')\n",
        "\n",
        "plt.xlabel(r'budget $\\rho$')\n",
        "plt.ylabel(r'loss reduction $r_{test}(\\%)$')\n",
        "# plt.title('Scores by person')\n",
        "plt.xticks(index + bar_width/2, (['{}'.format(i) for i in report_table_mean['rho_user'].to_numpy()]))\n",
        "plt.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sgyctn_qkAa3"
      },
      "source": [
        "0.37+0.37-0.9999"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}